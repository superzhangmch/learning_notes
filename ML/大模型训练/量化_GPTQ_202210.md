《GPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFORMERS》 https://arxiv.org/pdf/2210.17323

GPTQ 继承自 OBQ(Optimal Brain Quantization，2022), 而 OBQ 又是来自 OBS（Optimal Brain Surgeon，1993）。

名字由来: QPTQ = QPT(GENERATIVE PRE-TRAINED TRANSFORMERS) + Q(量化）。

这一类方法做的是训练后的量化。它针对的是 inference 中矩阵乘法的权重 w，而不管激活。它的基本思路是，选一批随机 sample 做 inference，从而可以得到每一个矩阵乘的激活 input X。对网络中的一个具体权重矩阵 W，寻找最佳量化 $\hat{W}$ 满足 $\text{argmin}_{\hat{W}} ∥W X − \hat{W} X∥_2^2$。

也就是说，用它做量化时，不关心网络结构是什么，原始 loss 是什么，而只需要知道一个权重 W 的典型 input X 啥样，就可以对 W 做量化。

GPTQ 文中试了 4 bit, 乃至于 2 bit 量化。

----

## 理论: OBQ - Optimal Brain Quantization，2022

发展脉络是 OBS => OBQ => GPTQ。但是背后的基本原理是一样的。看一些资料，OBS 是想把不重要的权重给置零，OBQ 是用同样的参数筛选方式，但是不是置零而是量化之。这里只看 GPTQ paper 中介绍的 OBQ。

### （1）、思路

如前所述，它只处理矩阵乘法，且只处理里面的权重。对一个具体权重矩阵 W，它的量化策略是，寻找 $\hat{W}=quant(W)$ 使得 $\hat{W}$ 满足 $∥W X − \hat{W} X∥_2^2$ 最小，其中 X 是一批样本做 inference 得来。

那么每个权重 $w \in W$ 都取最接近量化，这样可行吗？这样做都没反映出最小化 $∥..∥$，当然不好！。实际上，鉴于量化后的 $\hat{W}$ 是 “量化“的，也就是有限取值离散的，OBQ 真实想做的是：如果把所有可行解全遍历搜索一下，就能得到最佳 $\hat{W}$ 了。当然这样做会比较低效。所以 OBS/OBQ 在于有一套方法，能更高效低做，当然它的解法也不是全局最优解，只是比每个权重 w 取最接近量化要好。

注意，鉴于 OBQ 真实想做（但做不到的）的是全遍历搜出最佳解，所以对最终量化后的每个 $w \in \hat{W}$ 只是处于某一量化位而已，并不必然保证与原始 w 的数值差很小。

先列出每优化一个参数时的迭代公式，下文讲为啥。

参数迭代公式：

$$
\begin{cases}
w_q = \arg\min _ {w_q} \frac{(\text{quant}(w_q) - w_q)^2}{[H^{-1}] _ {qq}} & // 用来决定量化哪个参数\\
\delta_F = - \frac{w_q - \text{quant}(w_q)}{[H^{-1}] _ {qq}} \cdot (H^{-1}) _ {:,q} & // 其余参数怎么做补偿更新
\end{cases}
$$
- $[H^{-1}] _ {qq}$ 是 $H^{-1}$ 的 row, col=q, q 的元素
- $(H^{-1}) _ {:,q}$ 是 $H^{-1}$ 的第 q 列元素构成的列向量

涉及到的 Hessian 逆矩阵的迭代式：

$$
H _ {-q}^{-1} = \left( H^{-1} - \frac{1}{\left[H^{-1}\right] _ {qq}} H^{-1} _ {:,q} \, H^{-1} _ {q,:} \right) _ {-p}
$$

- 下标 -q 表示去掉第q维度。
- $H^{-1}$ 迭代前一步的 hessian 逆，用它可以得到当前 step 的 hessian 逆 $H _ {-q}^{-1}$。
- $H _ {-q}^{-1}$ 表示 $H^{-1}$ 去掉 q 行 q 列后所得的小一号的矩阵
- $\left[H^{-1}\right] _ {qq}$ 表示取 $H^{-1}$ 取 row, col = q, q 位置的标量元素
- $H^{-1} _ {:,q}$ 为 $H^{-1}$ 取第 q 列所得的列向量
- $H^{-1} _ {q,:}$ 为 $H^{-1}$ 取第 q 行所得的行向量

### （2）、为啥牵扯到 Hessian

鉴于 OBQ 是在最小化 $∥W X − \hat{W} X∥_2^2$，也就是以它为目标，可以列出 loss 如下；为方便理解，符号上令原权重是 $W_0$, 量化后结果权重是 $W$：

$$
Loss(W) = ∥W_0 X − W X∥_2^2
$$

一眼可看出该 loss 的最优解是 $W=W_0$，此时 loss = 0。岂不都不用优化了？注意这是个带约束的优化问题，约束是每个 w 都应该处于某一量化整点位。

对它做 $W_0$ 点的 Taylor 展开，注意 L 是二次的，所以正好可以精确展开到 2 阶：

$$
L(w) = L(w_0) + \nabla_w L(w_0)^\top (w-w_0) + \tfrac{1}{2}(w-w_0)^\top \nabla_w^2 L(w_0) (w-w_0)
$$

注意：
- 在 $w_0$ 点 L(w) 取得最小值，也就是 $\nabla_w L(w_0) = 0$
- $\nabla_w^2 L(w_0)$ 就是 Hessian 矩阵，记作 H。它乃对称矩阵，可算得等于 $2XX^T$。
  - 若 X 是一条样本的，则 X 是列向量， $XX^\top$ 是外积。如果 X 是多条数据，则 X 是列向量的拼接，X = { $x_1, x_2, .., x_n$ }, $XX^\top = \sum_i x_i x_i^\top$。

于是 $L(w) = L(w_0) + \tfrac{1}{2}(w-w_0)^\top H (w-w_0)$, 而任一种量化方案 loss 差是 $\Delta L = L(w) - L(w_0) = \tfrac{1}{2}(w-w_0)^\top H (w-w_0)$。于是，OBQ 的量化思想归结为选择 $W$ 使得下式最小：

$$
\tfrac{1}{2}(w-w_0)^\top H (w-w_0) = (w-w_0)^\top X X^\top (w-w_0)
$$

### （3）、启发式逐权重量化：思路

- 权重矩阵按行独立处理： 仿佛权重矩阵本来只是 1 x n 的。
  - $∥..W..∥_2^2 = \sum_{row} ∥..W_{row}..∥_2^2$, 所以可以一次处理一行
  - 权重 W 一般是矩阵，设 x 是 batchsize=1 的，Wx 是向量。OBQ 一次只考虑 W 的一行，这样 Wx 是 scalar 数字。
- 一次量化一个参数：启发式逐个推进
  - 对待量化参数 { $w_i$ }, 找到最适当的一个并量化它，同时补偿调整其他未量化参数的取值，使得 $\Delta L(w)$ 可控。

下面说下它这个启发式逐参数量化咋做的。

它是逐参数量化的，对于已经量化的就不再管了（而未量化参数集合则是每次迭代少一个）。于是每量化一个参数的时候，仿佛面对的就是原始参数向量 W 一样（只是长度短了）。

一次量化一个参数，但其他的参数也会补偿更新——也就是一个迭代其实更新了所有参数。对权重 $W$ = { $w_1, w_2, .., w_n$ }，假设选定要量化的是 $w_q$，更新后的这n个参数是 $w_q^{'}$ 与 $w'_{F}$ (下标 F 表示不包含 q 的所有index)。那么这些更新应该满足：
- $w_q$ 按量化规则量化得到的 $w_q^{'}$, 而其他参数的更新方法应该是： $w_q \rightarrow w_q^{'}$ 的基础上最小化 $\Delta L$, 也就是
  
$$
w_{F}^{'} = argmin_{w_{F}} L(\{ w_q^{'}, w_{F} \})
$$

- 反过来，对 $w_q$ 来说，它如果知道 $w_{F}$ 会怎样根据 $w_q^{'}$ 而更新，它应该把这点考虑进去——这就是被优化参数的选择原则：作量化的参数 index 是

$$
  index = argmin_{q}\ \text{L}(\\{quant(w_q), argmin_{w_{F}}\ \text{L}(\\{ quant(w_q), w_{F} \\}) \\})  \ \ 或者说
$$

$$
\begin{cases}
w_{F_new} := argmin_{w_{F}}\ \text{L}(\\{ quant(w_q), w_{F} \\}) \\
index = argmin_{q}\ \text{L}(\\{quant(w_q), w_{F_new} \\})
\end{cases}
$$

- 上面颇有点博弈论的那意思。

根据以上，那么 $w_1, w_{2:n}$ 的更新公式就容易推导了：令 $e_q = quant(w_q) - w_q$，推导出其他 n-1 个参数的最佳补偿更新 $\delta_F$, 然后把 $\delta_F$ 代入 $\Delta L$, 并选出最适合(即使得 $\Delta L$ 最小)量化的那个权重。

### （4）、启发式逐权重量化：具体怎么做

假设选定对第 $q$ 维量化，记量化差 $e_q := {quant}(w_q)-w_q$, 并记其余 n-1 各参数的补偿更新改变量是 $\delta_F$，则有如下分块表达式：

$$
\Delta L = \tfrac{1}{2} {\Delta w}^\top H {\Delta w} = \tfrac12
\begin{bmatrix}
e_q \\ , \delta_F
\end{bmatrix}^{\top}
\begin{bmatrix}
H_{qq},  H_{qF} \\
H_{Fq},  H_{FF}
\end{bmatrix}
\begin{bmatrix}
e_q \\ , \delta_F
\end{bmatrix},
$$

注意 Hessian H 是对称的， $H_{qF}^\top = H_{Fq}$, $e_q$ 与 $H_{qq} 都是标量数字。

现在要固定 $e_q$ 把最优 $\delta_F$ 求出来。可以得出其最优解是 $\delta_F^\star = -H_{FF}^{-1} H_{Fq} e_q$。

**怎么求：**

这可以用直接展开 $\Delta L$ 并求导得到：展开 $\Delta L$ 的分块矩阵乘法表达式， 并关于 $\delta_F$ 求导有：

$$
\begin{cases}
f(\delta_F) := \Delta L = \tfrac{1}{2} H_{qq} e_q^2 + \tfrac{1}{2} e_q (H_{qF}\delta_F + \delta_F^\top H_{Fq}) + \tfrac{1}{2}\delta_F^\top H_{FF}\delta_F \\
\nabla_{\delta_F} f(\delta_F) = H_{FF}\delta_F + e_q H_{Fq}
\end{cases}
$$

而最佳 $\delta_F$ 是满足 $\nabla_{\delta_F} f(\delta_F) = 0$ 的，于是解 $H_{FF}\delta_F + e_q H_{Fq} = 0$ 得 $\delta_F^\star = -H_{FF}^{-1} H_{Fq} e_q$。

再把 $\delta_F^\star$ 表达式代入 $\Delta L$, 又可得：

$$
\Delta L_{\min}(e_q) = \frac{1}{2} (H_{qq} - H_{qF} H_{FF}^{-1} H_{Fq} ) e_q^2
$$

于是需量化的那个参数下标 q，乃能令 $\Delta L_{\min}$ 取值最小的（其中 $H_{qq} - H_{qF} H_{FF}^{-1} H_{Fq}$ 称为矩阵的 Schur complement， 见下面）。

**迭代更新式:**

这样就得到了迭代更新式：

$$
\begin{cases}
e_q := {quant}(w_q)-w_q \\
q = argmin_q \ \frac{1}{2} (H_{qq} - H_{qF} H_{FF}^{-1} H_{Fq} ) e_q^2 \\
\delta_F = -H_{FF}^{-1} H_{Fq} e_q
\end{cases}
$$

看起来和 paper 所给的不一样, paper 中的是：

$$
\begin{cases}
w_q = \arg\min _ {w_q} \frac{(\text{quant}(w_q) - w_q)^2}{[H^{-1}] _ {qq}} \\
\delta_F = - \frac{w_q - \text{quant}(w_q)}{[H^{-1}] _ {qq}} \cdot (H^{-1}) _ {:,q}
\end{cases}
$$
- $[H^{-1}] _ {qq}$ 是 $H^{-1}$ 的 row, col=q, q 的元素
- $(H^{-1}) _ {:,q}$ 是 $H^{-1}$ 的第 q 列元素构成的列向量

下面用 Schur complement 推导出 paper 中的公式，并证明二者等价。先补充下相关知识

> （a)、Schur complement 的定义
> 
> 设一个分块矩阵

$$
M = \begin{bmatrix}
A & B \\
C & D
\end{bmatrix}, \quad D \text{ 可逆}.
$$

> 那么 $M$ 对 $D$ 的 **Schur complement** 定义为： $S = A - B D^{-1} C.$
> 
> 它的由来，见下图：
>
> <img width="708" height="396" alt="image" src="https://github.com/user-attachments/assets/d7ace931-8d81-4a01-86e2-c995399ab3fc" />
>
> Schur complement 有关内容，重点都在于消去一元后，关于另外一元的系数。
>
> （b)、用于矩阵求逆
>
>  <img width="1222" height="336" alt="image" src="https://github.com/user-attachments/assets/33c6d768-4f5f-4e93-ae2b-438f0f2adcf8" />
> 
> （c）、二次型消去一元，并求极值：对于任意二次型

$$
   Q(x,y) = \tfrac{1}{2}
   \begin{bmatrix} x, y \end{bmatrix}^\top
   \begin{bmatrix}
   A & B \\
   B^\top & D \end{bmatrix}
   \begin{bmatrix} x, y \end{bmatrix},
$$

> 如果要“固定 $x$，消去 $y$”，最优值总是

$$
Q_{\min}(x) = \tfrac{1}{2} x^\top (A - BD^{-1}B^\top)x,
$$

> 这个是一个非常标准的结论。从而代入可以直接得到上面 $\Delta L_{\min} = \frac{1}{2} (H_{qq} - H_{qF} H_{FF}^{-1} H_{Fq} ) e_q^2$ 的表达式。

用以上矩阵求逆式子，就可以得到 paper 中的迭代公式（注意每迭代一次，F 小 1）：下面假设量化的参数是第一个：

<img width="1222" height="1018" alt="image" src="https://github.com/user-attachments/assets/b3c097d1-e358-456d-8775-4476f0292dac" />

### （5）、Hessian 求逆的迭代求法

OBQ 在每步迭代中都是面对的一个新的 H（size 小 1），如果每量化一个参数，就算一个矩阵逆，计算量太大。作者给出了巧妙的迭代求逆式，每次借助上一个更大一圈的 H 的逆（注意 H 来自 X，每次的新 H 都是上一个更大一圈 H 的子矩阵）：

$$
H _ {-q}^{-1} = \left( H^{-1} - \frac{1}{\left[H^{-1}\right] _ {qq}} H^{-1} _ {:,q} \, H^{-1} _ {q,:} \right) _ {-p}
$$

- 下标 -q 表示去掉第q维度。
- $H _ {-q}^{-1}$ 表示 $H^{-1}$ 去掉 q 行 q 列后所得的小一号的矩阵
- $\left[H^{-1}\right] _ {qq}$ 表示取 $H^{-1}$ 取 row, col = q, q 位置的标量元素
- $H^{-1} _ {:,q}$ 为 $H^{-1}$ 取第 q 列所得的列向量
- $H^{-1} _ {q,:}$ 为 $H^{-1}$ 取第 q 行所得的行向量

仍然可以用 schur 补来证明此式。

<img width="1518" height="1046" alt="image" src="https://github.com/user-attachments/assets/ae070e8e-6472-4fd0-86b4-a60e5d3c9b1f" />

这样，对一个权重行的量化，只需要在最开始算一次 $H^{-1}$，接下来每次量化一个参数并需要算一个更小的 H 逆的时候，就用该迭代式逐次算就行了，计算量很小。

----

## GPTQ

GPTQ 只是对 OBQ 的弱化版本与加速版本。

### （1）、不再优选最佳量化的那个参数，而是随便选一个都行；额外好处：可以并行量化权重矩阵 W 的不同行

**为啥可以不关心顺序**

> this is because the slightly lower number(更少数量的) of quantized weights with large individual error(大的量化误差）is balanced out by those weights being quantized towards the end of the process, when only few other unquantized weights that can be adjusted for compensation remain.

the slightly lower number(更少数量的) of quantized weights with large individual error(大的量化误差）：OBQ 希望每个参数量化的时候，量化误差都小些。或者说它的贪心量化方式，意在降低有 "大的量化误差" 的参数数量。这是通过量化参数的优先级完成的——先量化的那些参数，误差会小。

由于参数调整，OBQ 过程的最后阶段的量化误差必然比较大。但这是所剩未量化参数已经不多，补偿更新作用比较弱，力度不够了。

总之，没必要斟酌，直接随便一个顺序来逐参数量化即可。

**为啥可以并行**

- 不同权重行的并行：OBQ 把 W 拆成行并独立进行。GPTQ 发现，如果可以不关心量化的参数顺序，那么这些行可以并行量化。这是因为对一行权重量化的时候，依赖于 Hessian $H=XX^\top$ , 而不同的 $W_i$ 都作用于同一个 X。

### （2）、一次量化一行权重的多个参数（Lazy Batch-Updates）

- 必要性：每次更新都要改动一个很大的矩阵（Hessian 逆矩阵里的行列），运算量很小但内存访问很大 → GPU 算力吃不满，速度受限于内存带宽。
- 好处：计算量没变，但把“很多零碎的小更新”合并成“大批量更新”，显著提高 GPU 利用率。
  - paper 实验里，能带来一个数量级的加速，对大模型特别关键。

结合前面一次处理 W 的所有行 {$W_i$ }，所以一次量化的参数量是 $B \times row\_cnt$, B=128 列。

批量量化的更新式子是：

$$
\begin{cases}
\delta_{F} = -(w_{Q} - quant(w_{Q})) \big([H_{F}^{-1}] _ {QQ}\big)^{-1} (H_{F}^{-1}) _ {:,Q} & // 补偿更新 \\
H_{-Q}^{-1} = \Big( H^{-1} - H _ {:,Q}^{-1} \big([H^{-1}] _ {QQ}\big)^{-1} H _ {Q,:}^{-1} \Big) _ {-Q}  & // Hessian 逆的迭代式
\end{cases}
$$
- 下标 "-Q" 表示排除掉某些列或行

注意：它和 OBQ 一次量化一个参数的迭代形式是一样的。推导过程也一模一样。不列。

### （3）、H 逆矩阵的更精确求解：Cholesky 分解

----

（1）、对于一个参数，怎么量化它的？用 scaling 吗

（2）、
