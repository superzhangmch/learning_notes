# transormer 训练的序列并行

transormer 的 hidden state shape = [batch, seq_len, hid_dim], 或者 [seq_len, batch, hid_dim]。除了 attn 之外不分，都是在单个token上做的，也就是 seq维度和 batch 维没区别，它们会被当做 seq*batch 的大batch。

所以序列并行，本质上来说，只和处理 attn 有关。

而 MHA，就单个 head 来说，是 softmax(QK')*V' 的形式，本质上还是矩阵乘法操作（即使 MLA 等 attention，仍可以拆为基本运算是矩阵乘法），而张量并行，就是把大的矩阵乘法拆为分块矩阵乘法，在不同 gpu 上算。从这个角度来说，序列并行和张量并行，解决的都是同一个问题。只是张量并行是更泛的，而序列并行处理的是某一种特殊矩阵乘法，从而可以更有针对性地优化而已。各种方法，可以说都是在分块矩阵乘法角度作优化。

序列并行和张量并行都在解决“算力和显存不够，如何把大算子拆到多 GPU 上”的问题。区别在于：
- 张量并行 = 矩阵通用切分。
- 序列并行 = 针对序列维度的特殊切分，主要优化 attention 的通信模式。

----

## 一、megatron-3 的序列并行：非真序列并行，只是为了降激活显存

它是和张量并行相结合的用来减少激活显存占用量的方案。确实在 seqence 维度切分了，但是和有助于 attn 计算那样的序列并行，没啥关系。

出自： megatron 第三篇 《Reducing Activation Recomputation in Large Transformer Models》 https://arxiv.org/pdf/2205.05198 ，该文旨在设法降低激活显存的占用。

**背景：** megatron-2 《https://arxiv.org/pdf/2104.04473》 4.2 节讲到，为了方便 MHA 计算时少一些显存 copy，把 hidden state 的 layout 由一般做法的 [B, S, num_heads, head_dim]， 换成了 [seq, Batch, num_heads, head_dim], 这样方便中间两维 batch*num_heads 结合。从这后， megatron 就是 [S, B, H] seq-first 的。笔记在 [这里](https://github.com/superzhangmch/learning_notes/blob/main/ML/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/transformer%20%E7%9A%84%E9%9A%90%E5%B1%82%20shape%EF%BC%9Absh%20%E4%B8%8E%20sbh.md) 。

megatron 发现 transformer 种相关张量并行后面的 layerNorm、dropout 需要存下不少显存，以便反向传播。于是选择对这些显存作切分并分布到不同节点上，用时拉取过来。如果是 batch-first，没准它对batch继续做拆分，但是鉴于它已经是 sequence-first 了，所以很自然地，megatron 选择了按 sequence 维度切分。

**激活显存问题由来 ：**

<img width="1410" height="936" alt="image" src="https://github.com/user-attachments/assets/f1c4ea1f-c632-4de8-ac45-c0e526b41e8e" />

如上图之下，无论是 attn 还是 MLP，在 tensor 并行的最后一步，都是 $\[Y_1, Y_2\]\cdot \[B_1, B_2\]^T = Y_1 B_1 + Y_2 B_2$ 形式。无论是 [B, S, H] 还是 [S, B, H], $Y_1, Y_2$ 都是把 H 维切分了，且与 S 和 B 的顺序无关。 $Y_i \cdot B_i$ 的 shape 和期望结果 shape 一样，但是分散在不同 gpu 上，需要聚合求和到一起，才能做 dropout(layout 是element wise的，如果分布式作，则需要全局一致的随机数种子) 与 layerNorm（layerNorm 是 token wise的，需拿到完整 hidden_dim 数据）。又因为layerNorm 后的下一layer 的输入，在每个 gpu 都需要全量数据，于是每一 gpu 都重复算了 layerNorm。反向传播时，也需要这些激活，因此需要存下来。

作者推算得，对于标准 transformer 来说，单层的总激活显存占用是： $sbh(34+5\frac {as} {h})$, 其中 s=seq, b=batch, h=hidden_dim, a=num_heads， $5\frac {as} {h}$ 乃 attn带来的。通过张量并行，可以把 $sbh(24+5\frac {as} {h})$ 的显存均分到张量并行数 t 上（变成 $sbh(\frac {24} t +5\frac {as} {th})$）。而 $10sbh$ 的显存，对应 dropout + layernrom 处，没法分摊。因此要通过序列并行把这部分显存分摊。

这 $10sbh$ 显存的具体构成（占据了 10/34, 可见占比确实不小）：
- 每个 dropout 占据 sbh, MLP 与 ATTN 后各有一个 dropout（注意 attn 内部 softmax 后还有个小 dropout，不是它）
- MLP 与 ATTN 后各有一个 layerNorm。layerNorm 需要 2bh，总共 4sbh。layerNorm 之后的 MLP 或者 attn QKV proj 的输入，共 4sbh，也需要存下。所以总共 8sbh。

**怎么用序列并行解决问题的：**

可以看到面对的问题，与 sequence 本身并没关系。只是当要切分时，因为 sequence-first，拿它——也就是tensor的第一个维——切分最方便高效（另外，batch未必很大，如果那样，按它切也会不大方便）。总之切分后，每个 gpu 的所有激活显存都是均分存储了。layernorm+dropout 激活均分了，当用的时候，当场拉来就行。

<img width="1496" height="992" alt="image" src="https://github.com/user-attachments/assets/89645a62-7173-4560-8cc9-a49a764993ef" />

注意：
- 上图上下对比看，可以发现通信量"基本"没变。
  - forward：张量并行后不跟序列并行，需要一次 all-reduce 通信
    - 若用了序列并行，需要两次不相邻的 scatter-reduce 与 all-gather，而一次 all-reduce 正好由这两个动作构成。总体保持不变。
  - backward：张量并行后不跟序列并行，也是需要一次 all-reduce 通信
    - 若用了序列并行，也是需要两次不相邻的 scatter-reduce 与 all-gather（但会和forward顺序反），而一次 all-reduce 正好由这两个动作构成。当然为了省显存考虑， $\{Y_i^s\} \rightarrow Y$ 的 g 算子，在backward时，仍要算一次还原 Y，于是额外再多一次 all-gather。即两次 all-gather，一次 scatter-reduce。但是这多的一次 all-gather 可以和计算重叠，并不会真的增加时间。总体上相当于不变。
- 经此序列并行后，整个流里，全是并行的了。

## 二、contextual parallel 序列并行：

## 三、DeepSpeed-Ulysses 的序列并行

微软：《DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models》 https://arxiv.org/pdf/2309.14509 

<img width="1940" height="896" alt="image" src="https://github.com/user-attachments/assets/0b7f83b7-1341-4d61-80bf-76a52e92c1be" />

执行流程如上。

<img width="840" height="536" alt="image" src="https://github.com/user-attachments/assets/4d21c5ee-ef5d-477f-a666-702b6723638a" />

Q/K/V 的 all-to-all 传输方式见上图。上图改编自讲 Ulysses 的 https://zhuanlan.zhihu.com/p/689067888 。

序列分块: 
- 把输入序列（长度 L）沿着 token 维度切分，分给不同 GPU。
  - 每个 GPU 存的仍是完整的模型参数，但只持有序列的一部分。

注意力本身就是分布式执行的：
- 每张 GPU 只存自己负责的 token 的 Q、K、V。
  - 比如总长 16k，4 张卡 → 每卡持有 4k tokens 的 Q/K/V。
- 分布式点积 (QKᵀ)
  - 为了让“本地 Q”能看到“全局 K”，使用 **all-to-all 通信**：】
    - note：本地 Q 对所有 K 做点积，不是直接复制整份 K/V 到每个 GPU，而是通过 all-to-all 通信：把 K/V 分块传给其他 GPU；每个 GPU 拿到一部分 K/V，就能和自己本地的 Q 做局部乘积；然后再把结果汇总。
- softmax + V 聚合
   * 局部点积后，还要做 softmax，这里需要跨卡同步归一化。
   * 然后用注意力权重乘 V，同样通过 all-to-all 来收集完整 V 分块。

总结：每个 GPU 只存部分 Q/K/V；通过 all-to-all 通信分批拿到别的卡的 K/V 来保证 Q 可以“看见全局”；

接受完整的 K，这不得都存下，显存不会爆炸了？
- 它并不是直接在每卡存一整份 K，而是边通信边计算，避免显存爆炸。
- 也就是流式 all-to-all 方式。
  - 每个 GPU 拿到一个 K 分块就立刻和本地的 Q 做局部 QKᵀ 计算，不需要等到完整 K 全部到齐。然后逐步累积，一个 block 一个 block 地算，得到的局部结果再拼起来；
  - softmax 归一化需要一次全局通信（比如 all-reduce max/sum），然后继续算 V 的加权和（也通过分块 all-to-all 完成）。

### 四、 Ring Attention

UC Berkeley：《Ring Attention with Blockwise Transformers for Near-Infinite Context》 https://arxiv.org/pdf/2310.01889 

做法：

- 把序列分片到多个 GPU。
- QKᵀ 点积时采用 环形传递 (ring all-to-all)：
- 每次只传一小块 K/V 给下一个 GPU，边传边算；
- 最终每个 Q 也能看到完整上下文。
- 优点：通信均衡（每张卡只和两个邻居交互），更适合大规模 GPU 集群。
- 局限：通信延迟会随环长度增加。

1. **序列切分**

   * 将输入序列沿着 **sequence 维度**分到多个 GPU 上。
   * 假设有 $P$ 张卡，序列长度是 $L$，每张卡先只存 $L/P$ 段的 Q/K/V。

2. **局部计算起步**

   * 每张卡先用自己本地的 Q 和本地的 K/V 计算出 attention logits（部分 attention 矩阵的一块）。
   * 但这只是整体 attention 矩阵的一部分，结果不完整。

3. **环形传递 (Ring All-to-All)**

   * 每个 step，把 K/V 块 **传给下一张卡**，同时接收来自上一张卡的 K/V。
   * 当前卡用自己的 Q 和新收到的 K/V 继续算一部分 attention。
   * 重复 $P-1$ 次，Q 就依次见过所有的 K/V 块。

4. **增量归一化**

   * 因为 softmax 需要全局归一化，不能等全部收完再做，否则要缓存所有 logits（内存爆炸）。
   * 所以 Ring Attention 采用 **online softmax** 技术：

     * 在每一轮计算时，维护当前的最大值和累计和，逐步更新 softmax 归一化因子。
     * 这样不需要保存完整 logits 矩阵，就能保证数值正确。

5. **结果聚合**

   * 当一张卡完成环传递循环后，它的 Q 已经和所有 K/V 交互过，能得到完整的 attention 输出。
   * 每张卡只保留自己负责的输出分片。


----

LLM 的**序列并行（Sequence Parallelism）**是一个比较宽泛的概念，目标都是把长序列的计算分布到多个 GPU 上，降低单卡显存压力并提高吞吐。
常见的实现方法主要分为几大类，我给你按**切分维度和通信模式**来分类。

## 1. Context Parallel / Sequence Parallel（Megatron-LM 类）

**代表算法**：

* **Megatron-LM Sequence Parallel**
* **Megatron-LM Context Parallel（CP）**

**思路**：

* 按 **序列长度维度（sequence length）** 把 token 切分到不同 GPU 上。
* 每个 GPU 保存自己那段 token 的激活值，注意力计算需要跨卡取全量 K/V。
* 通常需要 **AllGather / ReduceScatter** 进行跨卡同步。

**优点**：

* 显存占用与序列长度成反比，支持更长 context。
* 对 batch size 没有额外要求。

**缺点**：

* AllGather K/V 会带来较大通信开销，特别是长序列时。

## 2. 交错通信计算（Pipelined Sequence Parallel）

**代表算法**：

* **DeepSpeed-Ulysses**
* **Ring Attention**（严格来说属于通信优化，但也能算到这一类）
* **Tutel Sequence Parallel**

**思路**：

* 不一次性 AllGather 全量 K/V，而是将 K/V 分块（chunking）并环形或流水化传输。
* 每收到一块，就立刻计算对应注意力部分，然后继续传下一块。
* 最终得到完整注意力输出，但通信和计算是**重叠**的。

**优点**：

* 降低峰值显存占用（不需要一次性存全量 K/V）。
* 通信开销被计算掩盖，延迟低。

**缺点**：

* 算法实现更复杂，对拓扑和带宽敏感。

## 3. 双向分片（2D Parallel for Attention）

**代表算法**：

* **Colossal-AI Sequence Parallel**（2D Attention）
* **Mesh TensorFlow GSPMD**（Sequence Sharding + Head Sharding）

**思路**：

* 同时在**序列维度**和**注意力 head 维度**上分片。
* 每个 GPU 只持有自己负责的 token 片段 + 部分 head。
* 通信量减少，因为每个 GPU 计算的注意力规模更小。

**优点**：

* 内存和通信负担更平衡。
* 对大批量多头注意力很友好。

**缺点**：

* 需要额外的并行策略调度和复杂的 AllToAll。

## 4. 流式注意力 / 块稀疏注意力结合并行

**代表算法**：

* **FlashAttention-2/3** + Sequence Parallel
* **Blockwise Sparse Attention + Sequence Sharding**
* **StreamingLLM with Parallel KV Cache**

**思路**：

* 在序列并行的基础上，用流式 attention 或块稀疏减少需要通信的 token 数量。
* KV Cache 按序列分布到不同 GPU，推理时按需访问。

**优点**：

* 长序列训练和推理显存极低。
* 可以动态裁剪注意力计算范围。

**缺点**：

* 对全局依赖建模能力有限（稀疏模式下）。

## 5. 推理专用的序列并行

**代表算法**：

* **PagedAttention**（vLLM）
* **Tensor Parallel + Sequence Sharding for KV Cache**
* **SGLang / LightSeq KV Offload**

**思路**：

* 不在训练时切分，而是在推理时把 KV Cache 按 token 切分存到多 GPU / CPU / NVMe。
* 提升并行吞吐量，减少单卡 KV 压力。

**优点**：

* 推理吞吐高，可扩展到极长上下文。

**缺点**：

* 对训练无帮助，推理场景特化。

如果你愿意，我可以帮你画一张**序列并行算法家族树**，把 Megatron-LM 的 CP、DeepSpeed-Ulysses、Ring Attention、2D Attention 等放在同一个拓扑图上，这样可以直观看它们的差异和关系。
这样你一眼就能看出哪些是“经典切序列”，哪些是“通信优化”，哪些是“多维切分”。

```
                  ┌────────────────────────────┐
                  │   LLM Sequence Parallelism │
                  └────────────────────────────┘
                               │
        ┌──────────────────────┼──────────────────────┐
        │                      │                      │
        ▼                      ▼                      ▼
┌────────────────┐   ┌────────────────────┐   ┌─────────────────────┐
│  1. 经典序列切分 │   │  2. 交错通信计算优化  │   │  3. 双向分片 (2D)    │
│(Megatron-LM CP)│   │ (通信+计算重叠)      │   │ (序列+Head同时切)     │
└────────────────┘   └────────────────────┘   └─────────────────────┘
        │                      │                      │
        │                      │                      │
        ▼                      ▼                      ▼
┌─────────────────────┐  ┌─────────────────────┐  ┌────────────────────────┐
│ Token 按顺序切分到多卡 │  │ 分块KV → 环形/流水通信 │  │ 每卡持部分token+部分head │
│ 全量KV需 AllGather   │  │ 一边收一边算(Overlap) │  │ 降低通信量               │
└─────────────────────┘  └─────────────────────┘  └────────────────────────┘
        │                      │                      │
        │                      │                      │
        ▼                      ▼                      ▼
    代表算法:             代表算法:               代表算法:
  - Megatron-LM SP       - DeepSpeed-Ulysses      - Colossal-AI 2D
  - Megatron-LM CP       - Ring Attention         - Mesh-TF GSPMD
                         - Tutel SP

        ┌──────────────────────────────┬─────────────────────────────┐
        ▼                              ▼                             ▼
┌───────────────────────┐    ┌────────────────────────┐     ┌─────────────────────────┐
│  4. 稀疏/流式注意力结合SP│    │  5. 推理KV并行/缓存切分   │     │  辅助技术                │
└───────────────────────┘    └────────────────────────┘     └─────────────────────────┘
        │                         │                              │
        ▼                         ▼                              ▼
   - FlashAttn+SP             - vLLM PagedAttention         - ZeRO Stage3+SP
   - Block Sparse + SP        - Tensor Parallel + KV Split  - Mixed Parallel
   - StreamingLLM+SP          - SGLang KV Offload           - Checkpoint + SP
```
