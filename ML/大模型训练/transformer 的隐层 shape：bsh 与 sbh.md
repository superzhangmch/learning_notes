# transformer çš„éšå±‚ shape ç”¨ [B,S,H] è¿˜æ˜¯ [S,B,H] layout, å•¥åŒºåˆ«

megatron ç¬¬äºŒç¯‡ [ã€ŠEfficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LMã€‹](https://arxiv.org/pdf/2104.04473) 4.2 èŠ‚ï¼ˆè®¡ç®—ä¼˜åŒ–ä¸€èŠ‚ï¼‰è¯´ï¼š
> we changed the data layout in the transformer layer to avoid memory-intensive transpose operations, and to enable the use of strided batched GEMM kernels. Specifically, we changed the data layout from [ğ‘, ğ‘ , ğ‘, â„] to [ğ‘ , ğ‘, ğ‘, â„], where ğ‘, ğ‘ , ğ‘, and â„ are batch, sequence, attention-head, and hidden-size dimensions, respectively

ä¹Ÿå°±æ˜¯è¯´ä¸ºäº†ä½¿ç”¨ strided batched GEMM ä¹˜æ³•ï¼ˆå®ƒè¦æ±‚ [ğ‘ , ğ‘, ğ‘, â„] çš„è¾“å…¥ï¼‰ï¼Œç‰¹æ„ä» [B, S, H] è½¬åˆ°äº† [S, B, H]ã€‚è‹¥ä¸è½¬ï¼Œåˆ™ä½¿ç”¨ strided batched GEMM ä¼šæœ‰é«˜æ˜‚çš„ transpose æˆæœ¬ï¼ˆæ³¨æ„ï¼Œéä¸å¾—å·²ï¼Œåº•å±‚çš„å®ç°ä¸ç”¨åšç‰©ç†ä¸Šçš„ transposeï¼Œé‚£ä¹ˆè¿™é‡ŒæŒ‡çš„æ˜¯ç‰©ç†transposeï¼‰ã€‚ä»æ­¤ megatron å˜æˆäº† sequence first çš„ã€‚

è¿™ä¸€æ®µæ·±è¡¨å›°æƒ‘ã€‚ç‰¹ç ”ç©¶ä¸‹ã€‚

----

ä»¤ B=batch_sizeï¼Œ S=sequence_len, H=hidden_dim, transformer çš„ hidden shapeï¼Œå¯ä»¥æœ‰ä¸¤ç§é€‰æ³•ï¼š[B,S,H] (batch first) æˆ– [S,B,H] (sequence first)ã€‚

å¯¹äº transformer æ¥è¯´ï¼š
- åªæœ‰ softmax(QK')V' è¿™ä¸€æ­¥çœŸæ­£æ¶‰åŠåºåˆ—æ“ä½œï¼Œä»è€Œ [B,S,H]ã€[S,B,H] æœ‰åŒºåˆ«ã€‚
- å…¶ä»–åœ°æ–¹ï¼ŒåŒ…æ‹¬ attn çš„ projectionã€FNNã€layerNorm ç­‰ï¼Œ[B,S,H]ã€[S,B,H] æ²¡åŒºåˆ«ï¼šå®ƒä»¬éƒ½æ˜¯ token ç²’åº¦è®¡ç®—çš„ï¼Œæœ¬è´¨ä¸Š batch_size=B*Sã€‚ 

RNN æ—¶ä»£ hidden éƒ½æ˜¯ [S, B, H] å½¢å¼çš„ï¼ˆRNN/LSTM/GRU æ¨¡å—ï¼‰ã€‚transformer æ—¶ä»£ï¼Œä¸¤è€…éƒ½æœ‰ç”¨ï¼Œä¸»æµæ˜¯ [B,S,H]ï¼Œè€Œ megatron æ˜¯ç”¨çš„ [S,B,H] ï¼ˆä½†å†…éƒ¨ attn ä¼šè½¬æˆ [B, S, H]ï¼‰ã€‚ä½†æ— è®ºç”¨å“ªç§ï¼Œå½“å‰ï¼ˆ2025.08.17) transformer è®¡ç®— attn æ—¶ï¼Œ**éƒ½æ˜¯ç”¨çš„ [B,S,H]**, å½¢å¼ï¼Œå¦‚æœä¸æ˜¯ï¼Œåˆ™å…ˆè½¬æˆã€‚å¦‚å®åˆ™ megatron åšæ³•çœ‹èµ·æ¥å’Œå®ƒè¯´çš„ç›¸åï¼ˆä¸‹æ–‡è®²è¿™ç§çŸ›ç›¾).

----

ä¸‹é¢ä¸€æ¢å…·ä½“ä»£ç å±‚éƒ½æ˜¯æ€ä¹ˆå¤„ç†çš„ã€‚

å°†ä¼šçœ‹åˆ°è®¡ç®— attn æ—¶ï¼Œéƒ½ä¼šå˜æˆ [batch*num_head, seq, head_dim], å³ batch firstã€æ³¨æ„å› ä¸ºæ˜¯ multi heads, æ‰€ä»¥ H ä¼šæ‹†æˆ num_head ä¸ head_dim, ä¸”æŠŠ head_dim*batch ä½œä¸ºæ–°çš„å¤§batchã€‘ï¼š

### ï¼ˆ1ï¼‰ã€torch.nn.MultiheadAttention: æœ€ç»ˆç”¨ [B, S, H]

å®ƒé»˜è®¤æŠŠ input å½“ batch_first=False ï¼ˆ[S, B, H] å¤„ç†ã€‚çœ‹ä»£ç ï¼Œå¦‚æœä¸æ˜¯ï¼Œä¹Ÿä¼šå…ˆè½¬æˆ [S,B,H]ã€‚ä½†æ˜¯åˆ°äº†çœŸæ­£åš MHA çš„æ—¶å€™ï¼Œè¿˜æ˜¯ä¼šè½¬ [B,S,H]ï¼‰:

<img width="1368" height="600" alt="image" src="https://github.com/user-attachments/assets/233f5a9a-b626-453d-8032-f3b58c4ea689" />

å®ƒæ‰€ä»¥è¿™æ ·ï¼Œæ˜¯å†å²ä¼ ç»Ÿç»§æ‰¿ã€‚transformer ä¹Ÿæ˜¯åºåˆ—æ¨¡å‹ï¼Œå®ƒæ˜¯åœ¨æ¥å£å±‚é¢ï¼Œç»§æ‰¿äº† RNN æ—¶å€™çš„æƒ¯ä¾‹è€Œå·²ã€‚

### ï¼ˆ2ï¼‰ã€flashAttentionï¼šç”¨ [B, S, H]

flashAttention æ˜¯ batch first çš„ã€‚

### ï¼ˆ3ï¼‰ã€megatronï¼š attn ç”¨ [B, S, H]

megatron åœ¨å…¶ä»–åœ°æ–¹éƒ½ç”¨çš„ [S, B, H], ä½†æ˜¯ attn æœ€æ ¸å¿ƒå¤„ï¼Œä»ç„¶ç”¨çš„ [B, S, H] å½¢å¼ã€‚è‡³äºåŸå› ï¼Œè§ä¸‹æ–‡åˆ†æã€‚

<img width="1250" height="1134" alt="image" src="https://github.com/user-attachments/assets/ea0e0cd2-cd2b-497b-871d-411b0177c175" />

æ³¨æ„ç”¨äº†ï¼š torch.bmm/torch.baddbmm

æŒ‰å¼€å¤´æ‰€è¿°ï¼Œmegatron è¯´å› ä¸º..., æ‰€ä»¥è¦ç”¨ seq firstã€‚è¿™å’Œç°åœ¨æ‰€è§çš„å®é™… megatron ä»£ç ä¸ä¸€è‡´ã€‚

é‚£ä¹ˆï¼š
- æ˜¯ megatron ç ”å‘äººå‘˜åæ¥å‘ç°è¿˜æ˜¯ batch_first æ›´å¥½ï¼Œä½†æ˜¯é™äºæƒ¯æ€§ä¸å¥½æ”¹äº†ï¼Œäºæ˜¯æ•´ä½“ç”¨ [S, B, H], è€Œåœ¨ attn æ—¶ä¸´æ—¶ batch_first ä¸€ä¸‹å—? 
- è¿˜æ˜¯è¯´ä¸Šé¢ä»£ç ä¸­å‘ˆç°çš„ batch_first, èƒŒåå…¶å®å°±æ˜¯ï¼ˆæˆ–è€…è¯´"ä¹Ÿ"æ˜¯ï¼‰å®ƒ paper è¯´çš„æƒ…å½¢ï¼Ÿ

ç»¼åˆçœ‹ï¼ŒæŒ‡çš„æ˜¯åè€…ã€‚

----

### çŸ©é˜µä¹˜æ³•çš„ä¸€äº›çŸ¥è¯†

**ï¼ˆ1ï¼‰torch.bmmï¼š**

atch Matrix Multiplyï¼Œç›´æ¥åšæ‰¹é‡çŸ©é˜µä¹˜æ³•
```
A.shape = [Batch, M, K]
B.shape = [Batch, K, N]
output.shape = bmm(a,b).shape = [batch, M, N]
output = [A[i] Ã— B[i] for i in range(batch)]
```

noteï¼š

- torch.dot â†’ åªèƒ½ 1D å‘é‡ç‚¹ç§¯ã€‚ä¹Ÿå°±æ˜¯åªèƒ½ä¸¤ä¸ªå‘é‡ï¼ˆbatchsize=1ï¼‰ï¼Œ è‹¥è¦ batchsize > 1, torch ä¸­éœ€è¦ç»„åˆå¤šç®—å­æ¥å®ç°ã€‚
- torch.mm â†’ åªèƒ½ 2D çŸ©é˜µä¹˜ã€‚ä¹Ÿå°±æ˜¯ä¸¤ä¸ªçŸ©é˜µä¹˜æ³•ï¼Œæˆ–è€…è¯´ batchsize=1
- torch.bmm â†’ åªèƒ½ 3D æ‰¹é‡çŸ©é˜µä¹˜ã€‚batchsize > 1 æ—¶çš„çŸ©é˜µä¹˜æ³•

**ï¼ˆ2ï¼‰torch.baddbmmï¼š**

Batch Add + Matrix Multiplyï¼Œåœ¨çŸ©é˜µä¹˜çš„åŸºç¡€ä¸Šï¼Œè¿˜èƒ½æŠŠå·²æœ‰çš„çŸ©é˜µç»“æœåŠ è¿›å»ï¼ˆå¸¦ alpha/beta ç³»æ•°ï¼‰ã€‚åªæ˜¯ torch.bmm çš„ä¸€ä¸ªæ‹“å±•ã€‚
```
A.shape = [Batch, M, K]
B.shape = [Batch, K, N]
C.shape = [Batch, M, N] ä¸ºå·²æœ‰ç»“æœ
output = [Î²C[i] + Î±(A[i] Ã— B[i]) for i in range(batch)]
output.shape = torch.baddbmm(C, A, B).shape = [Batch, M, N]
```

**ï¼ˆ3ï¼‰strided batched GEMMï¼š**

å‰é¢ä¸¤ä¸ªéƒ½å¥½ç†è§£ã€‚strided batched GEMM æ˜¯è¿˜å¯ä»¥ä¸€æ¬¡å¤„ç†è¿™æ ·çš„çŸ©é˜µä¹˜æ³•ï¼š
```
A.shape = [M, Batch, K] è¿ç»­å­˜å‚¨
B.shape = [N, Batch, K] è¿ç»­å­˜å‚¨
output.shape = bmm(A,B').shape = [M, batch, N]
output = [A[:,i,:] Ã— B'[:,i,:] for i in range(batch)].trans_shape_to([M, batch, N])
```

å®ƒæ²¡æœ‰ torch çš„ç›´æ¥æ¥å£ï¼Œä½†æ˜¯ torch.bmm å†…éƒ¨ä¼šæŒ‰éœ€è°ƒç”¨å®ƒã€‚

å¯¹å®ƒï¼Œæ²¡æ·±å…¥ç ”ç©¶ã€‚

### çŸ©é˜µè½¬ç½®ç­‰å˜æ¢

**ä¸€ã€ä»€ä¹ˆæ˜¯çŸ©é˜µçš„è®¿é—®çš„ stridesï¼š**

ä¸€ä¸ªçŸ©é˜µ $A.shape = \[a, b, c, d\]$, å‡å¦‚æ˜¯è¿ç»­çš„:

åˆ™è®¿é—®å®ƒçš„ $A\[i,j,k,l\]$ å…ƒç´ çš„æ—¶å€™ï¼Œæ˜¯é€šè¿‡ strides å®Œæˆçš„ã€‚å®ƒçš„

$$strides = \[bcd, cd, d, 1\]$$

ä¸ºäº†è®¡ç®— $A\[i,j,k,l\] = data\[offset\]$, åªéœ€å†³å®š offset

$$\text{offset} = i \times (bcd) + j\times(cd) + k \times d + l$$

**äºŒã€å„ç§çŸ©é˜µè½¬ç½®äº¤æ¢æ“ä½œæ€ä¹ˆåšçš„ï¼š**

**a) permuteï¼š**

æ”¹å˜ç»´åº¦çš„é¡ºåºï¼Œæœ¬è´¨ä¸Šæ˜¯é‡æ–°å®‰æ’ strides çš„å¯¹åº”å…³ç³»ï¼Œä¸åšæ‹·è´ã€‚`transpose` äº¤æ¢ä¸¤ä¸ªç»´åº¦, ä¹ƒ `permute` çš„ç‰¹ä¾‹ã€‚

æ¯”å¦‚ `permute(0,2,1,3)` â†’ `[a,c,b,d]`ã€‚æ–° strides å°±æ˜¯æŠŠåŸæ¥çš„ `strides = [bcd, cd, d, 1]` æŒ‰ permute(0,2,1,3) ä½œé‡æ’ï¼Œä»è€Œå¾—åˆ°æ–° `strides = [bcd, d, cd, 1]`ã€‚

offset æ›´æ–°å…¬å¼å¯¹åº”æ¢ç»´ï¼š $\text{offset}(i,j,k,l) = i \times bcd + j \times d + k \times cd + l$

**b) reshape:**

å°è¯•ç”¨åŸ strides é‡æ–°è§£é‡Šæ•°æ®çš„å½¢çŠ¶ã€‚å¦‚æœæ–°å½¢çŠ¶ä¸åŸå†…å­˜å¸ƒå±€å…¼å®¹, åˆ™é›¶æ‹·è´ï¼›å¦åˆ™ PyTorch ä¼šåšä¸€æ¬¡æ‹·è´ç”Ÿæˆè¿ç»­çš„æ–°å¼ é‡ã€‚

æ¯”å¦‚ `[a,b,c,d] â†’ [a, b, cd]`ã€‚åŸ strides = `[bcd, cd, d, 1]`ï¼Œåˆå¹¶ c,d â†’ æ–° strides = `[bcd, cd, 1]`ã€‚

offset è®¡ç®—å…¬å¼ä¸ºï¼š $\text{offset}(i,j,k) = i \times bcd + j \times cd + k$

ä¸å…¼å®¹çš„æƒ…å†µï¼šæ¯”å¦‚ `transpose` åå† `reshape`ï¼Œä¼šå¯¼è‡´é‡æ–°åˆ†é…å†…å­˜ã€‚æ¯”å¦‚ä¸‹é¢ï¼š

èµ·ç‚¹æ˜¯ `shape=[a,b,c,d]`, `shape = [bcd, cd, d, 1]`ã€‚ç¬¬ä¸€æ­¥ï¼š`transpose(0,1)`ï¼Œå¾—åˆ° `shape =[b,a,c,d]`, `strides=[cd, bcd, d, 1]`ï¼Œæ­¤æ—¶å¼ é‡éè¿ç»­ï¼ˆä½†ç”¨èµ·æ¥ä»¿ä½›è¿ç»­ï¼‰ã€‚ç¬¬äºŒæ­¥ï¼šå°è¯• `reshape` åˆå¹¶ä¸­é—´ä¸¤ç»´æˆ `shape = [b, ac, d]`ã€‚

ä½†æ˜¯åˆå¹¶ä¸¤ç»´è¦æ»¡è¶³ stride ç›¸å®¹æ¡ä»¶ï¼š

$$
\text{stride}(\text{è¦åˆå¹¶çš„å‰ä¸€ç»´}) = \text{size}(\text{è¦åˆå¹¶çš„åä¸€ç»´}) \times \text{stride}(\text{è¦åˆå¹¶çš„åä¸€ç»´})
$$

ä»£å…¥å‘ç°æ˜¯ï¼š

$$
bcd \stackrel{?}{=} c \times d
$$

æ¡ä»¶ä¸æˆç«‹ã€‚æ‰€ä»¥ `(a,c)` è¿™ä¸¤ç»´åœ¨ç‰©ç†å†…å­˜ä¸­å¹¶ä¸ç´§é‚»ï¼Œæ— æ³•é›¶æ‹·è´åˆå¹¶ã€‚æ‰€ä»¥è¿™æ—¶å€™ä¼šè§¦å‘ **æ‹·è´**ï¼Œç”Ÿæˆæ–°çš„è¿ç»­å¼ é‡ã€‚

**c) æ€»ç»“ï¼š**
- permute/transposeï¼šä¸ä¼šæ‹·è´æ•°æ®ï¼Œåªæ˜¯é‡æ’ stridesï¼›offset å…¬å¼ = **æ–°ä¸‹æ ‡ Ã— å¯¹åº”çš„æ–° stride**ã€‚
- reshapeï¼šèƒ½å…¼å®¹æ—¶é›¶æ‹·è´ï¼ˆä¿®æ”¹ shapeã€è®¡ç®—æ–° stridesï¼‰ï¼Œå¦åˆ™è§¦å‘æ‹·è´å˜æˆæ–°çš„è¿ç»­å†…å­˜ã€‚
- è¿ç»­å¼ é‡ï¼šoffset = Î£ (index[dim] Ã— stride[dim])ã€‚

**ä¸‰ã€ä¾‹å­ï¼šä»€ä¹ˆæ—¶å€™ tensor çš„ reshape/transpose/permute ç­‰æ“ä½œä¼šå¯¼è‡´ç‰©ç†å†…å­˜ copy:**

ä¸æ˜¯æ¯ä¸€æ¬¡è°ƒç”¨éƒ½ä¼šå¯¼è‡´ å†…å­˜ copyï¼Œè€Œæ˜¯å¿…è¦æ—¶è¿›è¡Œã€‚ä¸¾å‡ ä¸ªæ¯”è¾ƒçœŸå®çš„ MHA ä¸­çš„ä¾‹å­ï¼š

ä¾‹å­ï¼ˆtorch.nn.MultiheadAttentionï¼Œ å…ˆè½¬ batch firstï¼‰ï¼š

```
# å‡è®¾è¾“å…¥
batch, seq, hid, num_heads = 2, 5, 16, 4
head_dim = hid // num_heads
q = torch.randn(batch, seq, hid)             # [batch, seq, hid], contiguous

q = q.transpose(0, 1)                        # [seq, batch, hid], zero-copy (stride æ”¹å˜)
# æ— è®ºæ€æ ·éƒ½å…ˆå˜æˆ [seq, batch, hid]ã€‚torch.nn.MultiheadAttention å°±æ˜¯è¿™æ ·ã€‚ç„¶åä½œå¤š head åˆ‡åˆ†

q = q.reshape(seq, batch, num_heads, head_dim)  # [seq, batch, num_heads, head_dim], zero-copy
q = q.reshape(seq, batch * num_heads, head_dim) # [seq, batch*num_heads, head_dim], âš ï¸ copy
q = q.reshape(batch * num_heads, seq, head_dim) # [batch*num_heads, seq, head_dim], zero-copy
```

å¦ä¸€ä¸ªä¾‹å­(æ€»æ˜¯ç¬¬ä¸€ç»´æ˜¯ batch)ï¼š

```
batch, seq, hid, num_heads = 2, 5, 16, 4
head_dim = hid // num_heads
q = torch.randn(batch, seq, hid)                # [batch, seq, hid], contiguous

q = q.view(batch, seq, num_heads, head_dim)     # [batch, seq, num_heads, head_dim], zero-copy
q = q.permute(0, 2, 1, 3)                       # [batch, num_heads, seq, head_dim], zero-copy
# => [B, num_heads, seq, head_dim]

q = q.reshape(batch * num_heads, seq, head_dim) # [batch*num_heads, seq, head_dim], âš ï¸ copy
```

å†ä¸€ä¸ªä¾‹å­ï¼ˆmegatronï¼‰ï¼š

```
batch, seq, hid, num_heads = 2, 5, 16, 4
head_dim = hid // num_heads
q = torch.randn(seq, batch, hid)                 # [seq=S, batch=B, hid=H], contiguousï¼Œ megatron æƒ…å†µã€‚strides=[B*H, H, 1]

q = q.view(seq, batch, num_heads, head_dim)      # [seq, batch, num_heads, head_dim], zero-copy (hid æ‹†æˆ H=num_heads*head_dim)ï¼Œè¿ç»­ã€‚strides=[B*H, H, head_dim, 1]
q = q.reshape(seq, batch * num_heads, head_dim)  # [seq, batch*num_heads, head_dim], zero-copyï¼Œè¿ç»­ã€‚ strides=[(B*num_head)*head_dim, head_dim, 1]=[B*H, head_dim, 1]

# ä¸‹é¢è½¬æˆäº† [B, seq, head_dim] 
q = q.permute(1, 0, 2)                           # [batch*num_heads, seq, head_dim], zero-copy (stride æ”¹å˜)ï¼Œä¸è¿ç»­ã€‚strides=[head_dim, B*H, 1]
```

è¿™ä¸ªä¾‹å­é‡Œï¼Œå…¨ç¨‹æ²¡æœ‰å‘ç”Ÿå†…å­˜ copyã€‚
- å¦‚æœåœ¨ q.shape = [seq, B=batch * num_heads, head_dim] çš„æ—¶å€™å°±æ‰“ä½ï¼Œkã€v ä¹ŸåšåŒæ ·æ“ä½œï¼šé‚£ä¹ˆå°±ç›¸å½“äºåœ¨ [S, B, H] shape ä¸Šç›´æ¥åš attentionï¼Œè¿™æ˜¯å¯ä»¥ç›´æ¥ç”¨ strided batched GEMM å¤„ç†çš„ã€‚
- å¦‚æœåƒ megatron é‚£æ ·ï¼Œå† permute æˆ [B, S, H], è¿™æ—¶å€™å†…å­˜ä¸è¿ç»­ï¼Œä½†æ˜¯æ²¡å‘ç”Ÿcopyã€‚å¦‚æœ kã€v ä¹ŸåŒæ ·å¤„ç†ï¼Œé‚£ä¹ˆ torch.bmm çš„æ—¶å€™ï¼Œï¼ˆçŒœæµ‹å¯èƒ½ï¼‰ä¹Ÿæ˜¯ç›´æ¥ç”¨ strided batched GEMM æ¥å®ç°äº†ã€‚
- ç»¼ä¸Šå¯ä»¥çœ‹åˆ° megatron sequence-first çš„é“ç†ï¼šä¸ç®¡ç®— attn æ—¶æ˜¯ BSH è¿˜æ˜¯ SBHï¼Œéƒ½æ²¡å‘ç”Ÿå†…å­˜ copyã€‚
  - ç®— attn æ—¶æ˜¯ $Q\times K^T$, è€Œ $K^T$ å¥½åƒå¯èƒ½éœ€è¦å†…å­˜ copy æˆ contiguousã€‚

### æ€»ç»“

é‰´äºä¸Šé¢æœ€åä¸€æ®µæ‰€è¯´ï¼Œé‚£ä¹ˆ megatron çš„ [S, B, H] çš„å¥½å¤„ï¼Œçœ‹æ¥å°±æ˜¯ MHA çš„æ—¶å€™ï¼Œ [S, B, H] => [S, B, num_head, head_dim] => [S, B1 = B*num_head, H1=head_dim] => [S, B1, H1] => [B1, S, H1] è¿™ä¸€è·¯éƒ½æ²¡å†…å­˜ copyã€‚è€Œæœ‰å®ƒä¹‹åå¯ä»¥ç”¨ strided batched GEMM è®¡ç®—ã€‚è€Œ [B, S, H] => [B1, S, H1] è¿‡ç¨‹ä¸­ï¼Œå¿…ç„¶æœ‰å†…å­˜ copyã€‚
