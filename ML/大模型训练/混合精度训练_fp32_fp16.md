
<img width="1314" height="566" alt="image" src="https://github.com/user-attachments/assets/e4150de3-2ab6-4f25-af88-973ed3cc3ae2" />

混合精度下
- forward、backward 都是在 fp16 下进行的。这样激活值/中间值，也都是 float 16 的形式。
  - 少部分操作如 Softmax、LayerNorm、Log、Exp、求和、归一化等，仍保持 float 32
- loss 本身需要是 fp32 的
- loss scale 操作：为了防止 loss 过小，导致 float 没法表示（underflow）。还会有 loss rescale 操作。在 backward 算出梯度后，需要 un-scale。这个scale 值要动态调整，使得梯度既不因太小而 underflow，也不因太大而 overflow。
- 在参数更新部分，则仍用 float32 （训练一个 iter 只三步：fowrad, backward, update）

### 计算量的好处

训练一个 iter 共三步：fowrad, backward, update， 计算主要集中在前两步，且都是 fp16 的了，所以混合精度降低计算开销很大。

### 对存储占用的分析

（1）固定部分

和 fp32 训练比没变化：
- 参数部分要存两份（fp16+fp32），变1.5倍。
- 优化器状态没变，仍然是 fp32（adam 下，乃参数的2倍存储）
- 另外还有 fp16 的梯度部分。

则对于每一个参数：以上需要 16个字节的显存。而非混合的 fp32 训练，可知参数+优化器状态+梯度还是 16 字节。

（2）对于动态部分显存

激活、以及一些中间值，都是fp16 存储的。少用一半。
