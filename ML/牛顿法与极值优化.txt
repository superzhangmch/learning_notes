牛顿法用于解方程。求极值可用之，因：极值处一阶导为0，从而可据牛顿法解出零点。

从求极值角度，牛顿法可理解为（以单参数函数为例）：在被迭代点处，可展开为二阶导的近似:f(x_0+t) = f(x_0) + f1(x_0) * t + 0.5* f2(x_0) *t*t + o(..)。【假设f1，f2分别是一二阶导】此近似在x_0的邻域内，是很好的一个近似。而牛顿法求极值的迭代一次后值，正是该2次近似函数的极值点（实际上，用二项式的配方法可得同样结果：t=-f1 / f2）。
所以才说，牛顿法是二次拟合，每次迭代都是在：先拟合一个二次函数，然后找该函数极值，然后在新极值处重复该过程。与之相对，梯度下降优化法，则是用切线来拟合函数，拟合完后，并不能一次跨越太远（否则一步就超越了极值了咋办）。

如果换成多变量函数，则也类似：牛顿法是在候选点先用“该点的二阶展开这样一个二次曲面“来拟合原函数，然后一次迭代就是找二次曲面的极值点，然后用该极值点作为新的候选点再次重复，直到得解。（同样是新步长t= - f1/f2, 不过这里f2是hessian矩阵了。求逆矩阵代价大，所以才有了各种“拟牛顿法”的优化）

二次曲面的拟合能力，显然好于一次切面。所以说，牛顿法是比梯度下降法速度更快的。

参：https://en.wikipedia.org/wiki/Newton's_method_in_optimization
