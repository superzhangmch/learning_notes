# 关于训练 LOSS 不降

深度神经网络训练的唯一目的，是在测试集上能表现良好。训练的唯一途径，是通过一次次梯度下降，令损失函数 cost 逐步变小。但是，cost 能变小，不一定test集上也会变好，所以需要处理好过拟合问题。但是如果cost都不能变小，test集上效果就更谈不上了。

<br>

近来训练有几次神经网络，发现样本数量比较多的时候（比如几百万，几千万），很容易出现这样的问题：才训练了几十万样本，损失函数 loss 已经趋于平稳，不怎么下降了——即使调小学习率直到很小很小也没用——而即使训练集上的预测结果也并不理想，更别说测试集上的效果。经过探究，得到了以下的经验。

<br>

**即使数据比较脏乱，只要神经网络深度够深，batch size 够大，容易使得在训练集上训得cost 很小**。

解释：（这里假设学习率总是正确选择了）深度够深，说明函数拟合上可以有更大的拟合能力。毕竟训练集再大还是有限样本集，所谓函数拟合也只是有限集上的拟合，只要样本没有标注不一致的矛盾之处，可以说cost可以想多接近0，就多接近0。batch size 如果就等于样本数量，那么就是在整体数据集上——也就是函数的整体定义域上——作函数优化，配合足够复杂的拟合函数，当然容易令cost趋于0（note：拟合函数不足够复杂，batch size较大，反而容易陷入局部最优；这里只是为了表达一种意思而已。batch size大比起小，更容易稳定梯度方向）.

回到我遇到的那几个问题，假如就是一心想降低cost。怎么调学习率都没用的时候，很可能是下面问题：
1. 数据太脏乱。极端情况，假设是0/1分类，如果每种情况都有同样数量的矛盾标注，那么无疑任何学习算法都不可能学出比较小的 cost。
2. 神经网络深度不够。否则函数拟合能力不够。比如数据是圆形分布的，却给了个线性函数，当然不可能拟合好。
3. batch size太小。极端情况，batch_size=1, 如果数据不太好，梯度东一头西一头，难以纠偏。

至于调学习率（loss不降的时候调下学习率），model超参数，就不细说了。

实际中，网络深度总是被恰当控制的，batch size往往是十几到几百的样子。结合上面可知，在这种情况下，可以说 cost 就是有一定的下界存在的，无论怎么训都超不了这个下界。

另外，当有cost不降，或者效果不收敛的情况时，可以先在一个很小的数据上试试。
