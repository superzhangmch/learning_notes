# vits 《Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech》 https://arxiv.org/pdf/2106.06103

下面是整体流程：

![image](https://github.com/user-attachments/assets/33c57b25-6448-4f9e-b9b6-a714f950eb51)

**基本原理是：**
- 对 audio wave 训练出 VAE 模型，从而可以采样 z 并经 decoder 生成 audio。但这样的 audio 是随机的，为了生成可控 audio（text 的 audio），需要在训练时以 text 为条件去预测对应的 audio 的 latent-z。这样在生成时就可以 `text => 特定 latent-z => 特定 audio`。
- text tokens list 与 audio frames list 需要作对齐，所以训练时引入了 monotonic alignment search（单调对齐搜索）方式作对齐。在生成时，需要能知道每个 text token 的 audio 时长，所以需要时长预测，因此才需要训练一个 duration predictor——又因为人说话时节奏并不恒定，所以用了"随机时长 predictor"。为引入随机性，采用的是 noise => duration 的flow-based 的方法（noise 是随机的，所以 duration 有随机性）。

![image](https://github.com/user-attachments/assets/66f5cef3-2806-4c4b-a1d6-86af68b0fe0d)

### loss

![image](https://github.com/user-attachments/assets/75b31d71-f681-4773-a2d4-44fef660f9e4)

如上是和标准 VAE 的对比。重建 loss 换成了 $L_1$, 而 KL 散度则是多了条件 c = [text, Align]。loss 拆解项为：

![image](https://github.com/user-attachments/assets/4ad17118-9daa-40d5-8c5a-26af2722753f)

对应到 model 图为：

![image](https://github.com/user-attachments/assets/53cc5f9d-379b-4891-9254-b3dca5494888)

note:
- 重建用的 mel 谱，但是有些地方用的线性谱，而非非线性的 mel-谱。
- text-audio 对齐：text 切分成 tokens 长度，和生成的 audios 的某种 frames 的长度不一样，且不能线性对应，所以需要类似 CTC 之类的东西来做对齐。vits 训练时用的是"单调对齐搜索"的方式，它本身只是一个对齐算法，本身没参数，本身不需要训练。但是对齐结果可以用来当场统计出每个 text token 的时长，从而用于训练 "时长预测器"。而 inference的时候，根据时长预测结果，把 text tokens 的 hidden 表示(mu, sigma) 做 repeat 即可。
- 用到了 GAN loss。
- f_theta(z) 以及 stochastic duration predictor 都是可逆的，所以图中箭头方向在 train / infer 中是反着的。

### 实现

它生成时，decoder 是 latent-z 一步生成 audio wave，而没经过 mel 谱（所用 hifi-gan 结构原本就是 `mel-谱 => audio wave` 的）。

![image](https://github.com/user-attachments/assets/985b98db-b1b1-4e88-a055-360d15de2969)

**MSA 对齐：**

最终可以归结为一段代码一个算法，不含参数。

![image](https://github.com/user-attachments/assets/f975f837-3aad-41a4-862b-fe3bd3cbc218)

**随机时长预测：**

![image](https://github.com/user-attachments/assets/125e21f5-ef94-4891-b79d-4f7237892d2f)

上面是训练图（infer 时只需要其中一部分，红线流所示）。内部有两个 flow-based 机构的东西存在：posterior encoder 与 flow $g_\theta$, 但是前者只是借用了 flow-based 结构做了潜在的分布转移，而后者是典型 flow-based model。图中两个 noise 第一个是 需要真实采样的 input，后一个表示期望 g(.) 的输出是正态 noise。这块构成了总 loss 中的 L_dur。

paper 中说的 “variational dequantization / variational data augmentation”, 对应图中的 posterior encoder。原始 duration 乃标量的整数，所以需要用 posterior encoder 转为稠密表示。

**speaker 音色控制：**

通过 speaker embedding 来控制说话人的声音特征。为每个说话人添加一个 speaker embedding，这个 embedding 用来标识说话人的身份，并且被嵌入到模型的多个模块中。

speaker embedding 通常是通过一个外部模块生成的（speaker encoder）。speaker encoder 是一个神经网络，使用 CNN 或 LSTM 等结构，接收原始音频信号作为输入，并输出一个固定长度的向量，作为该说话人的表示。

----

### 知识补充：关于文中的 flow-based model

文中两处用到。它用的是 **Affine Coupling Layer**（RealNVP《Density estimation using Real NVP》, Glow）法。具体做法是（部分内容来自 AI）：

把输入 $x$ 拆成两部分： $$x = [x_a, x_b]$$ ， 然后只对一部分做变换：

$$
\begin{cases}
y_a &= x_a \\
y_b &= x_b \cdot \exp(s(x_a)) + t(x_a)
\end{cases}
$$

其中 $s(x_a)$, $t(x_a)$ 是由神经网络输出的 scale 和 shift， $y = [y_a, y_b]$

特点是：
- 易于求逆： $$x_b = (y_b - t(x_a)) \cdot \exp(-s(x_a))$$
- Jacobian 是三角形式，行列式只需要计算 $s(x_a)$ 的和： $$\log |\det J| = \sum s(x_a)$$

鉴于只有一半维度做了变换，所以多个 layer 之间要交错变换其中的一半维度。

**训练目标：**

极大似然估计方式最大化原始数据 x 的 log 思然（而 loss = -该式)： $\log p_X(x) = \log p_Z(f(x)) + \log \left| \det \left( \frac{\partial f(x)}{\partial x} \right) \right|$，该公式是x 与 f(x) 之间的概率密度的变换公式，其形式有数学上的严格保证，其中 $p_X(x)$ 是原始数据 x 的概率密度， $p_Z(.)$ 是正态分布密度函数，f(.) 是多层的 affine coupling layer 的组合。训练结束后，期望 f(用户书序）~ N(0, I)。因为 f 可逆，所以采样一个正态分布数据 z ~ N(0, I)，则 $f^{-1}(z)$ 就是生成结果了。

为什么会最终 f(.) ~ N(0, I)? 因为 $p_Z(.)$ 是正态分布，如果换成其他，则最终 f(x) 拟合到的就是其他分布。这样通过极大似然，把 f(x) 拉向了 N(0, I) 的高密度区，从而实现了概率分布的转换映射。

note：对于 vits $f_\theta(z)$ 来说，它的网络结构与流程和标准 flow-based model 一样，但是它并不是 x -> N(0, I) 的分布转换，没通过上述极大似然方式来训，而是串进了整个流程。

**和其他 model 区别:**

扩散模型，以及 flow-matching 在训练的时候，会明确地采样一个噪声（正态 noise），预测这个噪声。而上面的 flow-based model，虽然生成的时候会采样 noise，但是训练的时候，并没 noise 参与。
