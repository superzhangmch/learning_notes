# transformer çš„éšå±‚ shape ç”¨ [B,S,H] è¿˜æ˜¯ [S,B,H] layout, å•¥åŒºåˆ«

ä»¤ B=batch_sizeï¼Œ S=sequence_len, H=hidden_dim, transformer çš„ hidden shapeï¼Œå¯ä»¥æœ‰ä¸¤ç§é€‰æ³•ï¼š[B,S,H] (batch first) æˆ– [S,B,H] (sequence first)ã€‚

å¯¹äº transformer æ¥è¯´ï¼š
- åªæœ‰ softmax(QK')V' è¿™ä¸€æ­¥çœŸæ­£æ¶‰åŠåºåˆ—æ“ä½œï¼Œä»è€Œ [B,S,H]ã€[S,B,H] æœ‰åŒºåˆ«ã€‚
- å…¶ä»–åœ°æ–¹ï¼ŒåŒ…æ‹¬ attn çš„ projectionã€FNNã€layerNorm ç­‰ï¼Œ[B,S,H]ã€[S,B,H] æ²¡åŒºåˆ«ï¼šå®ƒä»¬éƒ½æ˜¯ token ç²’åº¦è®¡ç®—çš„ï¼Œæœ¬è´¨ä¸Š batch_size=B*Sã€‚ 

å…ˆè®°ä¸‹ç»“è®ºï¼šæ— è®ºç”¨å“ªç§ï¼Œå½“å‰ï¼ˆ2025.08.17) transformer è®¡ç®— attn æ—¶ï¼Œ**éƒ½æ˜¯ç”¨çš„ [B,S,H]**, å½¢å¼ï¼Œå¦‚æœä¸æ˜¯ï¼Œåˆ™å…ˆè½¬æˆã€‚

RNN æ—¶ä»£éƒ½æ˜¯ [S, B, H] å½¢å¼çš„ï¼ˆRNN/LSTM/GRU æ¨¡å—ï¼‰ã€‚transformer æ—¶ä»£ï¼Œä¸¤è€…éƒ½æœ‰ç”¨ï¼Œä¸»æµæ˜¯ [B,S,H]ï¼Œè€Œ megatron æ˜¯ç”¨çš„ [S,B,H] ï¼ˆä½†å†…éƒ¨ attn ä¼šè½¬æˆ [B, S, H]ï¼‰ã€‚

----

ä¸‹é¢ä¸€æ¢ä¸‹ã€‚ æ³¨æ„å› ä¸ºæ˜¯ multi heads, æ‰€ä»¥ H ä¼šæ‹†æˆå¤šå¤´ï¼Œä»è€Œå®é™…æ˜¯ [B, S, n, h] æˆ– [S, B, n, h]ã€‚

### ï¼ˆ1ï¼‰ã€torch.nn.MultiheadAttention: æœ€ç»ˆç”¨ [B, S, H]

å®ƒé»˜è®¤æŠŠ input å½“ batch_first=False ï¼ˆ[S, B, H] å¤„ç†ã€‚çœ‹ä»£ç ï¼Œå¦‚æœä¸æ˜¯ï¼Œä¹Ÿä¼šå…ˆè½¬æˆ [S,B,H]ã€‚ä½†æ˜¯åˆ°äº†çœŸæ­£åš MHA çš„æ—¶å€™ï¼Œè¿˜æ˜¯ä¼šè½¬ [B,S,H]ï¼‰:

<img width="1368" height="600" alt="image" src="https://github.com/user-attachments/assets/233f5a9a-b626-453d-8032-f3b58c4ea689" />

å®ƒæ‰€ä»¥è¿™æ ·ï¼Œæ˜¯å†å²ä¼ ç»Ÿç»§æ‰¿ã€‚transformer ä¹Ÿæ˜¯åºåˆ—æ¨¡å‹ï¼Œå®ƒæ˜¯åœ¨æ¥å£å±‚é¢ï¼Œç»§æ‰¿äº† RNN æ—¶å€™çš„æƒ¯ä¾‹è€Œå·²ã€‚

### ï¼ˆ2ï¼‰ã€flashAttentionï¼šç”¨ [B, S, H]

flashAttention æ˜¯ batch first çš„ã€‚


### ï¼ˆ3ï¼‰ã€megatronï¼š attn ç”¨ [B, S, H]

megatron åœ¨å…¶ä»–åœ°æ–¹éƒ½ç”¨çš„ [S, B, H], ä½†æ˜¯ attn æœ€æ ¸å¿ƒå¤„ï¼Œä»ç„¶ç”¨çš„ [B, S, H] å½¢å¼ã€‚è‡³äºåŸå› ï¼Œè§ä¸‹æ–‡åˆ†æã€‚

<img width="1250" height="1134" alt="image" src="https://github.com/user-attachments/assets/ea0e0cd2-cd2b-497b-871d-411b0177c175" />

----

### å…³äº megatron

megatron ç¬¬äºŒç¯‡ [ã€ŠEfficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LMã€‹](https://arxiv.org/pdf/2104.04473) 4.2 èŠ‚ï¼ˆè®¡ç®—ä¼˜åŒ–ä¸€èŠ‚ï¼‰è¯´ï¼š
> we changed the data layout in the transformer layer to avoid memory-intensive transpose operations, and to enable the use of strided batched GEMM kernels. Specifically, we changed the data layout from [ğ‘, ğ‘ , ğ‘, â„] to [ğ‘ , ğ‘, ğ‘, â„], where ğ‘, ğ‘ , ğ‘, and â„ are batch, sequence, attention-head, and hidden-size dimensions, respectively

ä¹Ÿå°±æ˜¯è¯´ä¸ºäº†ä½¿ç”¨ strided batched GEMM ä¹˜æ³•ï¼ˆå®ƒè¦æ±‚ [ğ‘ , ğ‘, ğ‘, â„] çš„è¾“å…¥ï¼‰ï¼Œç‰¹æ„ä» [B, S, H] è½¬åˆ°äº† [S, B, H]ã€‚è‹¥ä¸è½¬ï¼Œåˆ™ä½¿ç”¨ strided batched GEMM ä¼šæœ‰é«˜æ˜‚çš„ transpose æˆæœ¬ï¼ˆæ³¨æ„ï¼Œéä¸å¾—å·²ï¼Œåº•å±‚çš„å®ç°ä¸ç”¨åšç‰©ç†ä¸Šçš„ transposeï¼Œé‚£ä¹ˆè¿™é‡ŒæŒ‡çš„æ˜¯ç‰©ç†transposeï¼‰ã€‚ä»æ­¤ megatron å˜æˆäº† sequence first çš„ã€‚

ä½†æ˜¯æ­£å¦‚ä¸Šé¢æ‰€ç¤ºï¼Œmegatron åæ¥ç‰ˆæœ¬çš„ attention è®¡ç®—ï¼Œå¹¶æ²¡ç”¨ sequence first æ–¹å¼ã€ä»£ç åº“ä¸­ä¹Ÿæ²¡æœåˆ° strided batched GEMMæœ‰å…³ä¸œè¥¿ã€‚ strided-batched-gemm æ²¡æœ‰ torch ç›´æ¥å‡½æ•°ï¼Œæœ‰ä¸ª cublas çš„å®ç° cublasGemmStridedBatchedExã€‘ã€‚è€Œæ˜¯ç”¨äº† batch first çš„  torch.bmm/torch.baddbmmã€‚

é‚£ä¹ˆï¼š
- æ˜¯ megatron ç ”å‘äººå‘˜åæ¥å‘ç°è¿˜æ˜¯ batch_first æ›´å¥½ï¼Œä½†æ˜¯é™äºæƒ¯æ€§ä¸å¥½æ”¹äº†ï¼Œäºæ˜¯æ•´ä½“ç”¨ [B, S, H], è€Œåœ¨ attn æ—¶ä¸´æ—¶ seq-first ä¸€ä¸‹å—?
- è¿˜æ˜¯è¯´ä¸Šé¢å‘ˆç°çš„ batch_first çš„ torch.bmm/torch.baddbmm å†…éƒ¨ï¼Œå®ƒåˆè¦ç»™å†è½¬æˆç‰©ç†è¿ç»­çš„ seq-firstï¼ˆè€ŒåŸç”Ÿ[S, B, H] åœ¨æ­¤å¤„è½¬çš„æ—¶å€™æ— æˆæœ¬ï¼‰ï¼Ÿ

