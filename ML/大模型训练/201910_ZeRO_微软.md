#《ZeRO: Memory Optimizations Toward Training Trillion Parameter Models》 https://arxiv.org/pdf/1910.02054

ZeRO 乃 [deepspeed](https://github.com/microsoft/deepspeed) 背后的技术

ZeRO 分为两种
- ZeRO-DP：关注的是train时常驻显存（参数、梯度、优化器状态，统称为 model states）的优化。
  - DP=数据并行的
- ZeRO-R：关注的是非常驻显存（包括 激活/temporary buffer/显存碎片等，统称为 residual memory）的优化。
  - R=residual memory。
    - residual=剩余的, 残留的

----

## ZeRO-DP

ZeRO-DP 优化的是fp16+fp32 混合精度训练场景（该混合态训练见[here](https://github.com/superzhangmch/learning_notes/blob/main/ML/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83_fp32_fp16.md)）。fp16+fp32 混合精度训练大概说来，显存占用时：fp32的优化器+fp32参数、fp16的梯度，每一个参数需要 16 个字节。

有三个 ZeRO-level（逐步把"fp32的优化器+参数、fp16的梯度、fp16的参数"分散到参与数据并行的节点上），逐渐更节约显存，下图是每个节点上的显存占用情况：

<img width="1986" height="824" alt="image" src="https://github.com/user-attachments/assets/1291711c-1571-4cfd-aa1d-b4bd271b8bf5" />

<img width="824" height="726" alt="image" src="https://github.com/user-attachments/assets/1e8c17a5-e7a7-4d8b-9919-6957300e5fc1" />

如果是非混合精度训练，则 zero-1， zero-2，就把该优化的优化了，而 zero-3 对应显存不存在。

| level | 优化内容 | 内存节省效果 | 通信变化 | 同步机制 |
| ----- | ----- | ----- | ----- | ----- |
| **ZeRO-1** | fp32的优化器状态+fp32参数 | $2Ψ +2Ψ+ \frac {kΨ} {N_d}$ | 1.5× DP | scatter_reduce + all_gather+all-Gather |
| **ZeRO-2** | 追加：fp16梯度 | $2Ψ +\frac {2+kΨ} {N_d}$  | 没变 | 仍可化为 reduce_scatter + all_gather |
| **ZeRO-3** | 追加：fp16参数 | $\frac {2+2+kΨ} {N_d}$ | 1.5× DP | scatter_reduce + all_gather(前向） + scatter_reduce（反向） |

Zero-DP 可以优化显存占用，通过把集中式、多副本存放的东西分散到数据并行的多个节点上，但是居然通信量增加可控。也就是说，这种优化并没特别折损什么，完全是不做白不做的。

#### （1）zero-1（P_os)： 分开存 fp32 的优化器状态与参数

每个 DP node 都有完整的 fp16 梯度与 fp16 参数，所以每个 node 本都可以作全部参数更新的。而在 zero-1 下，每个 node 只存储并只更新这一部分 fp32 参数，从而只能推断出这一部分的 fp16参数。为了得到其他 node 的 fp16 参数部分，需要多做一个 all-gather（全量 fp32，只有导出model时用）。

<img width="1666" height="888" alt="image" src="https://github.com/user-attachments/assets/42bcf105-872b-4841-82c4-a3922ba69bb2" />

#### （2）zero-2(P_os+g)：分开存 fp16 的梯度

zero-1 每个 DP node 只需更新一部分参数，只需要这一部分的fp16 梯度就行了，很自然地，没必要存下所有fp16 梯度，这就是 zero-2。

这样做还有一个好处是：zero-1 增加的通信量终于可以在 zero-2 找到地方补偿了，从而使得通信量和普通 DP 一样。为啥呢？

不用 zero 的普通 DP，需要同步的是 fp16 梯度，用的是 ring-all-reduce 这样的操作，它分为两步：scatter-reduce（使得有一个节点有reduce的（这里是梯度累加）最终结果） 与 all-gather（最终结果传播到所有节点）。而用了 zero-2，对梯度作 scatter-reduce 就行了，不需要 all-gather 了。只是需要把 fp16 的参数通过 all-gather 同步下而已。all-gather 增一个减一个，整体上仍然是 scatter_reduce + all_gather，所以通信量不变。

<img width="1662" height="926" alt="image" src="https://github.com/user-attachments/assets/72379870-09de-4809-bbed-0735db9e707b" />

其他技术补充：
- 梯度在跨节点传的时候会用 bucketization 策略：数据聚合后再通信，是各框架都爱做的策略，乃一惯常优化手段。否则的话，每一个梯度 tensor 都要做一次单独网络传输，相比批量传大buffer数据，开销较大，故可把多个tensor打包到一起传输。每聚够一桶后才传输，桶越大，越有规模效应，但占据显存也较大，故桶大小需 trade off。
  - deep speed 中对应配置是 `allgather_bucket_size 与reduce_reduce_size`(不过这两个用于deepspeed 内所有all-gather 与 reduce，不只梯度)。
- 计算与通讯重合：这也是一个惯常优化方法。指的是 backward 时，每算出一层的梯度，可以马上让他去做 reduce，同时并行着可以启动上一层的梯度计算。这两步时间上重叠，可缩短总用时。
  - deepspeed 中的配置 `overlap_comm` 与此有关。

#### （3）zero-3（P_os+g+p): 分开存 fp16 的参数

zero-2 下，每个 DP node 的 fp16 参数仍然存了全量，如果只存部分，那 zero-2 的 all-gather 也可以免了。这样通信量还能减少呢。

但是，下一个 iteration，本 node 上在 forward、backward 时需要用到全量 fp16 参数啊，这只需要用的时候，用 all-gather 拿过来就行了。一共是 scatter_reduce + all_gather + scatter_reduce + all-gather，所以是 1.5 倍的普通 DP 通信量。

<img width="1654" height="918" alt="image" src="https://github.com/user-attachments/assets/14c2741e-a618-40e3-b115-5d16eb0fe374" />

note：
- 集合网络交互原语
  - scatter-reduce：使得某一个节点有reduce 后的最终结果
  - all-gather：把一个节点有的数据，传播到所有节点。
  - 以上两个操作的网络通信量都可以做到和完整数据一样大（用 ring-XX 或 tree-XX 的方法），所以都按1倍数据大小算。
- 对应到 deepspeed
```  
  {
    "zero_optimization": {
        "stage": 1, # 0:关闭；1,2,3 代表 zero-1, zero-2, zero-3
        ...
    }
}
```

---

## ZeRO-R

### （1）、激活显存拆分（只用于张量并行 + 启用 activateion checkpoint 时）

在张量并行（MP）中，且启用 activateion checkpoint（也就是 re-computation）时，激活张量（activations）通常会在多个 GPU 上被完整复制，导致显存里有大量重复的激活数据。对于大模型，这部分内存占用非常可观。

ZeRO 发现可以：把 act-checkpoint 准备齐全，才能继续做 re-compute，而re-compute 的时间远大于激活准备时间。所以可以把这些 act-checkpoint 按模型并行的维度进行分区存储，每个 GPU 只保留自己对应分区的激活。当反向传播需要某个激活时，通过 all-gather（即分散各处的数据同步到每个地方） 在多个 GPU 间动态重构完整的激活。甚至可以把这些分区后的激活检查点卸载到 CPU 内存，这样显存占用几乎为零，但会增加 GPU–CPU 数据传输开销。

通信开销：在反向传播中 需要额外一次 all-gather 来重构被分区存储的激活。这次 all-gather 的通信量就是 message_size，在 LLM transformer 中，相当于原来通信量的不到 1/10（因为原来每 block 是 12 × message_size）。所以 通信开销增加很小（<10%），但换来了 MP 倍数的激活内存节省。

megatron 用了 MP 并行，所以按说它可以用来优化 magetron。megatron-3 中，用seq并行+选择性重计算方式，使得这里的方法的作用已不大。所以这里不展开上面的 1/10 到底怎么算出来的了。
 
### （2）、固定大小的显存 buffer

为了获得更好的效率，一般（如 NVIDIA Apex 或 Megatron）会在执行操作前，将所有参数融合（fuse）到一个单一的缓冲区中，再进行操作。也就是这个 buffer 是用于存放临时数据的临时缓冲区。buffer 大还是小，需要时间 vs 空间的 trade-off。ZeRO 的策略是用固定的 buffer，而 apex 与 megatron 是空间换时间用大 buffer。

### （3）、显存碎片

长短存活周期的显存碎片交错，导致显存利用率不高。所以可以区分分配的显存的存活周期，根据其长短弄出两类 buffer分别存储。

长短划分：比如，采用 activateion checkpoint 时，checkpoint是长期存储，而重算的激活值是短期的。activation 梯度（中间梯度，非最终的）是短期的，而参数梯度是长期的。

解法是作实时碎片整理，把长存活的数据放置到连续存储空间。

deepspeed 中的体现, 比如：
```
"zero_optimization": {
    "stage": 2,
    "contiguous_gradients": true, # For example, we have enabled contiguous_gradients to reduce memory fragmentation during backward pass. 
...

----

 "activation_checkpointing": {
    "contiguous_memory_optimization": false, 
```
