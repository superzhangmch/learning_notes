### 什么是 MOE

transformer MOE，其实就是把 FFN 扩展成并行的多个，并在入口处放一个 router，由router 来决定执行多个 FFN-专家中的哪几个，并在算出 FFN 后专家结果求和 merge 到一起，作为 FFN 层的输出。如下：

<img width="1200" height="670" alt="image" src="https://github.com/user-attachments/assets/bf199922-c3bd-421c-9672-d22404822f51" />

从算法层面，既然弄了多专家，那么就需要发挥专家潜力，那么每个专家就需要担当的事一样多。所以需要专家路由的均衡。这往往是靠特殊 loss正则项来实现的。 

### 为什么要并行

这群专家（FFN）都是同样地位的对等并列关系，所以天然适合并行：部署到不同的节点上，通过网络交互把上一层 output 传给它们。另外，MOE 层的参数占比很大，显存占用很可观（可能超单卡容量），这也导致必须对专家们分散存储。

### 网络流量问题

但是注意看：每个序列的每个 token，都是独立路由的，没法提前知道会路由到哪个专家。也就是说，并行训练的时候 $batch \times seq$ 这么多的 tokens, 会打散到不同的专家上；而 inference 的时候，对于当前 step，正在生成的 batch 个 token，也是打到不同专家上。这就引出了一个问题是，MOE 会带来不少的网络流量。

进 FFN-experts-group 的 inbound 网络流量是 $batch \times seq \times dim$，outbound 也是这样。但是这些网络流量，并是不把连续存储的东西 batch 方式收发的。而是完全零散的。所以本来是一个 all-to-all 的通信问题（是两次：收一次，算完发回再一次，分别叫 dispatch 和 combine），但是因为每个token  路由的不确定性，但是 `src -> 某专家` 的传输数据量并不确定。这更增加了网络交互的复杂性。所以 deepseek 的专家并行库的很重要一点就是实现高效的 all-to-all。

**（1）为啥说是 all-to-all**

前一层输出 [Batch, Seq, Hidden] 全部在一个 GPU 上。专家层的每个 expert 分散在别的节点（比如各自独占一个 GPU）。那么这个 GPU 需要把自己的 tensor 切分，把不同 token 分发到不同 expert-GPU 上。最后再把结果收回来。那么这不就是 1 → m (scatter) + m → 1 (gather)吗？为啥是 all-to-all？

论文/工程里提到的 “MoE 是 all-to-all 通信” 并不是针对这种“单 GPU → 多 GPU 专家”的极端情况，而是针对 大规模并行训练时的常见拓扑。在 张量并行 / 数据并行 下，每个 GPU 本身就持有一部分 batch 的 token。所以每个 GPU 上既有属于本地专家的 token，也有属于其他 GPU 专家的 token。每个 GPU 都要把一部分 token 发出去，同时接收其他 GPU 的 token。这种情况就是 N 台 GPU 互相交换子集数据 → 典型的 all-to-all。

<img width="1068" height="734" alt="image" src="https://github.com/user-attachments/assets/bb5907bd-7dbb-4356-8862-7dffc0b74b1b" />

如图： all-to-all 不是广播， （像是矩阵转置）， 像是把数据 hash 后分别发送到 hash 桶里。

<img width="1092" height="732" alt="image" src="https://github.com/user-attachments/assets/71f9ee4e-803a-4722-aab5-c9d245c15cfc" />
(图来自，https://www.zhihu.com/question/13295879833/answer/1893023924566070723 ，同时可参此文)
### 专家并行流程是怎样的

按 AI： 在 DeepSpeed-MoE、Megatron-MoE、FastMoE 等框架里，all-to-all 通常这样子：

1. 路由计算
* Router 输出一个 `(batch×seq)` 的 expert\_id 张量。
* 根据 expert\_id，把 token 分桶（bucketize），得到每个专家要接收多少 token。
2. 生成通信 plan
* 每个 GPU 统计自己要发送给其他 GPU 的 token 数量。
* 形成一个 **send\_counts\[]** 和 **recv\_counts\[]** 数组（类似 MPI Alltoallv）。
* 计算好 offset，打包成一个连续 buffer。
3. 执行 all-to-all
* 调用底层 collective：
  * **NCCL**：`ncclAllToAllv` 或手写 `send/recv` + `groupStart/groupEnd`。 
  * **Gloo/PyTorch**：用 `dist.all_to_all`（内部模拟）。
4. 本地专家计算：每个 GPU 上的专家处理收到的 token batch。
5. combine all-to-all：和 dispatch 类似，只是把专家输出再打包，按 token 原始位置 all-to-all 回去。

----

### deepseek 的 deepEP 专家并行库

https://github.com/deepseek-ai/DeepEP

- 实现了高吞吐，低延迟的 all-to-all 库（high-throughput and low-latency all-to-all GPU kernels）
  - 支持 NVLink 和 RDMA（直接存储访问） 的节点内 / 跨节点通信。总之就是针对硬件实际做了优化吧
- 支持 fp8 等低精度操作
  - 用于推理，那么降低了all-to-all 的通信量。那么训练时也能这样吗？
