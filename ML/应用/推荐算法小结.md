# 推荐算法小结

2017-09-28

某种程度上说，检索、广告点击预测、推荐，是没什么区别的。都是针对一定的请求条件，返回有若干个结果的结果列表，且结果应该按一定的排序规则排序。

但是一般还是把推荐区别于检索、CTR。因为推荐的检索条件往往很泛，不具体。但是再泛，往往也不会是完全没条件（往往至少会有uiser_id），否则就是完全没有个性化的静态推荐了。因此一般提到的推荐都是有一定条件的推荐。

最典型的推荐——比如电影推荐——会有两个集合，user 集合和 item集合。user 可以对任何一个 item 有评分(rating)，可以 是0/1两档评分，也可以有限多档评分；但单个user往往只对少量的item评分。这样所有评分构成了 user-item 稀疏矩阵。推荐的目的就是预测 user 会怎样 rating 其没评过的 item。

各种博客文章或者论文里所言的推荐算法，不言而喻所指往往就是处理以上这种问题。

另外，是可以有一系列的features（性别，年龄，职业） 来描述 user的，也可以有一系列的feature来描述item（比如对于电影，可以是title，类别，语种，年份等）。必要的时候，推荐可以使用这些features。

## content based 推荐
---------------------

对于以上问题，为了做推荐，一种思路就是具体问题具体分析，研究user、item 的那些特征，然后用这些设法做推荐。用到了user/item的具体内容，因此称为 content based。

该方法可以认为就是一种检索。用user 的某些 feature，或user的历史items，检索出最相似的items。当i计算 item-item 相关度时，是基于 item的features做的。

参考：http://recsys.baidu.com/recsys/doc?tpl=index&doc=similarity

## 协同过滤推荐
---------------

想到协同过滤思路并不复杂。物以类聚人以群分嘛。相似的user，当然rating情况也差不多；被rating得差不多的item，相比也比较类似。把这种趋势融合进推荐算法，提高效果应该是理所当然的。

我觉得，协同过滤算法牛就牛在，不是把上述那种趋势融进已有的任何推荐算法，而是只用这些信息，居然可以达到很好的推荐效果。也就是说不用任何 user 或 item 特征！只用 rating 矩阵！

协同过滤有两大方式: (1). 一个就是所谓的user-based CF（基于用户的协同过滤） / item-based CF（基于物品的协同过滤）, 基于对rating 的一些统计相关性；(2). 另一个是模型的方式。

### (1). user-based CF / item-based CF

感觉推荐算法的科普性质的介绍中总爱提它们，这一度使得我以为他们涵盖了比较牛逼的推荐算法的百分之90%。其实他们只是非常基本（当然比较有效）的两种方法而已。

核心点在于通过 rating matrix 的行/列作为user vector/item vector，然后用pearson相关系数或cosine夹角距离得到item-item、user-user 相关度。

有了item-item、user-user 相关度，可以用之为权对rate作加权平均作user-item评分预测，也可以用以作topN推荐。user-based CF先出，后来才有的 item-based CF。Item-based的更广为所用。

#### user-item评分预测
- item-based：该user rate过的所有item的评分的加权和，weight为item-item 相似度
- user-based: rate过该item的所有user（可以只取top_K近似用户）的评分的加权和，weight为user-user相似度

Note: i~i/u~u相似性可以用pearson相关系数或jarcard或cosine等计算。

#### topN推荐
- item-based: 对于用户u的所有rated的items，每个item找出top_K个相似item，这样得到一个item集合作为候选集合。然后对其中每个item做user-item评分预测，最后取top。
- user-based: 先找该user的k个最临近user，然后对这些user近邻的所有rated的item作为候选集合。然后对其中每个item做user-item评分预测，最后取top。

参考：http://blog.csdn.net/huagong_adu/article/details/7362908

推荐时，需要数据加载内存，占内存空间比较大，所以又称为memory-base CF。计算中用相关度找近邻，因此又称为 neighbour-basedCF。

注意：item-base CF 和 content-based recommendation看起来很像，他们之间的重要区别是item-item的相似度计算方式。

###  (2). model-based CF

指用机器学习学习到model，然后作预测的方式。所用哪种机器学习方法无所谓，都可称为model based。

#### 矩阵分解的方法
典型的model-based CF算法是基于矩阵分解的算法。

对于user-item 矩阵，可以作SVD分解，分解得到的两个矩阵分别作为user/item 的类似词向量一样的dense向量矩阵，或曰latent factors。然后就可以作最近邻推荐了。原生SVD方法不能很好处理矩阵缺省值，所以不能用SVD分解后再合成来得到预测值。

比较实用的矩阵分解方法并不是直接作矩阵分解，而是转化为取max/min的优化方法。这时仍然是求出矩阵U,V使得U*V≈M, 只是只需要能拟合M中有值的元素就行。把U*V展开后，只对M中有值的项，作回归逼近；此时loss函数就是是差平方和。至于怎样来求得U、V参数，可以用梯度下降SGD（还有一种叫ALS交替最小二乘的求解方法）。这时候确实在作矩阵分解，但其实已经不是SVD分解了，但因为是在努力化解SVD问题时发明的方法（Funk SVD），往往仍称之为SVD分解。

直接U*V≈M比原生SVD效果好了不少，但是更实用的是引入偏置bias，以及平均rate值。假设用户u对于item i的评分是rate_u_i, 用户u与item i的latent factor 分别是 U_u 与 V_i, 也就是在 U_u * V_i ≈ rate_u_i 基础上增加 mean_rate, b_u, b_i 使得：mean_rate + b_u + b_i +U_u * V_i ≈ rate_u_i 。b_u与b_i表示用户u和item i分别的评分bias（都是待训参数），而mean_rate 指整体的平均评分。一般地加入bias后才可以认为是矩阵分解CF的baseline。另外注意，去除矩阵分解后仅剩的 mean_rate + b_u + b_i ≈ rate_u_i，也是一种 model-based 的 CF baseline。

更复杂的矩阵分解CF方法比如 svd++ 往往是在 $mean\\_rate + b_u + b_i +U_u  V_i ≈ rate\\_u\\_i$ 基础上添加东西（更多bias或更多latent factor，latent factor 会加到 U_u 或 V_i 上）。正因为添来添去，仍然不离此范式，所以才有svd-feature 工具索性对此作了抽象，使得可以任意添加更多信息。

参：
- http://blog.csdn.net/winone361/article/details/50705750
- http://surprise.readthedocs.io/en/stable/matrix_factorization.html
- http://blog.csdn.net/liyusheng0100/article/details/9057499

#### 深度学习的方法
深度的方法，则是变化多端了。

youtube有文章是关于深度视频推荐的。所用方法大概说就是user, item 转化为embedding dense 向量, 然后在此基础上分类或回归。

还有先用矩阵分解得到latent factor，然后用深度方法回归逼近latent factor的。这是为了解决冷启动问题（这样新的user/item都可以用深度方法预测出一个不怎么靠谱但聊胜于无的latent factor）。

还有用RNN方法的。大概思路是带时间顺序的推荐，可以看做是序列预测问题（比如音乐推荐，基于用户的按时间听歌记录，预测他愿意听的下一曲歌）。



### 关于隐式显式反馈

有明确的喜好与不喜欢两种或多种评分的才是显式反馈。如果只有喜欢或不喜欢，都属于隐式反馈，另外即使是时长等，也算是隐式反馈（虽然这时候可以转化为一种显式表示）。

如果只有隐式反馈，那么矩阵分解方法就没法用了。但是基于item/user-based 的方法是可以的。矩阵分解这时候如果要起作用，必须设法构造负样本。有思路是负采样。
