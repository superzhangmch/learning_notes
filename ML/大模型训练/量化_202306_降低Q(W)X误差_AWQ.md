# 《AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration》 https://arxiv.org/pdf/2306.00978 

awq 和 gptq 一样，都是在量化矩阵乘法中的 weight。令 Q(W) 表示对 W 的量化，二者都是努力使得 $\lVert Q(W)X - WX \rVert$ 尽量小。 

awq 指出 gptq 的问题在于容易在调校数据集上过拟合（毕竟 gptq 是强力在范数层面对齐）：
> it(GPTQ) may overfit the calibration set during reconstruction, distorting the learned features on out-of-distribution domains, which is problematic
since LLMs are generalist models.

## AWQ 整体思路

（1）、思路

AWQ 大体思路是：既然希望 $Q(W)\cdot X$ 变化不能太大，那么如果 X 中有部分元素过大，那么就需要在 W 上做相应位置做文章有特别调整。

鉴于 X 作为 WX 的 input，往往是上一层的激活输出，所以 AWQ paper 的标题中说: Activation-aware（关注于激活 X 的） Weight Quantization（针对 W 的量化）。

（2）、为啥

之所以有上面的思路，是因为作者观察到：作为上一层激活输出的 X 里面可能有些通道取值比较大，这些通道占比较小（不足 1%), 如果对这些通道对应的 w 不作量化而简单量化其他，model 的效果折损很小。

<img width="1170" height="308" alt="image" src="https://github.com/user-attachments/assets/bc9da463-e0c7-4311-acfc-87ca4bd565b7" />

<img width="1540" height="666" alt="image" src="https://github.com/user-attachments/assets/0091e700-a409-408b-a842-6d85812dcc0b" />

- note：上图右是 XW 形式。

（3）、怎么做

这样如果选择性量化 99% 参数，看起来就 ok 了。但是会有问题是：不同精度的计算混杂在一起，实现起来费劲——如图 bad hardware efficiency。

于是作者需要用一种统一的方式来处理。粗说来，对不同的 weight channel，乘上不同的 scale s 然后再量化【当然与 X 乘时，需要再除以 s，具体见下文】。

<img width="934" height="570" alt="image" src="https://github.com/user-attachments/assets/d4815d73-47b7-483f-a71c-6a94cf52e9f9" />

按图中 XW 形式，具体说来是：假设 $W = [w_1, w_2, .., w_n]^T$, $w_i$ 是行向量， $X = [x_1, x_2, .., x_n]$ 是列向量组成的。则 $XW= \sum_i x_i w_i$ （ $x_i w_i$ 是列乘行，乃外积）, 而用 awq 量化做法，则是 

$$
X Q(W) = \sum_i \frac {x_i } {s_i} \cdot Q(w_i s_i)
$$

或者把 X、W 分别当行与列向量，则 awq 就是在内积的单项 x_i*w_i 上作 $\frac {x_i } {s_i} \cdot Q(w_i s_i)$。

## 为啥可以那么做

首先要说明，那样做根本上还是为了令 $\lVert .. \rVert$ 衡量的量化误差尽量小。

那样做的合法性看下图（只看单个参数）：

<img width="1302" height="628" alt="image" src="https://github.com/user-attachments/assets/fc14b9de-18df-46ca-8540-07848a601ab7" />

从图中看： 

1. $Err(Q(w)x)$ 其实就是 Q(w)x - wx 衡量的量化误差（注意这里 w, x 分别是 W、X 中的一个元素，乃 scalar 的）。因为  

$$
Err(Q(w)x) = ∆ · RoundErr( w ∆ ) · x = ∆ · [round( w ∆ ) - \frac w ∆ ]· x = ∆ · round( w ∆ ) x - wx = Q(w)x - wx
$$

2. 对单个参数：RTN（最近邻）量化的误差是 AWQ 的 s 倍。
  - 这是因为
    - $\Delta ≈ \Delta'$: $\Delta$ 是从一个参数组取 max 统计出的，而这里只考虑组内的一个参数——一个乘 s 作调整不会导致 $\Delta$ 的剧烈变化，可以认为 $\Delta ≈\Delta'$
    - RoundErr( $\frac w \Delta$ ) = RoundErr( $\frac {ws} \Delta$ ): 四舍五入的误差最大是 0.5，平均是 0.25。这两个舍入的平均误差都是 0.25，所以可以认为二者相等。
  - 因此看起来 s 越大越好。实际上否，因为：上面说一个参数乘 s 导致的 $\Delta \rightarrow \Delta'$ 变化不会很大；如果 s 很大，这个 $\Delta$ 变化就会很可观。这样一个 group 内其他参数也都受此影响了。
    - <img width="1118" height="766" alt="image" src="https://github.com/user-attachments/assets/aafdd842-8c63-4adc-a712-3dde124ebeab" />

## scale 因子 s 怎么选

如前，这个 s 不是 $Q(W) = \Delta round(w/\Delta)$ 中的 $\Delta$ 这个因子。而是 awq 对 salient 通道的调整因子 s： $Q(W) = \Delta round(\frac {w \cdot s} \Delta)$

这个 s 怎么选取呢？

### 选取原则

选取原则是 ||Q(W)X - WX|| 要小。也就是 

$$
\begin{cases}
L(s) &= \lVert Q( W \mathrm{diag}(s))\cdot (\mathrm{diag}(s)^{-1}X) - WX \rVert \\
s^{*} &= \mathrm{argmin} _ s L(s) 
\end{cases}
$$

其中：

$$
W \cdot \mathrm{diag}(s) =
\begin{bmatrix}
w_{11} & w_{12} & \cdots & w_{1n} \\
w_{21} & w_{22} & \cdots & w_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
w_{m1} & w_{m2} & \cdots & w_{mn}
\end{bmatrix}
\cdot
\begin{bmatrix}
s_1 & 0 & \cdots & 0 \\
0 & s_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & s_n
\end{bmatrix} = \begin{bmatrix}
w_{11}s_1 & w_{12}s_2 & \cdots & w_{1n}s_n \\
w_{21}s_1 & w_{22}s_2 & \cdots & w_{2n}s_n \\
\vdots & \vdots & \ddots & \vdots \\
w_{m1}s_1 & w_{m2}s_2 & \cdots & w_{mn}s_n
\end{bmatrix}
$$

还有

$$
\mathrm{diag}(s)^{-1}\mathbf{X} =
\begin{bmatrix}
\dfrac{1}{s_1}x_{11} & \dfrac{1}{s_1}x_{12} & \cdots & \dfrac{1}{s_1}x_{1k} \\
\dfrac{1}{s_2}x_{21} & \dfrac{1}{s_2}x_{22} & \cdots & \dfrac{1}{s_2}x_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
\dfrac{1}{s_n}x_{n1} & \dfrac{1}{s_n}x_{n2} & \cdots & \dfrac{1}{s_n}x_{nk}
\end{bmatrix}
$$

但是这个并不容易求出（有量化，所以不可导；有 approximated gradients 方法，作者发现仍然不能稳定收敛）

### 实际怎么选

对每一个input X 的通道 c，统计其均值 (average magnitude of activation (per-channel)), 

$$
s _ {X,c} = \frac 1 n \sum_j |x_{cj}|
$$

然后加上一个幂指数成为 

$$
s_c := s _ {X,c} ^\alpha
$$

这个 $\alpha \in [0, 1]$ 是整个 model 共享的。于是通过 grid search 遍历即可。

