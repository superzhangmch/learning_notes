可以参 《FP8 FORMATS FOR DEEP LEARNING》（下面用《FP8》代替） by nvidia/intel/arm, https://arxiv.org/pdf/2209.05433 

简单说
- fp8 主要是和计算单元的接口层而言，存储于带宽效率高
- 因 trian-inference 一致，比 int8 训练后量化好。
- 需要更精细的 rescale 控制
- 需要专门硬件支持。Hopper 架构 (H100 GPU, 2022 年 3 月发布) 才开始正式支持 FP8 (8-bit floating point) 精度。
  - 而一般 edge 芯片，比如 NPU，支持 int8，并不支持 fp8.

### fp8 格式

GPU 内， fp8 有 E5M2 与 E4M3 两种格式
- E5M2：1bit 符号位，5bit指数位，2bit 尾数(mantissa)位
  - 完全遵循 IEEE754，保留了 ±∞、NaN、0。
  - 动态范围大(-57344 ~ 57344)，但精度稍低，用于梯度。
    > 《FP8》：The recommended use of FP8 encodings is E4M3 for weight and activation tensors, and E5M2 for gradient tensors 
- E4M3：1位符号 + 4位指数 + 3位尾数
  - 不表示 ±∞，只有一个 NaN 编码，从而多出一段动态范围
  - 动态范围较小(-448 ~ 448)，但精度高，用于权重与激活

与 MXFPx 区别：
- MXFPx 在硬件层有 32 个数共享一个 scale 的支持：MX (mixed-precision) FP4 在 Hopper 的 Transformer Engine 里即如此，scaling 是 硬件强制的粒度，用户和 CUDA 库都不能改。
- FP8 (E4M3 / E5M2)：对于 FP8，NVIDIA 没有在硬件里强制固定分组 scale。Tensor Core 本身直接支持 E4M3 和 E5M2 两种 FP8 格式运算，就像 FP16/BF16 那样，输入是什么它就算什么。其“分组策略”（per-tensor、per-channel、per-tile 等）并不是硬件定死的，而是 由 Transformer Engine 等软件层决定。

### gpu 内怎么做的 

gpu (H100) 内部作 fp8 计算的时候，其实是先把 input tensor 转为高精度表示（这个高精度并不一定就是 FP16 而是特殊内部专用格式，因为fp16 是 gpu 对外接口层表示， gpu 内部犯不着也用这样格式），然后在高精度表示上做的计算。也就是说 FP8 只是**存储和接口**格式。这样看，不算真正的 fp8。

具体说来，在 H100 上执行类似 `D = A * B` 这样的 FP8 操作时，在 GPU 的 Tensor Core 里，这个过程大致如下：

1. **输入数据存储**：
   * A 和 B 可以以 FP8 格式存储（E4M3 或 E5M2，视任务选择）。
2. **计算时的内部提升 (accumulation promotion)**：
   * A 和 B 先从 FP8 **解码**到更高精度。
   * Tensor Core 执行在较高精度上执行矩阵运算
3. **输出存储 (可选量化回 FP8)**：
   * 如果模型需要继续保持低精度存储，D 会再量化回 FP8——但是只是临近输出时转回而已。
   * 如果是中间结果或最终输出，可以保存在 FP16/BF16/FP32。

再 《FP8》一文中，也提到这点：
> （fp8 输出结果是更高精度的：）It is expected that mathematical operations on FP8 inputs will produce outputs in higher precision, optionally converting the results to FP8 prior to writing them to memory.
>
> (接着说，因为fp16上的运算就是这样做的：）This is common practice today for the 16-bit floating point formats (FP16 and bfloat16) found on CPUs, GPUs, and TPUs. For example, matrix-multiplication or dot-product instructions produce single-precision(单精度即 fp32； fp16的矩阵运算产生 fp16 结果) outputs but less arithmetically-intensive operations are typically performed after casting the 16-bit inputs to single precision（计算量低的操作可以把 fp16 input 先转 fp32 再计算）.
>
> Thus, FP8 tensors will be generated by converting to FP8 from wider types, such as single precision floating point.

### 好处

虽然内部并没按 fp8 计算，但是可以用一半显存，一半带宽（提高吞吐效率）。所以能提高训练效率。

而且用 fp8 方式训，则 inference 的时候——虽然 weight 保存为了 fp32——可以按 forward 方式转 fp8，train-inference 一致，精度可以不受损。而 int8 量化，则是有损的。
> 《FP8》：8-bit inference deployment is greatly simplified by FP8 training, as inference and training use the same datatypes.
This is in contrast to int8 inference with networks trained in 32- or 16-bit floating point, which require post-training
quantization (PTQ) calibration and sometimes quantization-aware training (QAT) in order to maintain model accuracy.
Furthermore, even with quantization aware training some int8-quantized models may not completely recover the
accuracy achieved with floating point.

### 实际怎么用

fp8 train，不只是 gpu 内部计算时，即使在代码层，也需要结合高精度（比如主权重一般存 fp32），所以其实是混合精度训练。

fp16 混合训练的时候，因为表示范围和 fp32 的差异，需要作 rescale。fp16主要是防止 underflow，主要是在 loss 处统一做 rescale。而 fp8 混合训练，则需要更细粒度的 rescale。一般是不同 tensor 不同的 rescale 因子，甚至tensor的不同片段不同的 rescale 因子（deepseek-v3 如此）。

另外，优化器所托管的参数，以及优化器状态，都不是 fp8 存储，仍用原做法。
