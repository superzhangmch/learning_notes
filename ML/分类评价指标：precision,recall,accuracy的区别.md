# 分类评价指标：precision,recall,accuracy的区别

### FP、TP、FN、TN
评价肯定是在测试集上进行的，这四类构成了整个测试集。第2个字母的PN表示模型的判断。第一个字母是根据实际label/tag情况，所做的修饰词。

### 精度-precision
预测为正且真实为正的个数/预测正例个数。强调预测正例中有多少是真正的正例。

### 召回-recall
预测为正且真实为正的个数 / 所有真实正例个数。强调能召回多少真实正例

### 准率率-accuracy
正确预测的个数/总个数。

#### Note：
引入precision与recall的最大原因是，如果两分类很不均衡（比如点击预估），那么令模型为全取数量多的那类，则准确率必然很高。但是召回与精度却都很低。所以类别不均衡分类问题，只关注accuracy没什么大意义。应该是小类别（应该说是说是关注的那个类别；往往关注的是小类别）的precision/recall都应该足够高。 

###  F1值
衡量二分类模型精确度的一种指标。F1=2*精度*召回率/(精度+召回率). 更一般地，Fx=(x^2+1)*精度*召回率/(x^2*精度+召回率)。

### roc曲线
FPR/TPR分别为横纵坐标，卡不同的二分类阈值，所画出的曲线。

对于预测为正的:
- FPR=FP/(TN+FP)，预测为正实际为负/负样本数，负样本中错误预测率
- TPR=TP/(TP+FN)，预测为正实际为正/正样本数，正样本中的正确预测率

（计算ROC：把数据集分为按取值排序，然后逐点移动计算）

### AUC值
roc曲线下的面积。衡量二分类。往往二分类是靠卡阈值卡出来的分类，AUC能对此综合考虑。

直观理解：from https://blog.csdn.net/u013385925/article/details/80385873 ：首先AUC值是一个概率值，当你随机挑选一个正样本以及负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值，AUC值越大，当前分类算法越有可能将正样本排在负样本前面，从而能够更好地分类。

也就是：当做list长度为2的排序问题，排序正确的概率。

<hr>

对于LR二分类的阈值截点，应该综合以上各指标，按需求而卡，而不是简单0.5.
