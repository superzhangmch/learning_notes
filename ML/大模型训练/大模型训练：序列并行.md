# transormer 训练的序列并行

**（1）、除 attn 操作外，batch维 与 seq维本质都是 batch维**

transormer 的 hidden state shape = [batch, seq_len, hid_dim], 或者 [seq_len, batch, hid_dim]。除了 attn 之外不分，都是在单个token上做的，也就是 seq 维度和 batch 维没区别，它们会被当做 seq*batch 的大batch。

**（2）、序列并行的两种形式**

虽然 batch/seq 维度 attn 操作外本质没区别都可理解为 batch，但是一般把 batch 拆分当做数据并行。如果是在 seq 这个维度切分 batch，并不算做 data parallel。

另外，就 transformer model 的架构与计算流程来说，序列根本的体现在于 attn。

这样就有两种序列并行的定义：
- 对 attn 矩阵运算，把序列切开，作分块矩阵运算。
  - MHA，就单个 head 来说，是 softmax(QK')*V' 的形式，本质上还是矩阵乘法操作（即使 MLA 等 attention，仍可以拆为基本运算是矩阵乘法），而张量并行，就是把大的矩阵乘法拆为分块矩阵乘法，在不同 gpu 上算。从这个角度来说，序列并行和张量并行，解决的都是同一个问题。只是张量并行是更泛的，而序列并行处理的是某一种特殊矩阵乘法，从而可以更有针对性地优化而已。这种思路，在分块矩阵乘法角度作优化。
- 全程或部分节点把 seq 当做 batch 一样切分。但是作 attn 的时候，不作序列切分，仍然聚合出全序列（但是会按heads 切分）
  - 序列并行，无论如何，和 attn 摆脱不了干系。按该方式，关节节点仍然是围绕 attn。
  - 该方法的主要原理是：QKV 都可以按序列切分，对于一个 gpu 负责的 Q子序列，它只算这些 Q 的 attention。但是为了算attn，必须设法把其他节点的 KV 也都传过来（即 需要 KV 全序列）。

----

## 一、megatron-3 的序列并行：为了降激活显存（attn：全序列，分heads）

它是和张量并行相结合的用来减少激活显存占用量的方案。确实在某些地方按 seqence 维度切分了，但是算 attn 的时候，仍然是在全序列上做的。关于 attn 把序列切分的序列并行， megatron 中叫 context paralle，见其[官网](https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/context_parallel.html#context-parallelism-overview)。

出自： megatron 第三篇 《Reducing Activation Recomputation in Large Transformer Models》 https://arxiv.org/pdf/2205.05198 ，该文旨在设法降低激活显存的占用。

**背景：** megatron-2 《https://arxiv.org/pdf/2104.04473》 4.2 节讲到，为了方便 MHA 计算时少一些显存 copy，把 hidden state 的 layout 由一般做法的 [B, S, num_heads, head_dim]， 换成了 [seq, Batch, num_heads, head_dim], 这样方便中间两维 batch*num_heads 结合。从这后， megatron 就是 [S, B, H] seq-first 的。笔记在 [这里](https://github.com/superzhangmch/learning_notes/blob/main/ML/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/transformer%20%E7%9A%84%E9%9A%90%E5%B1%82%20shape%EF%BC%9Absh%20%E4%B8%8E%20sbh.md) 。

megatron 发现 transformer 种相关张量并行后面的 layerNorm、dropout 需要存下不少显存，以便反向传播。于是选择对这些显存作切分并分布到不同节点上，用时拉取过来。如果是 batch-first，没准它对batch继续做拆分，但是鉴于它已经是 sequence-first 了，所以很自然地，megatron 选择了按 sequence 维度切分。

**激活显存问题由来 ：**

<img width="1410" height="936" alt="image" src="https://github.com/user-attachments/assets/f1c4ea1f-c632-4de8-ac45-c0e526b41e8e" />

如上图之下，无论是 attn 还是 MLP，在 tensor 并行的最后一步，都是 $\[Y_1, Y_2\]\cdot \[B_1, B_2\]^T = Y_1 B_1 + Y_2 B_2$ 形式。无论是 [B, S, H] 还是 [S, B, H], $Y_1, Y_2$ 都是把 H 维切分了，且与 S 和 B 的顺序无关。 $Y_i \cdot B_i$ 的 shape 和期望结果 shape 一样，但是分散在不同 gpu 上，需要聚合求和到一起，才能做 dropout(layout 是element wise的，如果分布式作，则需要全局一致的随机数种子) 与 layerNorm（layerNorm 是 token wise的，需拿到完整 hidden_dim 数据）。又因为layerNorm 后的下一layer 的输入，在每个 gpu 都需要全量数据，于是每一 gpu 都重复算了 layerNorm。反向传播时，也需要这些激活，因此需要存下来。

作者推算得，对于标准 transformer 来说，单层的总激活显存占用是： $sbh(34+5\frac {as} {h})$, 其中 s=seq, b=batch, h=hidden_dim, a=num_heads， $5\frac {as} {h}$ 乃 attn带来的。通过张量并行，可以把 $sbh(24+5\frac {as} {h})$ 的显存均分到张量并行数 t 上（变成 $sbh(\frac {24} t +5\frac {as} {th})$）。而 $10sbh$ 的显存，对应 dropout + layernrom 处，没法分摊。因此要通过序列并行把这部分显存分摊。

这 $10sbh$ 显存的具体构成（占据了 10/34, 可见占比确实不小）：
- 每个 dropout 占据 sbh, MLP 与 ATTN 后各有一个 dropout（注意 attn 内部 softmax 后还有个小 dropout，不是它）
- MLP 与 ATTN 后各有一个 layerNorm。layerNorm 需要 2bh，总共 4sbh。layerNorm 之后的 MLP 或者 attn QKV proj 的输入，共 4sbh，也需要存下。所以总共 8sbh。

**怎么用序列并行解决问题的：**

可以看到面对的问题，与 sequence 本身并没关系。只是当要切分时，因为 sequence-first，拿它——也就是tensor的第一个维——切分最方便高效（另外，batch未必很大，如果那样，按它切也会不大方便）。总之切分后，每个 gpu 的所有激活显存都是均分存储了。layernorm+dropout 激活均分了，当用的时候，当场拉来就行。

<img width="1496" height="992" alt="image" src="https://github.com/user-attachments/assets/89645a62-7173-4560-8cc9-a49a764993ef" />

注意：
- 上图上下对比看，可以发现通信量"基本"没变。
  - forward：张量并行后不跟序列并行，需要一次 all-reduce 通信
    - 若用了序列并行，需要两次不相邻的 scatter-reduce 与 all-gather，而一次 all-reduce 正好由这两个动作构成。总体保持不变。
  - backward：张量并行后不跟序列并行，也是需要一次 all-reduce 通信
    - 若用了序列并行，也是需要两次不相邻的 scatter-reduce 与 all-gather（但会和forward顺序反），而一次 all-reduce 正好由这两个动作构成。当然为了省显存考虑， $\{Y_i^s\} \rightarrow Y$ 的 g 算子，在backward时，仍要算一次还原 Y，于是额外再多一次 all-gather。即两次 all-gather，一次 scatter-reduce。但是这多的一次 all-gather 可以和计算重叠，并不会真的增加时间。总体上相当于不变。
- 经此序列并行后，整个流里，全是并行的了。

## 二、DeepSpeed-Ulysses 的序列并行：全流程按 seq 切（但 attn：全序列，分heads）

微软：《DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models》 https://arxiv.org/pdf/2309.14509 

<img width="1940" height="896" alt="image" src="https://github.com/user-attachments/assets/0b7f83b7-1341-4d61-80bf-76a52e92c1be" />

执行流程如上。

<img width="840" height="536" alt="image" src="https://github.com/user-attachments/assets/4d21c5ee-ef5d-477f-a666-702b6723638a" />

Q/K/V 的 all-to-all 传输方式见上图（all-to-all 像是做了矩阵转置）。上图改编自讲 Ulysses 的 https://zhuanlan.zhihu.com/p/689067888 ，同时可参该文。

和 megatron-2 一样，注意力本身并不是分布式的。只是一种按 heads 分布式计算的方式。

每个 gpu 确实从头到尾都是按序列切分的，但是到了 ATTN 的时候，要先用 all-to-all 把数据转成每个 gpu 上存完整序列，但是只是 hidden dim 一部分。也就是 shape: [S_part, H_full] => [S_full, H_part]。然后每个 gpu 上按老方法算 attn，完后再用 all-to-all 弄成按序列切分。

## 三、 Ring Attention：类似 flash-attn，分块矩阵法做 attn

UC Berkeley：《Ring Attention with Blockwise Transformers for Near-Infinite Context》 https://arxiv.org/pdf/2310.01889 

【更早一篇 ring attn 是 《Sequence Parallelism: Long Sequence Training from System Perspective》 https://arxiv.org/pdf/2105.13120 . berkeley 指出引用它，并说它不支持计算通信重叠】

这个在算 attn 时会把序列切分，所用方式类似 Flash Attention，且采用 online softmax 技术。

<img width="1504" height="722" alt="image" src="https://github.com/user-attachments/assets/5af6f6cf-2c9b-41ad-b332-ae02f15f614c" />

- 本地存序列的一部分，并可算出本地的 Q/K/V。
- 然后节点之间按环状交流传输 KV：每个节点持续传给下一节点，同时从上一节点接受；不是只传一次，而是要持续多次（并行数减一次）直到见过所有 KV。
  - 边传边计算，传完了也算完了。
  - 采用 online softmax 技术：在每一轮计算时，维护当前的最大值和累计和，逐步更新 softmax 归一化因子。
- 最终获得的是本地 Q 对应的 attention。过程中不切分 heads。

具体计算方式如下（下图之下：分块矩阵角度怎么做的，包括 online-softmax）：

<img width="1202" height="1364" alt="image" src="https://github.com/user-attachments/assets/d45b85a3-719c-4f93-b6b8-e678a73b04b6" />

online-softmax 如下（实际使用需要作数值稳定性调整）：

$$
\begin{align}
\alpha_{t+1} &= q \cdot k_{t+1} \\
P_{t+1} &= P_t + e^{\alpha_{t+1}} v_{t+1} \\
S_{t+1} &= S_t + e^{\alpha_{t+1}} \\
out &= \frac{P_n}{S_n}
\end{align}
$$

注意：
- 虽然叫 ring，且过程中和 collective 通信的 ring 通信很像，但是区别是后者对于应用层来说是原子的，自动完成。而 ring attn，则是应用层发起的多次独立的点到点通信。
- 过程中计算与通信重合，效率还很高。
  >  Concretely, for any host-i, during the computation of attention between
its query block and a key-value block, it concurrently sends key-value blocks to the next host-(i + 1)
while receiving key-value blocks from the preceding host-(i − 1). If the computation time exceeds
the time required for transferring key-value blocks, this results in no additional communication cost(只要计算用时比通信更耗时，则这个 overlaping 就非常可行).
This overlapping mechanism applies to both forward and backward passes of our approach since
the same operations and techniques can be used（backward 时也能这样）. 
- 看起来只能用于 inference，其实可以用于训练。paper 中有 backward 的算法概要(其具体未研究)：
  ```
  def _ring_attention_bwd(axis_name, float32_logits, blockwise_kwargs, res, g):
    ...
    def scan_kv_block(carry, idx):
      ...
      k, v, dk, dv = map(lambda x: lax.ppermute(x, axis_name, perm=[(i, (i + 1) % axis_size) for i in range(axis_size)]), (k, v, dk, dv))
      # lax.ppermute: We（作者） use collective operation jax.lax.ppermute to send and receive key value blocks between nearby hosts.（forward 也用 lax.ppermute）
      ...
      (dq, dk, dv, k, v), _ = lax.scan(scan_kv_block, init=(dq, dk, dv, k, v), xs=jnp.arange(0, axis_size))
      dq, dk, dv = dq.astype(q.dtype), dk.astype(k.dtype), dv.astype(v.dtype)
      return dq, dk, dv, None
  ```

## 四、megatron 的 context-parallel

context-parallel 的官网介绍见[这里 context-parallelism-overview](https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/context_parallel.html#context-parallelism-overview)。

据 https://zhuanlan.zhihu.com/p/5502876106 , megatron 的 context-parallel 大约就是 ring-attention 序列并行。

- 相比之前的 megatron SP(=sequence parallel)，CP 不仅切分 Dropout 和 LayerNorm 的激活，而是切分网络 input 和所有激活。
- 除注意力（Attention）外，其他模块（如 Linear、LayerNorm 等）都可以照常运行，因为它们没有跨 token 的操作。
- 注意力机制的处理：
  - 每个 gpu 只存全序列中的部分 Q，而 Q 需要和整个序列的 K、V（Key、Value） 交互。因此需要：
    - 前向传播时，执行 all-gather 来收集完整序列的 K、V。
    - 反向传播时，对 K、V 的激活梯度执行 reduce-scatter。
    - 为省显存：每个 GPU 在前向时只保存子序列的 K、V，反向时再重新 all-gather 得到完整 K、V。
