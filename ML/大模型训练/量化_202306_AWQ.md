# 《AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration》 https://arxiv.org/pdf/2306.00978 

awq 和 gptq 一样，都是在量化矩阵乘法中的 weight。令 Q(W) 表示对 W 的量化，二者都是努力使得 $\lVert Q(W)X - WX \rVert$ 尽量小。 

awq 指出 gptq 的问题在于容易在调校数据集上过拟合（毕竟 gptq 是强力在范数层面对齐）：
> it(GPTQ) may overfit the calibration set during reconstruction, distorting the learned features on out-of-distribution domains, which is problematic
since LLMs are generalist models.

## AWQ 整体思路

（1）、思路

AWQ 大体思路是：既然希望 $Q(W)X$ 不能变化太大，那么如果 X 中有部分元素过大，那么就需要在 W 上做相应位置做文章有特别调整。鉴于 X 作为 WX 的 input，往往是上一层的激活输出，所以 AWQ paper 的标题中说 Activation-aware（关注与 X 的） Weight Quantization（针对 W 的量化）。

（2）、为啥

之所以有次思路，是因为作者观察到：作为上一层激活输出的 X 里面可能有些通道取值比较大，这些通道占比较小（不足 1%), 如果对这些通道对应的 w 不作量化而简单量化其他，model 的效果折损很小。

<img width="1540" height="666" alt="image" src="https://github.com/user-attachments/assets/0091e700-a409-408b-a842-6d85812dcc0b" />

- note：上图右是 XW 形式。

（3）、怎么做

这样如果选择性量化 99% 参数，看起来就 ok 了。但是会有问题是：不同精度的计算混杂在一起，实现起来费劲——如图 bad hardware efficiency。

于是作者需要用一种统一的方式来处理。粗说来，对不同的 weight channel，乘上不同的 scale s 然后再量化【当然与 X 乘时，需要再除以 s，具体见下文】。

<img width="934" height="570" alt="image" src="https://github.com/user-attachments/assets/d4815d73-47b7-483f-a71c-6a94cf52e9f9" />

- note：上图仍然是 XW 形式。上面两图外，其他地方又会用 WX 形式。

按图中 XW 形式，具体说来是：假设 $W = [w_1, w_2, .., w_n]^T$, $w_i$ 是行向量， $X = [x_1, x_2, .., x_n]$ 是列向量组成的。则 $XW= \sum_i x_i w_i$ （ $x_i w_i$ 是列乘行，乃外积）, 而用 awq 量化做法，则是 

$$
X Q(W) = \sum_i \frac {x_i } {s_i} \cdot Q(w_i s_i)
$$

### 
