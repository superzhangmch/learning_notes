deepseek v3 原版是671B参数。量化版有低至 1.58 bit 的。

该量化的源头似乎是这个： https://unsloth.ai/blog/deepseekr1-dynamic

1.58bit量化，即每个参数用0, 1, -1三个状态来表示（从而不再需要矩阵乘法）。log_2(3) = 1.58。存储上需要2bit存储，那么岂不是浪费一个状态，是不是莫若用四个状态来表示？我觉得没这样搞，是因为那样没法消除矩阵乘法。为了实现1.58bit 存储，需要启用某种的压缩技术：比如它其实就是3进制，那就把连续的多个参数用二进制表示：比如（12210012210)_3 用二进制表示后，就可以逼近1.58bit。但是这样使用的时候还需要解压缩。

1.58bit 的引入是 https://arxiv.org/pdf/2310.11453 、 https://arxiv.org/pdf/2402.17764 ，他们并没提到怎么量化已有模型，而是用来从头训练新model的。但是看起来是这个 unsloth 公司把它扩展得可以作训后量化了，他整合了四者：

![Uploading image.png…]()

从它的 blog 看，他们这样的量化技术叫动态量化技术。总之不是简单地就可以量化成 1.58bit的。
