# 神经网络：梯度消失与梯度爆炸

深度神经网络训练的时候，采用的反向传播方式，该方式背后其实是链式求导，计算每层梯度的时候会涉及一些连乘操作，因此如果网络过深，那么如果连乘的因子大部分小于1，最后乘积可能趋于0；另一方面，如果连乘的因子大部分大于1，最后乘积可能趋于无穷。这就是所谓梯度消失与梯度爆炸。

为防止梯度爆炸，一种方式是设置梯度剪切阈值 gradient_clipping_threshold， 一旦梯度超过改值，直接置为该值。
