# LLaDA 《Large Language Diffusion Models》 https://arxiv.org/pdf/2502.09992

![image](https://github.com/user-attachments/assets/1197a8d3-0497-40ca-9a8c-f8a9e7cc0b9d)

训练时对于 sample 的 text seq，根据 time_step = t ~ [0, 1], 随机位置掩盖掉序列中正比于 t 数量 的 tokens，并于交叉熵预测这些 masked tokens。这是加噪过程。

inference 的时候，则逐步随机位置预测出一些 tokens。经过多步，把所有位置填充，从而实现降噪生成。确实和扩散模型的 train 与 inference 流程很像，所以叫它扩散大语言模型。从这个角度是合理的。

但到了公式层面，则非常的不扩散模型。所以有人说就是 BERT 嘛。

### 生成长度怎么控制

提前分配长度空间，然后通过预测特殊end token "|EOS|" 来决定具体生成长度。

### 生成流程

- 标准做法：每一 step 再随机 mask 掉一些
  - ![image](https://github.com/user-attachments/assets/eb15abd5-9e58-4a3b-be83-dbec88362ae5)
- Low-confidence Remasking：每一 step 根据预测 tokens 的概率值，mask 掉低置信的
  - ![image](https://github.com/user-attachments/assets/d62846fa-51cf-4c1c-8da7-94ec92cbd5dc)

注意，无论哪种方式，早起 step 确定了的 token，后续 step 就不能改写了。

还可以半自回归生成（从左往右推进，每个 block 内，可以用上面的标准做法，或者Low-confidence 法）：

![image](https://github.com/user-attachments/assets/9988d291-6891-4bbf-9f6b-0c823d6dc3e9)

### classifier-free guidance
