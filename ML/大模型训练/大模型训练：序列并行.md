# transormer 训练的序列并行

transormer 的 hidden state shape = [batch, seq_len, hid_dim], 或者 [seq_len, batch, hid_dim]。除了 attn 之外不分，都是在单个token上做的，也就是 seq维度和 batch 维没区别，它们会被当做 seq*batch 的大batch。

所以序列并行，本质上来说，只和处理 attn 有关。

而 MHA，就单个 head 来说，是 softmax(QK')*V' 的形式，本质上还是矩阵乘法操作（即使 MLA 等 attention，仍可以拆为基本运算是矩阵乘法），而张量并行，就是把大的矩阵乘法拆为分块矩阵乘法，在不同 gpu 上算。从这个角度来说，序列并行和张量并行，解决的都是同一个问题。只是张量并行是更泛的，而序列并行处理的是某一种特殊矩阵乘法，从而可以更有针对性地优化而已。

序列并行和张量并行都在解决“算力和显存不够，如何把大算子拆到多 GPU 上”的问题。区别在于：
- 张量并行 = 矩阵通用切分。
- 序列并行 = 针对序列维度的特殊切分，主要优化 attention 的通信模式。

----


LLM 的**序列并行（Sequence Parallelism）**是一个比较宽泛的概念，目标都是把长序列的计算分布到多个 GPU 上，降低单卡显存压力并提高吞吐。
常见的实现方法主要分为几大类，我给你按**切分维度和通信模式**来分类。

## 1. Context Parallel / Sequence Parallel（Megatron-LM 类）

**代表算法**：

* **Megatron-LM Sequence Parallel**
* **Megatron-LM Context Parallel（CP）**

**思路**：

* 按 **序列长度维度（sequence length）** 把 token 切分到不同 GPU 上。
* 每个 GPU 保存自己那段 token 的激活值，注意力计算需要跨卡取全量 K/V。
* 通常需要 **AllGather / ReduceScatter** 进行跨卡同步。

**优点**：

* 显存占用与序列长度成反比，支持更长 context。
* 对 batch size 没有额外要求。

**缺点**：

* AllGather K/V 会带来较大通信开销，特别是长序列时。

## 2. 交错通信计算（Pipelined Sequence Parallel）

**代表算法**：

* **DeepSpeed-Ulysses**
* **Ring Attention**（严格来说属于通信优化，但也能算到这一类）
* **Tutel Sequence Parallel**

**思路**：

* 不一次性 AllGather 全量 K/V，而是将 K/V 分块（chunking）并环形或流水化传输。
* 每收到一块，就立刻计算对应注意力部分，然后继续传下一块。
* 最终得到完整注意力输出，但通信和计算是**重叠**的。

**优点**：

* 降低峰值显存占用（不需要一次性存全量 K/V）。
* 通信开销被计算掩盖，延迟低。

**缺点**：

* 算法实现更复杂，对拓扑和带宽敏感。

## 3. 双向分片（2D Parallel for Attention）

**代表算法**：

* **Colossal-AI Sequence Parallel**（2D Attention）
* **Mesh TensorFlow GSPMD**（Sequence Sharding + Head Sharding）

**思路**：

* 同时在**序列维度**和**注意力 head 维度**上分片。
* 每个 GPU 只持有自己负责的 token 片段 + 部分 head。
* 通信量减少，因为每个 GPU 计算的注意力规模更小。

**优点**：

* 内存和通信负担更平衡。
* 对大批量多头注意力很友好。

**缺点**：

* 需要额外的并行策略调度和复杂的 AllToAll。

## 4. 流式注意力 / 块稀疏注意力结合并行

**代表算法**：

* **FlashAttention-2/3** + Sequence Parallel
* **Blockwise Sparse Attention + Sequence Sharding**
* **StreamingLLM with Parallel KV Cache**

**思路**：

* 在序列并行的基础上，用流式 attention 或块稀疏减少需要通信的 token 数量。
* KV Cache 按序列分布到不同 GPU，推理时按需访问。

**优点**：

* 长序列训练和推理显存极低。
* 可以动态裁剪注意力计算范围。

**缺点**：

* 对全局依赖建模能力有限（稀疏模式下）。

## 5. 推理专用的序列并行

**代表算法**：

* **PagedAttention**（vLLM）
* **Tensor Parallel + Sequence Sharding for KV Cache**
* **SGLang / LightSeq KV Offload**

**思路**：

* 不在训练时切分，而是在推理时把 KV Cache 按 token 切分存到多 GPU / CPU / NVMe。
* 提升并行吞吐量，减少单卡 KV 压力。

**优点**：

* 推理吞吐高，可扩展到极长上下文。

**缺点**：

* 对训练无帮助，推理场景特化。

如果你愿意，我可以帮你画一张**序列并行算法家族树**，把 Megatron-LM 的 CP、DeepSpeed-Ulysses、Ring Attention、2D Attention 等放在同一个拓扑图上，这样可以直观看它们的差异和关系。
这样你一眼就能看出哪些是“经典切序列”，哪些是“通信优化”，哪些是“多维切分”。

```
                  ┌────────────────────────────┐
                  │   LLM Sequence Parallelism │
                  └────────────────────────────┘
                               │
        ┌──────────────────────┼──────────────────────┐
        │                      │                      │
        ▼                      ▼                      ▼
┌────────────────┐   ┌────────────────────┐   ┌─────────────────────┐
│  1. 经典序列切分 │   │  2. 交错通信计算优化  │   │  3. 双向分片 (2D)    │
│(Megatron-LM CP)│   │ (通信+计算重叠)      │   │ (序列+Head同时切)     │
└────────────────┘   └────────────────────┘   └─────────────────────┘
        │                      │                      │
        │                      │                      │
        ▼                      ▼                      ▼
┌─────────────────────┐  ┌─────────────────────┐  ┌────────────────────────┐
│ Token 按顺序切分到多卡 │  │ 分块KV → 环形/流水通信 │  │ 每卡持部分token+部分head │
│ 全量KV需 AllGather   │  │ 一边收一边算(Overlap) │  │ 降低通信量               │
└─────────────────────┘  └─────────────────────┘  └────────────────────────┘
        │                      │                      │
        │                      │                      │
        ▼                      ▼                      ▼
    代表算法:             代表算法:               代表算法:
  - Megatron-LM SP       - DeepSpeed-Ulysses      - Colossal-AI 2D
  - Megatron-LM CP       - Ring Attention         - Mesh-TF GSPMD
                         - Tutel SP

        ┌──────────────────────────────┬─────────────────────────────┐
        ▼                              ▼                             ▼
┌───────────────────────┐    ┌────────────────────────┐     ┌─────────────────────────┐
│  4. 稀疏/流式注意力结合SP│    │  5. 推理KV并行/缓存切分   │     │  辅助技术                │
└───────────────────────┘    └────────────────────────┘     └─────────────────────────┘
        │                         │                              │
        ▼                         ▼                              ▼
   - FlashAttn+SP             - vLLM PagedAttention         - ZeRO Stage3+SP
   - Block Sparse + SP        - Tensor Parallel + KV Split  - Mixed Parallel
   - StreamingLLM+SP          - SGLang KV Offload           - Checkpoint + SP
```
