# LLaDA 《Large Language Diffusion Models》 https://arxiv.org/pdf/2502.09992

![image](https://github.com/user-attachments/assets/1197a8d3-0497-40ca-9a8c-f8a9e7cc0b9d)

训练时对于 sample 的 text seq，根据 time_step = t ~ [0, 1], 随机位置掩盖掉序列中正比于 t 数量 的 tokens，并于交叉熵预测这些 masked tokens。这是加噪过程。

inference 的时候，则逐步随机位置预测出一些 tokens。经过多步，把所有位置填充，从而实现降噪生成。确实和扩散模型的 train 与 inference 流程很像，所以叫它扩散大语言模型。从这个角度是合理的。

但到了公式层面，则非常的不扩散模型。所以有人说就是 BERT 嘛。

### 生成长度怎么控制

提前分配长度空间，然后通过预测特殊end token "|EOS|" 来决定具体生成长度。

### 生成流程

- 标准做法：每一 step 再随机 mask 掉一些
  - ![image](https://github.com/user-attachments/assets/eb15abd5-9e58-4a3b-be83-dbec88362ae5)
- Low-confidence Remasking：每一 step 根据预测 tokens 的概率值，mask 掉低置信的
  - ![image](https://github.com/user-attachments/assets/d62846fa-51cf-4c1c-8da7-94ec92cbd5dc)

注意，无论哪种方式，早起 step 确定了的 token，后续 step 就不能改写了。

还可以半自回归生成（从左往右推进，每个 block 内，可以用上面的标准做法，或者Low-confidence 法）：

![image](https://github.com/user-attachments/assets/9988d291-6891-4bbf-9f6b-0c823d6dc3e9)

### classifier-free guidance

启用后，概率公式如下，其中 $p_0$ 是 prompt， $r_t$ 是 step=t时的状态, $r_0$ 是最终输出， w 是 guidance 强度， $p_θ(r_0|m, r_t)^w$ 表示把 prompt 替换成了 mask，从而是对应扩散模型 CFG 中的非条件生成分支。

![image](https://github.com/user-attachments/assets/29e58f85-a6bd-4844-93a9-1722c162d183)

### 体验例子

- query = 长江和黄河哪个更长？
- answer = 长江和黄河都是中国的重要河流，但长江的长度大于黄河。长江全长约6,300公里，而黄河全长约5,464公里。

![image](https://github.com/user-attachments/assets/db337843-1f3e-40f4-a39a-2b734d961e23)


