
### èƒŒæ™¯

å…³äº deepseek-v3 çš„ fp8 è®­ç»ƒçš„èƒŒæ™¯ä»‹ç»ï¼ŒåŸæ–‡æ‘˜å½•å¦‚ä¸‹ï¼ˆ3.3èŠ‚ ã€ŠFP8 Trainingã€‹ï¼‰ï¼š

> Inspired by recent advances in low-precision trainingï¼ˆè§ä¸‹é¢å¼•æ–‡A,B,Cï¼‰, we propose a fine-grained mixed precision framework utilizing the FP8
data format for training DeepSeek-V3.
>
> ã€å—åˆ°åˆ«çš„ fp8 æ–¹æ¡ˆå¯å‘ã€‘
- A=ã€ŠGpt3. int8 (): 8-bit matrix multiplication for transformers at scaleã€‹- 2022.08 - https://arxiv.org/pdf/2208.07339
  - int8 é‡åŒ–æ¥æé€Ÿæ¨ç†ï¼šInt8 çŸ©é˜µä¹˜æ³•æ–¹æ¡ˆï¼Œç”¨åœ¨ Transformer FFN å’Œ Attn projã€‚
  - å‘ç°ç‰¹å¾ç»´åº¦ä¸­æœ‰ä¸€äº› outliers å½±å“é‡åŒ–æ•ˆæœï¼šäºæ˜¯åˆ†è€Œæ²»ä¹‹ã€‚
  - <img width="1000" alt="image" src="https://github.com/user-attachments/assets/752d7062-b186-474e-b1b9-5962ecde0542" />
- B=ã€Š8-bit numerical formats for deep neural networksã€‹- 2022.06 - https://arxiv.org/pdf/2206.02915
  - å…³æ³¨è®­ç»ƒã€‚ç”¨æµ®ç‚¹ fp8 æ¯” fixed-pointï¼ˆint8ï¼‰ å¥½ã€‚å¹¶æ¨èï¼šæ¿€æ´»/æƒé‡ç”¨ 1.4.3ï¼Œæ¢¯åº¦ç”¨ 1.5.2
  - ç”¨å…¨å±€ loss scale è€Œéç»†ç²’åº¦ scaleï¼ˆä¸”æ­£å› æ­¤æ¯”å®šç‚¹çš„å¥½ï¼šå®šç‚¹æƒ…å†µï¼Œéœ€è¦é€å±‚ç»†ç²’åº¦ scaleï¼‰
- C=ã€ŠFP8-LM: Training FP8 large language modelsã€‹ - 2023.10 - https://arxiv.org/pdf/2310.18313

> While low-precision training holds great promise, it is often limited by the presence of outliers in activations, weights, and gradientsï¼ˆè§ä¸‹é¢å¼•æ–‡D,Eï¼‰.
>
> ã€ä½†æ˜¯å½“å‰çš„ fp8 æ€»æ˜¯å—å›°äº outlier é—®é¢˜ã€‘
- D=ã€ŠScaling FP8 training to trillion-token llmsã€‹- 2024.09 - https://arxiv.org/pdf/2409.12517
  - ç”¨ 2T token è®­äº†ä¸ª 7B modelï¼Œå‘ç° fp8 çš„è®­ç»ƒä¸ç¨³æ¥è‡ª SwiGLU å¯¼è‡´çš„å¼‚å¸¸å€¼æ”¾å¤§ï¼Œå¹¶ç”¨ Smooth-SwiGLU æ”¹è¿›ä¹‹ã€‚
- E=ã€ŠMassive activations in large language modelsã€‹ - 2024.02 - https://arxiv.org/pdf/2402.17762
  - æå°‘æ•°è¶…å¤§ outlier æ¿€æ´»å€¼æ™®éå­˜åœ¨äºå„ LLMï¼ˆä¹ƒè‡³å¤§å‡º 10 ä¸‡å€ï¼‰ï¼Œæ–‡ä¸­å«è¿™ outliers ä¸º massive activationsï¼ˆä¸”è§äº paper æ ‡é¢˜ï¼‰ã€‚
    - æ­¤æ–‡å¹¶ä¸æ˜¯è®² FP8 è®­ç»ƒæ‰å¦‚æ­¤ã€‚è€Œæ˜¯å„ç§ç²¾åº¦çš„éƒ½æœ‰å¯èƒ½
  - æŸäº›ç»´ã€æŸäº› token æ‰å®¹æ˜“å‘ç”Ÿ
    - ä¸æ˜¯æ‰€æœ‰ channel éƒ½ massiveï¼šoutliers æ€»æ˜¯å‡ºç°åœ¨æŸäº› channel ç»´åº¦ï¼ˆä¸”å‡ºç°å‡ ç‡å¾ˆå°ï¼‰ã€‚
    - ä¸æ˜¯æ‰€æœ‰ token éƒ½ massiveï¼šåœ¨ä¸€äº›ç‰¹æ®Š token ä¸Šï¼ˆèµ·å§‹ <BOS>ã€å¥å· â€œ.â€ã€æ¢è¡Œç¬¦ \nã€åˆ†éš”ç¬¦ç­‰ï¼‰æ‰å¦‚æ­¤ã€‚
    - <img width="1162" height="658" alt="image" src="https://github.com/user-attachments/assets/293d84b3-e8b3-471c-a250-1d7633336fb2" />
  - ä»–ä»¬èµ·çš„ä½œç”¨æ˜¯ biasesï¼Œè‹¥å»æ‰ä¹‹ä¼šæ€§èƒ½ä¸‹é™ï¼ˆMassive activations act as fixed but important biases in LLMsï¼‰ã€‚attn ä¸­ç›¸å½“äºéšå¼ bias

> Although significant progress has been made in inference quantization (è§ä¸‹é¢å¼•æ–‡F,G), there are relatively few studies demonstrating successful application of low-precision techniques in large-scale language model pre-training (è§ä¸‹é¢å¼•æ–‡ D).
>
> ã€æ¨ç†é‡åŒ–æœ‰è¿›å±•ï¼Œä½†æ˜¯å¤§è§„æ¨¡ä½ç²¾åº¦è®­ç»ƒçš„æˆåŠŸè¿˜æœªè§ã€‘
- æ¨ç†æ—¶é‡åŒ–ï¼š
  - Fï¼šã€ŠGptq: Accurate post-training quantization for generative pre-trained transformersã€‹- 2022.10 - https://arxiv.org/pdf/2210.17323
  - Gï¼šã€ŠSmoothquant: Accurate and efficient post-training quantization for large language modelsã€‹ - 2022.11 - https://arxiv.org/pdf/2211.10438
- Dï¼šã€ŠScaling FP8 training to trillion-token llmsã€‹- 2024.09 - ï¼ˆæ³¨æ„ä¸Šé¢ä¹Ÿå‡ºç°äº†ï¼‰ https://arxiv.org/pdf/2409.12517
  - ç”¨ 2T token è®­äº†ä¸ª 7B modelï¼Œå‘ç° fp8 çš„è®­ç»ƒä¸ç¨³æ¥è‡ª SwiGLU å¯¼è‡´çš„å¼‚å¸¸å€¼æ”¾å¤§ï¼Œå¹¶ç”¨ Smooth-SwiGLU æ”¹è¿›ä¹‹ã€‚

> To address this challenge and effectively extend the dynamic range of the FP8 format, ã€äºæ˜¯æ¨å‡º deepseek-v3 çš„è§£æ³•ã€‘
>
> ï¼ˆ1ï¼‰ã€we introduce a fine-grained quantization strategy: tile-wise grouping with 1 Ã— ğ‘ğ‘ elements or block-wise grouping with ğ‘ğ‘ Ã— ğ‘ğ‘ elements. The associated dequantization overhead is largely mitigated under our increased-precision accumulation process, a critical aspect for achieving accurate FP8 General Matrix Multiplication (GEMM).
>
> ï¼ˆ2ï¼‰ã€Moreover, to further reduce memory and communication overhead in MoE training, we cache and dispatch activations in FP8, while storing low-precision optimizer states in BF16.
>
> ï¼ˆ3ï¼‰ã€We validate the proposed FP8 mixed precision framework on two model scales similar to DeepSeek-V2-Lite and DeepSeekV2, training for approximately 1 trillion tokens (see more details in Appendix B.1). Notably, compared with the BF16 baseline, the relative loss error of our FP8-training model remains consistently below 0.25%, a level well within the acceptable range of training randomness.
