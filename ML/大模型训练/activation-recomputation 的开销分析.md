> 一个深度网络，共 $n$ 层，每层结构和大小相同。那么如果用 activation-recomputation，为了最大节约显存，应该分成作多少个 block？这时候显存、计算分别是多少？

答案是应该分成 $\sqrt{n}$ 个 block。且只要划分得粒度较粗，则无论怎样划分，training 计算都是增加 1/3。

问题由来：《ZeRO: Memory Optimizations Toward Training Trillion Parameter Models》 https://arxiv.org/pdf/1910.02054 3.2 节提到的这个问题。它来自引入 activateion-recomputation 的 《Training deep nets with sublinear memory cost》一文。

下面是详细解释（由 chatgpt 做了整理）：

---

### 1. 背景

* 我们有一个深度网络，共 $n$ 层，每层结构和大小相同。
* 正常情况下，反向传播需要用到正向时每一层的激活值，如果全部保存，显存占用会很大。
* **激活重计算** 的思路是：在反向传播时，不保存中间激活，而是先保存少量“检查点”，然后需要时临时重算中间激活，从而减少显存占用。

### 2. 分块策略

* 将 $n$ 层网络切成 $K$ 个连续的**块**（checkpoint 位置）。
* 每块的输入（即前一块的输出）需要保存下来，假设每个输入大小为 1（单位内存），那么保存这些输入需要 $K$ 个单位显存。
* 在反向传播某一块时，需要先从保存的输入开始，**重算这一块的正向激活**，这一块长度为 $n/K$ 层，所以额外会临时占用 $n/K$ 个单位显存。
* 总显存占用 ≈

  $$
  \underbrace{K}_{\text{保存的块输入}} + \underbrace{n/K}_{\text{重算中临时激活}}
  $$

### 3. 最优分块方式

* 为了让显存占用最小，令：

  $$
  K + \frac{n}{K} \quad \text{最小化}
  $$
* 这是个标准的均衡问题，在 $K > 0$ 时最小值出现在：

  $$
  K = \sqrt{n}
  $$
* 最小显存占用为：

  $$
  K + \frac{n}{K} = 2 \sqrt{n}
  $$

### 4. 计算开销分析

* 不重算时：

  * 正向：耗时 $t$
  * 反向：耗时 $2t$ （本来就有的一个结论：反向传播是正向的2倍计算量）
  * 总耗时：$3t$
* 重算时：

  * 反向传播每块前要先重做一次该块的正向计算，总共等效多做一次正向传播（耗时 $t$）。
  * 所以总耗时：

    $$
    正向2t + 反向2t = 4t
    $$
    
* 相比不重算，多了：

  $$
  \frac{4t - 3t}{3t} = \frac{t}{3t} \approx 33\%
  $$

上面计算过程中，不涉及怎么划分各层。
