# QAT: 《Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference》 https://arxiv.org/pdf/1712.05877 google

现在主流的 QAT 方法，基本都源自 google 这篇 paper 的思路，包括 PyTorch 的 QAT 实现。

它以量化感知方式训练，然后再 inference 的时候，用全 int 推理。

对该paper 来说，是权重与激活都量化。

## QAT 训练：simulated quantization

网络内插入 FakeQuantize： `r' = dequant(quant(r))`, 而在反向传播时，仿佛没有该操作（因 dequant、quant 不可导），而是直接梯度回传。

<img width="588" height="552" alt="image" src="https://github.com/user-attachments/assets/efecb52b-208c-4a40-b82b-ccf088c73ddb" />

$$
\begin{cases}
\text{clamp}(r) &:= clip(x, r_{min}, r_{max}) &// 值约束于 min, max 内 \\
\text{step} &:= \frac{r_{max} - r_{min}}{n - 1} &// 量化步长；n是分桶数。\\
\text{fakeQuant}(r) &:= \left\lfloor \frac{\text{clamp}(r) - v_{min}}{\text{step}} \right\rfloor \text{step} + r_{min}
\end{cases}
$$

- 它所用的量化是 $[r_{min}, r_{max}] \rightarrow [0, n]$，量化到了 uint8
  - 其中 $r_{min}, r_{max}$ 会调整使得 0.0 量化后为整数（记为 Z，这就是 `r = S(q − Z)` 中 Z 由来）
    - `r = S(q − Z)` 中: 量化为 signed int8, 则不需 Z. 引入 Z 是为了 dequant(quant(0.0) = 0 (零经常有特殊含义，所以需要保持）
- quant(r) 是整数，而 dequant(.) 是浮点数。

## QAT 推理

**推理数据类型：全整数**

paper 中推理时数据类型：
- 只用整数（如上图左可见确实如此）
 - 权重 和 激活 都是 int8
 - 累加器 用 int32，以避免溢出。
 - bias 用 int32，因偏置误差累积会影响整体精度

torch QAT: 对于被量化的算子也如此。

**怎么做到的全 int 网络推理：**

该 paper 中是整个 tensor 用一个 scaling 参数。

它基于这么一点：一个 tensor 可以用 [q, S] 两部分来表示（这里忽略 Z），即 `ori_val = q S`, 其中 q 是 int8 量化部分，S 是 float scaling。网络中每个算子的input 和 output 的 tensor 参数都要用 [q, S] 表示。

<img width="526" height="548" alt="image" src="https://github.com/user-attachments/assets/9eebe282-ddc4-4ff1-82b0-2a62827938d7" />

对于矩阵乘：用 [q, S] 表示，则 $Y=XW$ 就是 $[q_{xw}, S_{xw}] = [q_x, S_x] \times [q_w, S_w]$, 故关键在于获得 $[q_{xw}, S_{xw}]$。

对 Y=XW:
- 一方面， $Y = S_x \ quant(X) \cdot S_w \ quant(W)=（S_x\ S_w)\cdot(quant(X) \ quant(W))$, 其中 $S_x, S_y \in \mathbb{R}$ 是量化缩放因子。
- 另一方面，若假设 XW 已算出，则 $Y = S_{xw} \ quant(XW)$

于是 $quant(Y) = quant(XW) = \frac {S_x\ S_w}{S_{xw}}(quant(X) \ quant(W)$。这样得到了 Y 的 $[q_y, S_y]$ 表示。【假设下一层是 $Z=Y W_1$, 则操作 $[q_y, S_y] \times [q_{w1}, S_{w1}]$ 即可】

$S_x, S_w, S_{xw}$ 都是可以提前 offline 算好的【`S = (range_max-range_min) / bucket_num`, 所以关键在于选好 r_max, r_min】，于是 $M := \frac {S_x\ S_w}{S_{xw}}$ 可以提前算好。但是 S 是浮点数，从而 M 也是浮点数，而这里想要全整数计算，怎么办？作者注意到 M 总是在 [0, 1] 之间。于是 M 可以找到一个近似表示： $M = \frac {M_0} {2^{n}}$, 其中 $M_0$ 是整数，n 是某一整数（非量化的bit数）。当要乘 M 的时候，先乘 $M_0$, 然后左移 n 位即可。

---

### QAT 使用时机

一般是先 float 预训练 → QAT fine-tune。如果一开始就 QAT 训练，则更难收敛，因为模型一开始就承受量化误差。且收敛速度慢，需要更长训练时间。
