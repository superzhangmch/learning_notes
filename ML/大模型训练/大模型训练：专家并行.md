### 什么是 MOE

transformer MOE，其实就是把 FFN 扩展成并行的多个，并在入口处放一个 router，由router 来决定执行多个 FFN-专家中的哪几个，并在算出 FFN 后专家结果求和 merge 到一起，作为 FFN 层的输出。如下：

<img width="1200" height="670" alt="image" src="https://github.com/user-attachments/assets/bf199922-c3bd-421c-9672-d22404822f51" />

从算法层面，既然弄了多专家，那么就需要发挥专家潜力，那么每个专家就需要担当的事一样多。所以需要专家路由的均衡。这往往是靠特殊 loss正则项来实现的。 

### 为什么要并行

这群专家（FFN）都是同样地位的对等并列关系，所以天然适合并行：部署到不同的节点上，通过网络交互把上一层 output 传给它们。另外，MOE 层的参数占比很大，显存占用很可观（可能超单卡容量），这也导致必须对专家们分散存储。

### 网络流量问题

但是注意看：每个序列的每个 token，都是独立路由的，没法提前知道会路由到哪个专家。也就是说，并行训练的时候 $batch \times seq$ 这么多的 tokens, 会打散到不同的专家上；而 inference的时候，对于当前 step，正在生成的 batch 个 token，也是打到不同专家上。这就引出了一个问题是，MOE 会带来不少的网络流量。

进 FFN-experts-group 的 inbound 网络流量是 $batch \times seq \times dim$，outbound 也是这样。但是这些网络流量，并是不把连续存储的东西 batch 方式收发的。而是完全零散的。所以本来是一个 all-to-all 的通信问题（是两次：收一次，算完发一次），但是因为每个token  路由的不确定性，但是 `src -> 某专家` 的传输数据量并不确定。这更增加了网络交互的复杂性。所以 deepseek 的专家并行库的很重要一点就是实现高效的 all-to-all。

----


