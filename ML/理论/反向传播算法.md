# 反向传播算法


神经网络一般用反向传播算法作梯度下降来训练。

看似高大上，说穿了，就是数学里的多元函数按偏导数求极值。只是在数学中，往往只关注逻辑上能不能/存在不存在，不关注具体怎么高效做。BP 反向传播只是令这个求偏

导过程可以高效进行而已。

<br>

神经网络最终的损失函数如果展开来，就是关于所有待训参数的函数。全展开一下子把偏导都求出来效率毕竟低下。神经网络是按类似积木一样层层搭建起来的，基本单元是一个个的 $\text{actFunc}(x\cdot W+b)$ 。这导致损失函数其实是层层递归出的复合函数，于是当然可以用复合函数求导的链式公式。loss是个很复杂的复合函数，如果展开为复杂的长长的求导链，也不见得方便多少。

实际上，有这么个求导法则可用：

假设有函数 $f(u_1, u_2, .., u_n)$ , $u_1, u_2, .., u_n$ 都是 x 的函数，那么有 f 关于 x 的函数可以表达为: 

$$\frac {df}{dx} = \sum_{i=1}^n \frac {∂f}{∂u_i} \frac {du_i}{dx}$$

对于任何一个神经网络中的单元（不一定是神经元，也可能是比向量积计算等等），往往由3部分构成：
1. 多维的输入，
2. 内部的多维参数，
3. 多维的输出；

写成表达式是 （intn=内部的多维参数）

$$\text{output}=f(\text{input}, \text{intn})$$

最终的 Loss=L 函数，可以说是关于 output 的函数，也可以把 output=o 展开说是关于 input 或 internal_parameter=intn 的函数。但无论如何，只要能求得 

$$\frac {∂(L)}{∂(o)}$$

根据前面求导法则

$$\frac {d(L)}{d(\text{intn})}$$ 

是可以求得的。

【注意 internal_parameter 是多维的；据上面法则有： 

$$\frac {d(\text{L})} {d(\text{intn}ᵢ)} = \sum_{j=1}^{o.size}{ \frac {\partial(\text{L})}{\partial(o_j)} \frac {d(o_j)}{d(\text{intn}_i)}}$$

】

同理， $\frac {d(L)}{d(input)}$ 也是可以求出的。

这时候问题在于怎么求出 $\frac {d(L)}{d(o)}$ 。但请注意，此神经单元的input其实来自上一个单元的output，同时其output又会作为下一层的input。因此，如果能求出 $\frac{d(L)}{d(o)}$ ，则 $\frac {d(L)}{d(input)}$ 可得，从而前一层的output的梯度可得；如果能求得后一层的output的梯度，则后一层的input的梯度也就是本层的 $\frac {d(L)}{d(o)}$ 迎刃而解。这不就是归纳法一样的递归嘛。从最后一层往前递归求解好了。

所以BP应该说是链式递归求导。

<br>

另外，RNN等带循环结构的神经单元的反向传播会有所不同，但是背后的道理一样。本质上还是数学上的求偏导。

参：
- https://www.zhihu.com/question/27239198?rf=24827633
- http://www.cnblogs.com/charlotte77/p/5629865.html


