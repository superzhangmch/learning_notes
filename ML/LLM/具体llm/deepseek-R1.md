单从paper看：

对于采用用的 model $\pi_{old}$:

| 策略 | PPO | GRPO（其论文实现） |
|-------|-----|----------------------|
| π_old 更新频率 | 每隔 N 步 | ✅ 每步都同步 π_θ |
| 采样策略 | 用 π_old 采样 | ✅ 也用 π_old，但它刚同步来 |
| 是否 off-policy | 是（延迟一点） | 几乎 on-policy（同步） |
| 理由 | 保稳定性 | 简化架构，不用 critic，靠 group reward 做稳定控制 |
 
----

### PPO

bellow by chatgpt：

在PPO（Proximal Policy Optimization）中，确实会涉及到 **采样模型（actor）** 和 **被训练模型（policy）** 的关系。

PPO的一般流程是这样的：

1. 用当前策略（policy，也就是 actor model）与环境交互，**收集一批轨迹（trajectory）数据**，比如收集T步数据。
2. 使用收集到的数据进行**多次训练（epochs）**，每次使用 minibatch 对策略和价值函数进行更新。
3. 更新完之后，**替换采样模型为当前的策略模型**，然后开始下一轮采样。

---

### 回答你的问题：

#### 1. 是不是每次参数更新后都复制更新采样的 model？

不是每次参数更新都同步复制。通常是：

> **每采样完一批数据（称为 rollout 或者 batch）之后才更新策略，然后才用于下一轮采样。**

也就是说，在这一个 batch 的采样过程中，使用的 policy 是固定的。只有在训练结束后，才将这个新参数用于下一轮数据采样。

---

#### 2. 多少步做一次更新？

这个取决于你设置的 **采样步数 T 或 batch size**，比如：

- 一般会采样 **2048** 步（timesteps），然后在这 2048 步的数据上训练几轮（比如 10 个 epochs，每轮再分成若干个 minibatch）。
- 比如 PPO 的经典实现（OpenAI Baselines / stable-baselines3），默认设置：
  - `n_steps=2048`（也就是每2048步做一次训练更新）
  - `batch_size=64`
  - `n_epochs=10`

然后训练完后，再用这个更新后的模型继续采样新的2048步。

---

### 总结一句话：

> **PPO 并不是每更新一次参数就同步采样模型，而是每采样一批完整的数据（比如2048步）之后，再训练更新，更新后才用于下一轮采样。**

