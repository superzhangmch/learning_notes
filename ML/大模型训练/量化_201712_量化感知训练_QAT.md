# QAT: 《Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference》 https://arxiv.org/pdf/1712.05877 google

现在主流的 QAT 方法，基本都源自 google 这篇 paper 的思路，包括 PyTorch 的 QAT 实现。

它以量化感知方式训练，然后再 inference 的时候，用全 int 推理。

## QAT 训练：simulated quantization

网络内插入 FakeQuantize： `r' = dequant(quant(r))`, 而在反向传播时，仿佛没有该操作（因 dequant、quant 不可导），而是直接梯度回传。

<img width="588" height="552" alt="image" src="https://github.com/user-attachments/assets/efecb52b-208c-4a40-b82b-ccf088c73ddb" />

$$
\begin{cases}
\text{clamp}(r) &:= clip(x, r_{min}, r_{max}) &// 值约束于 min, max 内 \\
\text{step} &:= \frac{r_{max} - r_{min}}{n - 1} &// 量化步长；n是分桶数。\\
\text{fakeQuant}(r) &:= \left\lfloor \frac{\text{clamp}(r) - v_{min}}{\text{step}} \right\rfloor \text{step} + r_{min}
\end{cases}
$$

- 它所用的量化是 $[r_{min}, r_{max}] \rightarrow [0, n]$, 其中 $r_{min}, r_{max}$ 会调整使得 0.0 量化后为整数（记为 Z，这就是 `r = S(q − Z)` 中 Z 由来）。
- quant(r) 是整数，而 dequant(.) 是浮点数。

## QAT 推理

**推理数据类型：全整数**

paper 中推理时数据类型：
- 只用整数（如上图左可见确实如此）
 - 权重 和 激活 都是 int8
 - 累加器 用 int32，以避免溢出。
 - bias 用 int32，因偏置误差累积会影响整体精度

torch QAT: 对于被量化的算子也如此。

**怎么做到的：**

该 paper 中是整个 tensor 用一个 scaling 参数。

对 Y=XW:
- $Y = S_x \ quant(X) \cdot S_w \ quant(W)=（S_x\ S_w)\cdot(quant(X) \ quant(W))$, 其中 $S_x, S_y \in \mathbb{R}$ 是量化缩放因子。
- $Y = S_{xw} \ quant(XW)$

于是 

---

### QAT 使用时机

一般是先 float 预训练 → QAT fine-tune。如果一开始就 QAT 训练，则更难收敛，因为模型一开始就承受量化误差。且收敛速度慢，需要更长训练时间。

### 其他

- 为什么 paper 中 `r = S(q − Z)` 要减 Z：以为它是要量化到 uint8，如果是 int8，则不需要这不操作。
