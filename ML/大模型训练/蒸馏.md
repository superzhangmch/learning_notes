 
# 原始paper 《Distilling the Knowledge in a Neural Network》 https://arxiv.org/pdf/1503.02531

教师模型输出 logits： $z_i^{(t)}$ ，学生模型输出 logits： $z_i^{(s)}$

引入 softmax 温度 (T)：

$$
p_i^{(t)}(T) = \frac{e^{z_i^{(t)} / T}}{\sum_j e^{z_j^{(t)} / T}}, \quad
p_i^{(s)}(T) = \frac{e^{z_i^{(s)} / T}}{\sum_j e^{z_j^{(s)} / T}}
$$

蒸馏目标： $Loss = (1-\alpha) L_{\text{hard}} + \alpha T^2 L_{\text{soft}}$

其中：

* $L_{\text{hard}}$ ：学生对真实 one-hot 标签的交叉熵； hard 指的是 hard-label，下面 soft 指的 soft-label。
* $L_{\text{soft}}$ ：学生与教师的全词表的概略分布的交叉熵（或 KL 散度）： $L_{\text{soft}} = - \sum_i p_i^{(t)}(T) \log p_i^{(s)}(T)$
* T：softmax 温度，用来“软化”概率分布；
* $\alpha$：控制两部分损失的权重；
* $T^2$：梯度缩放补偿（Hinton 原论文特别强调这一点）。

温度的作用:

* 当 (T=1)，softmax 输出通常非常尖锐（接近 one-hot）；
* 当 (T>1)，输出分布更平滑；
* 这使学生能学习到 **教师的“相对判断”**，比如：

  * 教师对“猫 vs 狮子”的相似性；
  * 教师对“汽车 vs 狗”的明显区分。

这种信息在 one-hot 标签中是完全丢失的。
 
---

### LLM 中怎么做

如果能获得 LLM 的 logits，则也是用 `loss = L_soft + L_hard = KL(Teacher_logits || Student_logits) + CE(Student, Ground_truth)` （这里不考虑weight）。注意 KL 和 CE 其实是等价的。如果拿不到预测的词的分布，才退化为只用 L_hard。

另外，必要的时候还可以对中间层作拟合，即特征蒸馏：对齐中间层的hidden states、对齐attention maps。

参考： https://zhuanlan.zhihu.com/p/1908894189313323730
