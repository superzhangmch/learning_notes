# 模型的方差与偏差

我们常说到模型的方差与偏差，比如 bagging 降低方差，boost 降低偏差等等，有时难免会很费解。

原因在于对模型的方差与偏差到底指什么，不甚清楚。

衡量模型算法好坏的一个方法是衡量其泛化误差的大小。针对一个具体的训练集，训练出的模型在测试集上表现好，不见得换一个同分布训练集后训得的模型仍然如此。算法的泛化性能或者说泛化误差，应该是不取决于训练集的选取的，应该是所有同分布同规模训练集构成的集合上的泛化能力的平均。

据周志华《机器学习》，以上方式定义的泛化误差，可以分解为`偏差 + 方差 + 噪音`三部分。误差由这三部分组成：

xxxx 
(x 代表固定的某测试样本, D 代表训练集，f 代表预测模型，f:=E_D[f(x,D)]表示f的平均，y_D代表测试集中x的标注， y表示x的真实标注，y_D!=y则表示有标注噪声)

于是，说模型的方差大，就是对于一个具体的测试集，多次选不同训练集，训得的多个模型实例在测试集上的预测方差大。不恰当的极端情况例子，假设测试集只有一个样本，那么多个模型实例对一个测试样本的预测结果作为一个集合，有较大的方差。方差可能是数据本身问题导致，如果数据没问题，那么就在于算法了。

模型的方差，反映了换一批训练集训练出的模型的稳定性（是不是与前一次的模型比较接近）。往往说过拟合的模型方差大，就是这个意思。

说模型的偏差小，也是类似，指的训练集组成的集合训练出的多个模型实例对同一个测试集而言。

（按直观理解，模型偏差就是预测值与直接值的均值diff大小，模型方差就是预测值作为一个集合的方差。这样理解是片面的。）
