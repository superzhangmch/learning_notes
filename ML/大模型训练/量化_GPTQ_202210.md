《GPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFORMERS》 https://arxiv.org/pdf/2210.17323

GPTQ 继承自 OBQ(Optimal Brain Quantization，2022), 而 OBQ 又是来自 OBS（Optimal Brain Surgeon，1993）。

这一类方法做的是训练后的量化。它针对的是 inference 中矩阵乘法的权重 w，而不管激活。它的基本思路是，选一批随机 sample 做 inference，从而可以得到每一个矩阵乘的激活 input X。对网络中的一个具体权重矩阵 W，寻找最佳量化 $\hat{W}$ 满足 $\text{argmin}_{\hat{W}} ∥W X − \hat{W} X∥_2^2$。

也就是说，用它做量化时，不关心网络结构是什么，原始 loss 是什么，而只需要知道一个权重 W 的典型 input X 啥样，就可以对 W 做量化。

GPTQ 文中试了 4 bit, 乃至于 2 bit 量化。

----

## 理论

发展脉络是 OBS => OBQ => GPTQ。但是背后的基本原理是一样的。看一些资料，OBS 是想把不重要的权重给置零，OBQ 是用同样的参数筛选方式，但是不是置零而是量化之。这里只看 GPTQ paper 中介绍的 OBQ。

### OBQ - Optimal Brain Quantization，2022

**（1）、思路**

如前所述，它只处理矩阵乘法，且只处理里面的权重。对一个具体权重矩阵 W，它的量化策略是，寻找 $\hat{W}=quant(W)$ 使得 $\hat{W}$ 满足 $∥W X − \hat{W} X∥_2^2$ 最小，其中 X 是一批样本做 inference 得来。

那么每个权重 $w \in W$ 都取最接近量化，这样可行吗？这样做都没反映出最小化 $∥..∥$，当然不好！。实际上，鉴于量化后的 $\hat{W}$ 是 “量化“的，也就是有限取值离散的，OBQ 真实想做的是：如果把所有可行解全遍历搜索一下，就能得到最佳 $\hat{W}$ 了。当然这样做会比较低效。所以 OBS/OBQ 在于有一套方法，能更高效低做，当然它的解法也不是全局最优解，只是比每个权重 w 取最接近量化要好。

注意，鉴于 OBQ 真实想做（但做不到的）的是全遍历搜出最佳解，所以对最终量化后的每个 $w \in \hat{W}$ 只是处于某一量化位而已，并不必然保证与原始 w 的数值差很小。

**（2）、引出 Hessian 矩阵**

鉴于 OBQ 是在最小化 $∥W X − \hat{W} X∥_2^2$，也就是以它为目标，可以列出 loss 如下；为方便理解，符号上令原权重是 $W_0$, 量化后结果权重是 $W$：

$$
Loss(W) = ∥W_0 X − W X∥_2^2
$$

一眼可看出该 loss 的最优解是 $W=W_0$，此时 loss = 0。岂不都不用优化了？注意这是个带约束的优化问题，约束是每个 w 都应该处于某一量化整点位。

对它做 $W_0$ 点的 Taylor 展开，注意 L 是二次的，所以正好可以精确展开到 2 阶：

$$
L(w) = L(w_0) + \nabla_w L(w_0)^\top (w-w_0) + \tfrac{1}{2}(w-w_0)^\top \nabla_w^2 L(w_0) (w-w_0)
$$

注意：
- 在 $w_0$ 点 L(w) 取得最小值，也就是 $\nabla_w L(w_0) = 0$
- $\nabla_w^2 L(w_0)$ 就是 Hessian 矩阵，记作 H。它乃对称矩阵，可算得等于 $2XX^T$。
  - 若 X 是一条样本的，则 X 是列向量， $XX^\top$ 是外积。如果 X 是多条数据，则 X 是列向量的拼接，X = { $x_1, x_2, .., x_n$ }, $XX^\top = \sum_i x_i x_i^\top$。

于是 $L(w) = L(w_0) + \tfrac{1}{2}(w-w_0)^\top H (w-w_0)$, 而任一种量化方案 loss 差是 $\Delta L = L(w) - L(w_0) = \tfrac{1}{2}(w-w_0)^\top H (w-w_0)$。于是，OBQ 的量化思想归结为选择 $W$ 使得下式最小：

$$
\tfrac{1}{2}(w-w_0)^\top H (w-w_0) = (w-w_0)^\top X X^\top (w-w_0)
$$

**（3）、启发式逐权重量化：思路**

- 权重矩阵按行独立处理： 仿佛权重矩阵本来只是 1 x n 的。
  - $∥..W..∥_2^2 = \sum_{row} ∥..W_{row}..∥_2^2$, 所以可以一次处理一行
  - 权重 W 一般是矩阵，设 x 是 batchsize=1 的，Wx 是向量。OBQ 一次只考虑 W 的一行，这样 Wx 是 scalar 数字。
- 一次量化一个参数：启发式逐个推进
  - 对待量化参数 { $w_i$ }, 找到最适当的一个并量化它，同时补偿调整其他未量化参数的取值，使得 $\Delta L(w)$ 可控。

下面说下它这个启发式逐参数量化咋做的。

它是逐参数量化的，对于已经量化的就不再管了（而未量化参数集合则是每次迭代少一个）。于是每量化一个参数的时候，仿佛面对的就是原始参数向量 W 一样（只是长度短了）。

一次量化一个参数，但其他的参数也会补偿更新——也就是一个迭代其实更新了所有参数。对权重 $W$ = { $w_1, w_2, .., w_n$ }，假设选定要量化的是 $w_q$，更新后的这n个参数是 $w_q^{'}$ 与 $w'_{F}$ (下标 F 表示不包含 q 的所有index)。那么这些更新应该满足：
- $w_q$ 按量化规则量化得到的 $w_q^{'}$, 而其他参数的更新方法应该是： $w_q \rightarrow w_q^{'}$ 的基础上最小化 $\Delta L$, 也就是
  
$$
w_{F}^{'} = argmin_{w_{F}} L(\{ w_q^{'}, w_{F} \})
$$

- 反过来，对 $w_q$ 来说，它如果知道 $w_{F}$ 会怎样根据 $w_q^{'}$ 而更新，它应该把这点考虑进去——这就是被优化参数的选择原则：作量化的参数 index 是

$$
  index = argmin_{q}\ \text{L}(\\{quant(w_q), argmin_{w_{F}}\ \text{L}(\\{ quant(w_q), w_{F} \\}) \\})  \ \ 或者说
$$

$$
\begin{cases}
w_{F_new} := argmin_{w_{F}}\ \text{L}(\\{ quant(w_q), w_{F} \\}) \\
index = argmin_{q}\ \text{L}(\\{quant(w_q), w_{F_new} \\})
\end{cases}
$$

- 上面颇有点博弈论的那意思。

根据以上，那么 $w_1, w_{2:n}$ 的更新公式就容易推导了：令 $e_q = quant(w_q) - w_q$，推导出其他 n-1 个参数的最佳补偿更新 $\delta_F$, 然后把 $\delta_F$ 代入 $\Delta L$, 并选出最适合(即使得 $\Delta L$ 最小)量化的那个权重。

**（3）、启发式逐权重量化：具体怎么做**

假设选定对第 $q$ 维量化，记量化差 $e_q := {quant}(w_q)-w_q$, 并记其余 n-1 各参数的补偿更新改变量是 $\delta_F$，则有如下分块表达式：

$$
\Delta L = \tfrac{1}{2} {\Delta w}^\top H {\Delta w} = \tfrac12
\begin{bmatrix}
e_q \\ , \delta_F
\end{bmatrix}^{\top}
\begin{bmatrix}
H_{qq},  H_{qF} \\
H_{Fq},  H_{FF}
\end{bmatrix}
\begin{bmatrix}
e_q \\ , \delta_F
\end{bmatrix},
$$

注意 Hessian H 是对称的， $H_{qF}^\top = H_{Fq}$, $e_q$ 与 $H_{qq} 都是标量数字。

现在要固定 $e_q$ 把最优 $\delta_F$ 求出来。可以得出其最优解是 $\delta_F^\star = -H_{FF}^{-1} H_{Fq} e_q$。

**怎么求：**

这可以用直接展开 $\Delta L$ 并求导得到：展开 $\Delta L$ 的分块矩阵乘法表达式， 并关于 $\delta_F$ 求导有：

$$
\begin{cases}
f(\delta_F) := \Delta L = \tfrac{1}{2} H_{qq} e_q^2 + \tfrac{1}{2} e_q (H_{qF}\delta_F + \delta_F^\top H_{Fq}) + \tfrac{1}{2}\delta_F^\top H_{FF}\delta_F \\
\nabla_{\delta_F} f(\delta_F) = H_{FF}\delta_F + e_q H_{Fq}
\end{cases}
$$

而最佳 $\delta_F$ 是满足 $\nabla_{\delta_F} f(\delta_F) = 0$ 的，于是解 $H_{FF}\delta_F + e_q H_{Fq} = 0$ 得 $\delta_F^\star = -H_{FF}^{-1} H_{Fq} e_q$。

再把 $\delta_F^\star$ 表达式代入 $\Delta L$, 又可得：

$$
\Delta L_{\min} = \frac{1}{2} (H_{qq} - H_{qF} H_{FF}^{-1} H_{Fq} ) e_q^2
$$

于是需量化的那个参数下标 q 的即能令上式取值最小的那个。


----

## 实际操作

（1）、对于一个参数，怎么量化它的？用 scaling 吗

（2）、
