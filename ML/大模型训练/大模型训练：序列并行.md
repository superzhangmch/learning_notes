# transormer 训练的序列并行

transormer 的 hidden state shape = [batch, seq_len, hid_dim], 或者 [seq_len, batch, hid_dim]。除了 attn 之外不分，都是在单个token上做的，也就是 seq维度和 batch 维没区别，它们会被当做 seq*batch 的大batch。

所以序列并行，本质上来说，只和处理 attn 有关。

而 MHA，就单个 head 来说，是 softmax(QK')*V' 的形式，本质上还是矩阵乘法操作（即使 MLA 等 attention，仍可以拆为基本运算是矩阵乘法），而张量并行，就是把大的矩阵乘法拆为分块矩阵乘法，在不同 gpu 上算。从这个角度来说，序列并行和张量并行，解决的都是同一个问题。只是张量并行是更泛的，而序列并行处理的是某一种特殊矩阵乘法，从而可以更有针对性地优化而已。

序列并行和张量并行都在解决“算力和显存不够，如何把大算子拆到多 GPU 上”的问题。区别在于：
- 张量并行 = 矩阵通用切分。
- 序列并行 = 针对序列维度的特殊切分，主要优化 attention 的通信模式。

----

## 一、megatron-3 的序列并行：非真序列并行，只是为了降激活显存

它是和张量并行相结合的用来减少激活显存占用量的方案。确实在 seqence 维度切分了，但是和有助于 attn 计算那样的序列并行，没啥关系。

出自： megatron 第三篇 《Reducing Activation Recomputation in Large Transformer Models》 https://arxiv.org/pdf/2205.05198 ，该文旨在设法降低激活显存的占用。

**背景：** megatron-2 《https://arxiv.org/pdf/2104.04473》 4.2 节讲到，为了方便 MHA 计算时少一些显存 copy，把 hidden state 的 layout 由一般做法的 [B, S, num_heads, head_dim]， 换成了 [seq, Batch, num_heads, head_dim], 这样方便中间两维 batch*num_heads 结合。从这后， megatron 就是 [S, B, H] seq-first 的。笔记在 [这里](https://github.com/superzhangmch/learning_notes/blob/main/ML/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/transformer%20%E7%9A%84%E9%9A%90%E5%B1%82%20shape%EF%BC%9Absh%20%E4%B8%8E%20sbh.md) 。

megatron 发现 transformer 种相关张量并行后面的 layerNorm、dropout 需要存下不少显存，以便反向传播。于是选择对这些显存作切分并分布到不同节点上，用时拉取过来。如果是 batch-first，没准它对batch继续做拆分，但是鉴于它已经是 sequence-first 了，所以很自然地，megatron 选择了按 sequence 维度切分。

**激活显存问题由来 ：**

<img width="1410" height="936" alt="image" src="https://github.com/user-attachments/assets/f1c4ea1f-c632-4de8-ac45-c0e526b41e8e" />

如上图之下，无论是 attn 还是 MLP，在 tensor 并行的最后一步，都是 $\[Y_1, Y_2\]\cdot \[B_1, B_2\]^T = Y_1 B_1 + Y_2 B_2$ 形式。无论是 [B, S, H] 还是 [S, B, H], $Y_1, Y_2$ 都是把 H 维切分了，且与 S 和 B 的顺序无关。 $Y_i \cdot B_i$ 的 shape 和期望结果 shape 一样，但是分散在不同 gpu 上，需要聚合求和到一起，才能做 dropout(layout 是element wise的，如果分布式作，则需要全局一致的随机数种子) 与 layerNorm（layerNorm 是 token wise的，需拿到完整 hidden_dim 数据）。又因为layerNorm 后的下一layer 的输入，在每个 gpu 都需要全量数据，于是每一 gpu 都重复算了 layerNorm。反向传播时，也需要这些激活，因此需要存下来。

作者推算得，对于标准 transformer 来说，单层的总激活显存占用是： $sbh(34+5\frac {as} {h})$, 其中 s=seq, b=batch, h=hidden_dim, a=num_heads， $5\frac {as} {h}$ 乃 attn带来的。通过张量并行，可以把 $sbh(24+5\frac {as} {h})$ 的显存均分到张量并行数 t 上（变成 $sbh(\frac {24} t +5\frac {as} {th})$）。而 $10sbh$ 的显存，对应 dropout + layernrom 处，没法分摊。因此要通过序列并行把这部分显存分摊。

这 $10sbh$ 显存的具体构成（占据了 10/34, 可见占比确实不小）：
- 每个 dropout 占据 sbh, MLP 与 ATTN 后各有一个 dropout（注意 attn 内部 softmax 后还有个小 dropout，不是它）
- MLP 与 ATTN 后各有一个 layerNorm。layerNorm 需要 2bh，总共 4sbh。layerNorm 之后的 MLP 或者 attn QKV proj 的输入，共 4sbh，也需要存下。所以总共 8sbh。

**怎么用序列并行解决问题的：**

可以看到面对的问题，与 sequence 本身并没关系。只是当要切分时，因为 sequence-first，拿它——也就是tensor的第一个维——切分最方便高效（另外，batch未必很大，如果那样，按它切也会不大方便）。总之切分后，每个 gpu 的所有激活显存都是均分存储了。layernorm+dropout 激活均分了，当用的时候，当场拉来就行。

<img width="1496" height="992" alt="image" src="https://github.com/user-attachments/assets/89645a62-7173-4560-8cc9-a49a764993ef" />

注意：通信量没变。张量并行后没序列并行，需要一次 all-reduce 通信；若用了序列并行，需要两次不相邻的 scatter-reduce 与 all-gather，而一次 all-reduce 正好由这两个动作构成。

----

LLM 的**序列并行（Sequence Parallelism）**是一个比较宽泛的概念，目标都是把长序列的计算分布到多个 GPU 上，降低单卡显存压力并提高吞吐。
常见的实现方法主要分为几大类，我给你按**切分维度和通信模式**来分类。

## 1. Context Parallel / Sequence Parallel（Megatron-LM 类）

**代表算法**：

* **Megatron-LM Sequence Parallel**
* **Megatron-LM Context Parallel（CP）**

**思路**：

* 按 **序列长度维度（sequence length）** 把 token 切分到不同 GPU 上。
* 每个 GPU 保存自己那段 token 的激活值，注意力计算需要跨卡取全量 K/V。
* 通常需要 **AllGather / ReduceScatter** 进行跨卡同步。

**优点**：

* 显存占用与序列长度成反比，支持更长 context。
* 对 batch size 没有额外要求。

**缺点**：

* AllGather K/V 会带来较大通信开销，特别是长序列时。

## 2. 交错通信计算（Pipelined Sequence Parallel）

**代表算法**：

* **DeepSpeed-Ulysses**
* **Ring Attention**（严格来说属于通信优化，但也能算到这一类）
* **Tutel Sequence Parallel**

**思路**：

* 不一次性 AllGather 全量 K/V，而是将 K/V 分块（chunking）并环形或流水化传输。
* 每收到一块，就立刻计算对应注意力部分，然后继续传下一块。
* 最终得到完整注意力输出，但通信和计算是**重叠**的。

**优点**：

* 降低峰值显存占用（不需要一次性存全量 K/V）。
* 通信开销被计算掩盖，延迟低。

**缺点**：

* 算法实现更复杂，对拓扑和带宽敏感。

## 3. 双向分片（2D Parallel for Attention）

**代表算法**：

* **Colossal-AI Sequence Parallel**（2D Attention）
* **Mesh TensorFlow GSPMD**（Sequence Sharding + Head Sharding）

**思路**：

* 同时在**序列维度**和**注意力 head 维度**上分片。
* 每个 GPU 只持有自己负责的 token 片段 + 部分 head。
* 通信量减少，因为每个 GPU 计算的注意力规模更小。

**优点**：

* 内存和通信负担更平衡。
* 对大批量多头注意力很友好。

**缺点**：

* 需要额外的并行策略调度和复杂的 AllToAll。

## 4. 流式注意力 / 块稀疏注意力结合并行

**代表算法**：

* **FlashAttention-2/3** + Sequence Parallel
* **Blockwise Sparse Attention + Sequence Sharding**
* **StreamingLLM with Parallel KV Cache**

**思路**：

* 在序列并行的基础上，用流式 attention 或块稀疏减少需要通信的 token 数量。
* KV Cache 按序列分布到不同 GPU，推理时按需访问。

**优点**：

* 长序列训练和推理显存极低。
* 可以动态裁剪注意力计算范围。

**缺点**：

* 对全局依赖建模能力有限（稀疏模式下）。

## 5. 推理专用的序列并行

**代表算法**：

* **PagedAttention**（vLLM）
* **Tensor Parallel + Sequence Sharding for KV Cache**
* **SGLang / LightSeq KV Offload**

**思路**：

* 不在训练时切分，而是在推理时把 KV Cache 按 token 切分存到多 GPU / CPU / NVMe。
* 提升并行吞吐量，减少单卡 KV 压力。

**优点**：

* 推理吞吐高，可扩展到极长上下文。

**缺点**：

* 对训练无帮助，推理场景特化。

如果你愿意，我可以帮你画一张**序列并行算法家族树**，把 Megatron-LM 的 CP、DeepSpeed-Ulysses、Ring Attention、2D Attention 等放在同一个拓扑图上，这样可以直观看它们的差异和关系。
这样你一眼就能看出哪些是“经典切序列”，哪些是“通信优化”，哪些是“多维切分”。

```
                  ┌────────────────────────────┐
                  │   LLM Sequence Parallelism │
                  └────────────────────────────┘
                               │
        ┌──────────────────────┼──────────────────────┐
        │                      │                      │
        ▼                      ▼                      ▼
┌────────────────┐   ┌────────────────────┐   ┌─────────────────────┐
│  1. 经典序列切分 │   │  2. 交错通信计算优化  │   │  3. 双向分片 (2D)    │
│(Megatron-LM CP)│   │ (通信+计算重叠)      │   │ (序列+Head同时切)     │
└────────────────┘   └────────────────────┘   └─────────────────────┘
        │                      │                      │
        │                      │                      │
        ▼                      ▼                      ▼
┌─────────────────────┐  ┌─────────────────────┐  ┌────────────────────────┐
│ Token 按顺序切分到多卡 │  │ 分块KV → 环形/流水通信 │  │ 每卡持部分token+部分head │
│ 全量KV需 AllGather   │  │ 一边收一边算(Overlap) │  │ 降低通信量               │
└─────────────────────┘  └─────────────────────┘  └────────────────────────┘
        │                      │                      │
        │                      │                      │
        ▼                      ▼                      ▼
    代表算法:             代表算法:               代表算法:
  - Megatron-LM SP       - DeepSpeed-Ulysses      - Colossal-AI 2D
  - Megatron-LM CP       - Ring Attention         - Mesh-TF GSPMD
                         - Tutel SP

        ┌──────────────────────────────┬─────────────────────────────┐
        ▼                              ▼                             ▼
┌───────────────────────┐    ┌────────────────────────┐     ┌─────────────────────────┐
│  4. 稀疏/流式注意力结合SP│    │  5. 推理KV并行/缓存切分   │     │  辅助技术                │
└───────────────────────┘    └────────────────────────┘     └─────────────────────────┘
        │                         │                              │
        ▼                         ▼                              ▼
   - FlashAttn+SP             - vLLM PagedAttention         - ZeRO Stage3+SP
   - Block Sparse + SP        - Tensor Parallel + KV Split  - Mixed Parallel
   - StreamingLLM+SP          - SGLang KV Offload           - Checkpoint + SP
```
