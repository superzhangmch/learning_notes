# 《QLoRA: Efficient Finetuning of Quantized LLMs》 https://arxiv.org/pdf/2305.14314

qlora 只是量化权重，不管激活。权重量化到 NF4 4-bit 格式后，在此基础上再作 fp16 的qora fine tune。大思路如此。

### NF4 = 4-bit NormalFloat

因为权重一般都是分布良好的零均值正态分布，不像 activation 可能有 outliers 通道，所以 qlora 把权重缩放后，用正态分布分桶方式量化它。

**（1）、 4 bit 正态分桶构造**

首先是构造 N(0, I) 的 16 分桶（对应 4 bit）。需要满足每个分桶内概率累积 $\int p$ 一样多。构造公式是：

$$
q_i = \frac{1}{2} \left( Q_X \left(\frac{i}{2^k+1}\right) + Q_X \left(\frac{i+1}{2^k+1}\right) \right)
$$

k 表示 k-bit 编码， $q_i$ 表示第 i 桶分界点， $Q_X(.)$ 是累积分布函数 (CDF)的反函数——分位数函数。

这样做的时候，不能精确表示 0（而精确的 0 很重要；为啥如此？大概验证构造式即可知）。于是用上述公式的某非对称形式，总之，所得如下(注意正负非对称）：

```
[-1.0, -0.6961928009986877, -0.5250730514526367,
-0.39491748809814453, -0.28444138169288635, -0.18477343022823334,
-0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,
0.24611230194568634, 0.33791524171829224, 0.44070982933044434,
0.5626170039176941, 0.7229568362236023, 1.0]
```

<img width="1100" height="692" alt="image" src="https://github.com/user-attachments/assets/3f600121-ec6a-464e-b0bc-f2103c00f9fe" />

也就是它的最大最小值分界点，正是标准差 1.

按作者说他们的 NF4 方案是最优的。而据 https://arxiv.org/pdf/2306.06965 《NF4 Isn’t Information Theoretically Optimal (and that’s Good)》，并不如此。
> Their proposed NF4 data type is said to be information theoretically optimal for representing
normally distributed weights. I show that this can’t quite be the case, as the distribution of the
values to be quantized depends on the blocksize.

**（2）、怎么量化**

对原始权重矩阵，flatten 成一行后，截取成不同 blocks，每个 block 上取 absmax 归一化: `block / absmax(block)`, 然后查上面的分桶表，即完成了量化。它不考虑 per-tensor 还是 per-channel，乃 per-block。

一个 diff 是，应该是 block 的标准差对齐到 N(0, I) 的 stdev=1，而非其 absmax。但是就是这样操作了。

### lora

weight 量化后，开始冻结 NF4 weight 并作 fp16 的 lora finetuning: $Y = X W_{NF4} + s X L_1 L_2$， 详细为：

$$
\begin{cases}
Y^{BF16} = X^{BF16} W + X^{BF16} L_1^{BF16} L_2^{BF16} \\
W = dequant(dequant(c_1^{FP32}, c_2^{k\text{-bit}}), W^{4bit})= W^{BF16}
\end{cases}
$$

