一个 LLM model，说训练 x T 的 token。是 train 到这个数字后，loss 不再下降？还是就是不训练了？

ai回答：一般来说，loss 还能下降，但下降速度变慢，收益不再值得，所以就不训了。

大模型预训练中，loss 会对数式下降：初期下降快，后期趋于平缓。验证集 loss 仍在下降，但已经进入“收益极小区”。换句话说，每多训练 100B token，可能 loss 只下降 0.01，对真实 downstream 任务提升几乎没意义，
