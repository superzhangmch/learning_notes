核心思想：用"最大前导零"估算基数                      

### 1. 概率直觉

假设你抛硬币，连续出现 k 个正面的概率是 1/2^k。

如果你观察到最多连续出现了 5 个正面，那大概抛了 2^5 = 32 次左右。
                                                           
HyperLogLog 把这个思路用在哈希值上：
- 把每个 user_id 哈希成二进制数     
- 统计二进制中"前导零的最大个数" 
- 如果最大前导零是 k，估算大约有 2^k 个不同元素

### 2. 分桶降低方差   

单个估计值方差太大，所以用 m 个桶（本例用 16384 个）：

```
hash(user_id) = 0101 1000 1101 0010 ... 
                ^^^^
                前14位 → 决定放入哪个桶 (桶号 0~16383)
                     ^^^^^^^^^^^^^^^^^
                     剩余位 → 统计前导零个数
```

每个桶独立估算，最后取调和平均，大幅降低误差。

### 3. 内存效率惊人
```
┌─────────────┬──────────┬──────────────────────────────────┐ 
│    方法      │ 内存占用  │               说明                │
├─────────────┼──────────┼──────────────────────────────────┤
│ Set (精确)   │ ~5.5 MB  │ 存储 69,252 个 UUID               │
├─────────────┼──────────┼──────────────────────────────────┤
│ HyperLogLog │ ~12 KB   │ 只需 16,384 个寄存器，每个 6 bit    │
└─────────────┴──────────┴──────────────────────────────────┘
```
用 0.2% 的内存，达到 99.5% 的准确率！

### 4. 简化版代码逻辑
```
def add(user_id):
    h = hash(user_id)           # 64位哈希
    bucket = h >> 50            # 前14位 → 桶号
    trailing = count_leading_zeros(h & 0x3FFFFFFFFFFFF)  # 剩余50位的前导零
    registers[bucket] = max(registers[bucket], trailing + 1)

def count():
    # 调和平均 + 偏差修正
    return alpha * m^2 / sum(2^(-register[i])) # m: 分桶数; 
```

这就是为什么 Redis、BigQuery、Spark 都内置了 HyperLogLog —— 用极小内存就能统计海量 UV。

----

### 关于 Alpha 的来源

HyperLogLog 的原始估计公式是：

E_raw = m² / Σ(2^(-register[j]))

但这个估计有系统性偏差，需要乘以 alpha 修正。

理论推导

Alpha 来自对调和平均的期望值分析。当 m 个桶独立且哈希均匀分布时：

$$α_m = 1 / (m · ∫ _0 ^\infty (\log_2((2+u)/(1+u)))^m du)$$

这个积分没有封闭解，但当 m → ∞ 时收敛到：

α_∞ = 1 / (2 · ln(2)) ≈ 0.7213

实际取值

论文给出了不同 m 值的精确常数：
```
┌──────────┬────────────┬────────────────────────┐
│ m (桶数) │ p (精度位) │         alpha          │
├──────────┼────────────┼────────────────────────┤
│ 16       │ 4          │ 0.673                  │
├──────────┼────────────┼────────────────────────┤
│ 32       │ 5          │ 0.697                  │
├──────────┼────────────┼────────────────────────┤
│ 64       │ 6          │ 0.709                  │
├──────────┼────────────┼────────────────────────┤
│ ≥128     │ ≥7         │ 0.7213 / (1 + 1.079/m) │
└──────────┴────────────┴────────────────────────┘
```
代码中的实现：

```
if m >= 128:
    alpha = 0.7213 / (1 + 1.079 / m)  # 渐近公式
elif m == 64:
    alpha = 0.709
elif m == 32:
    alpha = 0.697
else:
    alpha = 0.673
```

直观理解

- 没有 alpha：估计值会系统性偏大
- alpha ≈ 0.72：相当于把原始估计"打个七折"修正回来

这些常数是 Flajolet 等人在 2007 年原始论文《HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm》中通过数学分析得出的。
