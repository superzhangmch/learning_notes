一般意义上，深度学习用 fp32：梯度、参数、激活。

后来有了低精度的训练，意义在于省显存、带宽，并加速计算。

### FP16 精度

fp16/bf16 的表示范围比较大，所以只需要有个全局的 loss scaling 就可以了。

<img width="1314" height="566" alt="image" src="https://github.com/user-attachments/assets/e4150de3-2ab6-4f25-af88-973ed3cc3ae2" />

### fp8/fp4 精度

**意义：**

fp16/8/4 都需要硬件支持。而 fp16 各种硬件都支持，fp8/4只有最新的才能支持。

p8 和 int8 比
- 显存带宽一样，而计算量差不多
- 但 int8 量化对效果有折损，而 fp8 训与推理一致，效果无折损
  - fp8 也当做量化：int8 量化乃线性间隔的，而 fp8 是非线性间隔，从这一点 fp8 也天然比 int8 好

所以如果硬件支持，那么在推理上也就用不着搞 int8 量化了。所以可能未来 fp8/fp4 硬件在各种设备(包括 edge device) 上普及后，可能 int8 量化推理就没生存空间了。

**关键点：outliers 与分组 scaling**

fp8/fp4 表示范围有限，从这个角度看有更细粒度的数值 scaling（即一组数值，用一个scaling factor 作放缩）是可以理解的。另外，人们也发现 LLM 中激活值有有实际作用的 outlier channel，这也要求必须细粒度 scaling。实际上，低比特的训练与推理，很大程度上在围绕激活 outliers（weight 还好，一般 N(0，I)）。

fp8 需要软件层实现scaling，而 MXFPx 则是硬件层实现了 32 个数一组的 scaling 机制。

**怎么训练**

<img width="852" height="650" alt="image" src="https://github.com/user-attachments/assets/fc7df4bd-e13f-4c2f-ac29-916942bc24f8" />

上面是 deepseek-v3 的做法（可见大思路和 fp16 混合精度训练一样）：

<img width="1130" height="874" alt="image" src="https://github.com/user-attachments/assets/012790d8-38df-4957-bb04-7586267dadf7" />

**怎么预测**

训练过程就决定了按 forward 方式对参数做 casting，然后预测就行。天然支持低比特推理。

**deepseek-v3 的 fp8**

它是基于硬件支持fp8的gpu上做的。它发现当前硬件实现的一些问题，于是做了软件上的修正。

### QAT：int 量化感知训练

若硬件不支持 fp8/fp4, 可以做 QAT。基本思路是在 weight 与激活 output 位置引入 fake quantization 节点：

$$
X' = fakeQuant(X) = 
\begin{cases}
dequant(quant(X)) &//forward时 \\
Identity(X) & //反向传播时
\end{cases}
$$

而 inference 的时候，按 quant(X) 作矩阵乘法等，做完才 rescaling 调过来。

如果只做 weight QAT，原理类似。

---

## 量化

量化指量化到 int8。最简单的方法是逐参数的最近邻量化，但是这样往往不够。比较实用的需要作误差量化控制、误差分析。

- 误差量化控制角度：一般是要求量化前后的 ||XW - X'W'|| 变化不能太大。gptq, awq 即此类。
- 误差分析：Y=XW 中，X 一般有 outliers 维度，于是设法规避这点。smoothQuant, awq, Gpt3.int8 都是从这角度出发。

针对参数、激活，有两种量化组合
- 只参数量化
  - 推理时要完全还原原参数，然后作下一步（如矩阵乘法）
  - qlora, gptq, awq
- 参数与激活都量化
  - 低 bit 上做矩阵乘法等运算，算完要做 rescaling，从而得到正确结果
  - smoothQuant

