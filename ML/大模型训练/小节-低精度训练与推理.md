一般意义上，深度学习用 fp32：梯度、参数、激活。

后来有了低精度的训练，意义在于省显存、带宽，并加速计算。

### FP16 混合精度训练

fp16/bf16 的表示范围比较大，所以只需要有个全局的 loss scaling 就可以了。

<img width="1314" height="566" alt="image" src="https://github.com/user-attachments/assets/e4150de3-2ab6-4f25-af88-973ed3cc3ae2" />

### fp8/fp4 混合精度训练

fp8/fp4 表示范围有限，从这个角度看有更细粒度的数值 scaling 是可以理解的。
