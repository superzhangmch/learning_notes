多机多卡训练

### 数据并行

只做数据并行，则多机多卡与单机多卡无本质区别，都是在本卡上作forward再backward拿到本卡分配的数据的梯度后，在多卡上对梯度求和，再用此梯度“和”来更新参数。也就是前向后向是并行的，但梯度求求和与参数更新却没法并行。

梯度求求和与参数更新不能并行，为此有两种方法：
- 参数 server 方法：求和与更新集中在某一server 上，但显然扩展性差。
  - 有一个（或几个）专门的服务器节点负责收集所有工作卡的梯度，求和更新，再把新参数广播回去。
  - 优点：实现简单，逻辑清晰。
  - 缺点：通信瓶颈严重 → 所有梯度数据都要集中到 server，server 带宽/计算容易成为瓶颈，扩展到很多卡时性能急剧下降。
- all-reduce：即每个卡都做求和更新，这样通信开销大。
  - 没有集中式服务器，而是所有卡通过分布式通信协议（如 NCCL、Ring All-Reduce）在彼此之间传输数据，最终得到相同的梯度和（reduce的方式求和，而非拿到各分片独立求和），然后各自覆盖更新自己的参数副本。
  - 优点：去中心化，通信负载分摊，扩展性好（尤其是 GPU-NVLink 或高速网络下）。
  - 缺点：通信量依然大（传输的是整个梯度张量），而且需要优化拓扑和调度来减小延迟。

### 大模型分布式训练，网络带宽要求很高。

现在是大模型时代，1B 参数的都不算大，但 1B 参数的model，单卡虽承载的了，但本卡把自己的梯度传出去就需 4GB 带宽(单参数用 float32)，接受最终结果也得 4GB（用比如 ring-all-reduce 方式，则发出和接受的通信量一样。见下文），在一个 iteration 内就需这么大的网络流量。何况同时有好多个GPU （假设只在做数据并行）都在这样做。

所以才模型分布式训练，首先对网络通信要求就好高（注意在传统分布式系统中（非分布式训练），网络流量要小得多）

上面说的是用 ring-all-reduce 这样的神奇算法做极致了 1B model 一次迭代，单卡只需 4+4 = 8GB 带宽。而接朴素想法，网络流量就更高了： 假如有 N 卡并行，则本卡需将 4G 大小的梯度都传给 N 卡的，总通信是N(N-1)/2；或者换成广播方式，这耗时也会太长。

### ring-all-reduce 等方式降分布式网络通信量

为降网络通讯量，人们想出了 Ring-allreduce 方法：即对其 N 卡把参数切分为 N 份，每卡只负责一份；这份参数以环状流经各卡并逐次累加，传到负责它的卡时，该卡拿到了最终梯度和，于是该卡可以更新参数，并把更新后参数再沿环传一圈。于是，每卡的进出数据量都等于约各总梯度数据，实现了最小化。这个方法问题是：数据要沿环转一圈，卡数多时，latency太大。

于是人们想出了其它方法来改进，主要思路是分层做 ring-allreduce，比如单机多卡间通讯快，那就先单机内reduce出梯度，然后再在机器间做 ring-allreduce。简单注意：GPU间无论单机内还是机器间，往往有特殊高速连接设备（比如 NV-Link、NV-Switch），及各种复杂的拓扑结构，而非通过 TCP-IP 这样的互联，否则也实现不了 n多卡时分布式训练的。

英伟达的NCCL即实现多机多卡间通讯的一个库，它实现了一个叫 double-二叉树 的算法，比 ring-allreduce 还高效；在保持网络流量持平基础上，延时由线性降到了(log(N卡))，因其用了二叉树的逐层往上reduce的方式，故时延是log的，而用两棵树是为了最小化网络流量：令每卡要么在这树中做leaf，要么另一树中做leaf，从而两树中每个卡的父卡与子卡个数都正好是2，这样每树负责一半的数据即可。

这样，在特殊网络硬件与好的算法支持下，做一个几百卡的分布式训练就不是梦了。
