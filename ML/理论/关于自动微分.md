# 关于自动微分

现在的深度学习框架一般都用的自动微分方法来求导。对于有循环递归引用的RNN之类，在自动微分下再也不需要特殊处理（不再用复杂的BPTT(沿时间线的反向传播算法)，或者说自然方式处理BPTT）。因此自动微分确实是很方便的。

首先亮明，自动微分仍然只是为了求导求微分。所以从纯数学上说，真没任何special的。

<br>

对需要算其导数的复杂函数，总是可以把它分解成一个个、一层层的简单函数的复合。我们可以把这样的函数的复合表示成树结构：最顶层是最终函数取值，最底层是依赖的各个简单函数，中间各层是函数组合与复合。

这样有两种方式来进行自动微分。
- 一种方式是自下往上，边计算各函数取值，边计算各函数的导数，从下往上推，一次得到最终函数取值与导数。树中每个节点处的函数的取值和导数都可以由其最近子节点的取值与导数得出。
- 另一种方式是自上往下。先算出最终函数的取值，然后往下倒推。这也就是一般BP反向传播的思路了。

当然，以上方式在每步都是基于对求导链式法则的运用。

<br>

对于没有类似RNN这样的递归迭代定义的复杂函数，上述方法是可以很自然理解的。

有类似RNN这样的递归迭代定义的, 以 $h(w, x) = f(w, f(w, f(w, f(w, x))))$ 为例，需要求 $\frac{dh}{dw}$ 。
- 在从下往上方式下，关于w的导数值会以一种累加方式层层上传。  
  注意： $\frac {dF(u(x), v(x))}{dx}=\frac {dF}{du} \frac {du}{dx}+ \frac {dF}{dv} \frac {dv}{dx}$ ，从而可累加上传。
- 在从上往下方式下，则是以全局变量方式来存w导数，每往下经过一层，累加当前层的w导数。  
  同样因为 $\frac {dF(u(x), v(x))}{dx}=\frac {dF}{du} \frac {du}{dx}+\frac {dF}{dv} \frac {dv}{dx}$ 。
注意：前者是累加后还要作一些运算得到新值，后者属于纯粹的累加。

当前神经网络框架一般用的从上往下式。好处是可以积木式搭建神经网络。每个基本功能块定义好自己的正向求值、逆向求导运算就可以了。算逆向导的时候，根据传下来的 $\frac {d(Loss)}{d(output)}$ 值，当前 output 值，按定义的计算算出关于 w 的导数后，累加存放 w 导数的某变量即可。

参考:
- http://blog.csdn.net/zhouguangfei0717/article/details/78641934
- http://blog.csdn.net/okcd00/article/details/78294212
- https://github.com/dlsys-course/assignment1/blob/master/autodiff.py
- http://dlsys.cs.washington.edu/pdf/lecture3.pdf
http://dlsys.cs.washington.edu/pdf/lecture4.pdf
