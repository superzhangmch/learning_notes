### 思路总结：

对于被推荐的 item（video item），token 化表示它。从而对于用户的历史消费 items，就可以变成 token list。对它们作 encode 的结果给到一个 T5 一样的 encoder-decoder 结构的 model，由该 model 作自回归生成新的 token list（每个token代表一个 item），完成推荐。
- 所用的大模型，不是预先训好的（通用）模型，而是从零开始训的——毕竟它不使用 text tokens，只使用 item tokens。
- 用户对单个 item 的反馈（时长、点踩）在特征输入层面没有反馈，这个是通过 RLHF-DPO 时的 reward model 使用到的。
- 所用 model 是 1B 参数的。

model inference 流程：

<img width="1688" height="664" alt="image" src="https://github.com/user-attachments/assets/9266f9b2-3cf5-479b-a927-5274c802f797" />

### 推荐 item 怎么 token 化

用的 RQ-VAE 算法离散化成的 token。

推荐 item 的 token 化过程：先提取 item embedding，然后依次通过多个 codebook，逐次对前几级处理的残差作最近邻匹配得到 code，并把各级code拼起来得到最终 token。

<img width="702" height="566" alt="image" src="https://github.com/user-attachments/assets/ea56028b-63d3-48a1-aadf-fd88d2e42b99" />

而那些残差 codebooks 怎么来：从 item 库，逐级残差方式统计出来的（用 Balanced K-means Clustering 方法， paper 中 算法一）。

### RLHF-DPO 与反馈数据的使用

DPO 算法的数据是 pair 偏好数据（两个结果，哪个好哪个差）。若用它，就需要构建这样的 pair 数据。它采用的方法是训练一个 reward model：对于 model 针对同一个 prompt 随机生成的一批数据，用 reward model 打分，从而选最高最低分，就可以构建出训练所需的 pair 数据。

关注的几类用户反馈：
- session watch time (swt)
- view probability (vtr)
- follow probability (wtr)
- like probability (ltr)

每一类的 reward：

$$
\begin{cases}
R(u, S) = reward(u=\text{historySession}, S=\text{geneteratedItems}) = \text{Sigmoid}(\text{MLP}(\text{sum}({SelfAttention}(h)))) \\
S = \{v_1, v_2, .., v_m\} \\
h = \{e_1, e_2, .., e_m\} \\
e_m = v_i⊙u
\end{cases}
$$

这里 ⊙ 不是点乘，而是某种操作，比如按原文 represents the target-aware operation (such as target attention toward user behavior)。

或如图：

<img width="1134" height="622" alt="image" src="https://github.com/user-attachments/assets/b098eff5-4528-4751-98da-6f807bb02df4" />
