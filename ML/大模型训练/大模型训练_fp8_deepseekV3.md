
### èƒŒæ™¯

å…³äº deepseek-v3 çš„ fp8 è®­ç»ƒçš„èƒŒæ™¯ä»‹ç»ï¼ŒåŸæ–‡æ‘˜å½•å¦‚ä¸‹ï¼ˆ3.3èŠ‚ ã€ŠFP8 Trainingã€‹ï¼‰ï¼š

> Inspired by recent advances in low-precision trainingï¼ˆè§ä¸‹é¢å¼•æ–‡A,B,Cï¼‰, we propose a fine-grained mixed precision framework utilizing the FP8
data format for training DeepSeek-V3.
>
> ã€å—åˆ°åˆ«çš„ fp8 æ–¹æ¡ˆå¯å‘ã€‘
- A=ã€ŠGpt3. int8 (): 8-bit matrix multiplication for transformers at scaleã€‹- 2022.08 - https://arxiv.org/pdf/2208.07339
  - int8 é‡åŒ–æ¥æé€Ÿæ¨ç†ï¼šInt8 çŸ©é˜µä¹˜æ³•æ–¹æ¡ˆï¼Œç”¨åœ¨ Transformer FFN å’Œ Attn projã€‚
  - å‘ç°ç‰¹å¾ç»´åº¦ä¸­æœ‰ä¸€äº› outliers å½±å“é‡åŒ–æ•ˆæœï¼šäºæ˜¯åˆ†è€Œæ²»ä¹‹ã€‚
  - <img width="1000" alt="image" src="https://github.com/user-attachments/assets/752d7062-b186-474e-b1b9-5962ecde0542" />
- B=ã€Š8-bit numerical formats for deep neural networksã€‹- 2022.06 - https://arxiv.org/pdf/2206.02915
  - å…³æ³¨è®­ç»ƒã€‚
  - ç”¨æµ®ç‚¹ fp8 æ¯” fixed-pointï¼ˆint8ï¼‰å¥½ã€‚
    - å®šç‚¹ int8 å¯è¡¨ç¤ºçš„æ•°åˆ—ï¼Œç›¸é‚»é—´éš”å›ºå®šã€‚è€Œ fp8ï¼Œåˆ™æ˜¯é—´éš”ä¸ä¸€ï¼ˆ0 é™„è¿‘ç²¾ç»†ï¼Œè€Œç»å¯¹å€¼è¶Šå¤§ï¼Œçº¦ç²—ï¼‰ã€‚è€Œç¥ç»ç½‘ç»œçš„å‚æ•°æ¿€æ´»æ¢¯åº¦ç­‰éƒ½æ˜¯é›¶å‡å€¼çš„ã€‚æ‰€ä»¥ç”¨ fp8 æ›´å¥½ã€‚
    - <img width="1096" height="846" alt="image" src="https://github.com/user-attachments/assets/afd8684e-62aa-423f-87fd-1d14469d71b7" />
    - fp8 è¡¨ç¤ºçš„éçº¿æ€§æ€§å¦‚å›¾ï¼Œå¯å‚ https://asawicki.info/articles/fp8_tables.phpã€‚æ³¨æ„ E4M3 è¿˜æœ‰ä¸€ç§ç®—æ³•èŒƒå›´æ˜¯ -448~448.
  - æ¨èï¼šæ¿€æ´»/æƒé‡ç”¨ fp8=E4M3ï¼Œæ¢¯åº¦ç”¨ fp8=E5M2
  - å®ƒç”¨äº†å…¨å±€ loss scale è€Œéç»†ç²’åº¦é€å±‚æˆ–é€å¼ é‡ scale
- C=ã€ŠFP8-LM: Training FP8 large language modelsã€‹ - 2023.10 - https://arxiv.org/pdf/2310.18313
  - Nvidia Transformer Engineåªå¯¹çŸ©é˜µä¹˜æ³•ç”¨ fp8ï¼Œæœ¬æ–‡æŠŠ FP8 åº”ç”¨åˆ°è®¡ç®—ã€å­˜å‚¨å’Œé€šä¿¡å…¨è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œåˆ†å¸ƒå¼è®­ç»ƒã€‚
  - per-tensor scaling
  - ç²¾åº¦åˆ†é…
    - ä¸»æƒé‡ fp16, ä¼˜åŒ–å™¨adamçŠ¶æ€ï¼šfp8ï¼ˆä¸€é˜¶ï¼‰+fp16ï¼ˆäºŒé˜¶ï¼‰ï¼›æ¢¯åº¦ fp8ã€‚è¿™äº›æœ¬æ¥ä¸€ä¸ªå‚æ•°éœ€è¦16å­—èŠ‚ï¼Œå˜æˆäº† 6å­—èŠ‚
    - forwardã€backward æ—¶ï¼Œå…³é”®åœ°æ–¹å¤–ï¼ˆGELUã€Softmaxã€LayerNorm, dropoutç­‰ï¼‰ï¼Œéƒ½æ˜¯ fp8

> While low-precision training holds great promise, it is often limited by the presence of outliers in activations, weights, and gradientsï¼ˆè§ä¸‹é¢å¼•æ–‡D,Eï¼‰.
>
> ã€ä½†æ˜¯å½“å‰çš„ fp8 æ€»æ˜¯å—å›°äº outlier é—®é¢˜ã€‘
- D=ã€ŠScaling FP8 training to trillion-token llmsã€‹- 2024.09 - https://arxiv.org/pdf/2409.12517
  - ç”¨ 2T token è®­äº†ä¸ª 7B modelï¼Œå‘ç° fp8 çš„è®­ç»ƒä¸ç¨³æ¥è‡ª SwiGLU å¯¼è‡´çš„å¼‚å¸¸å€¼æ”¾å¤§ï¼Œå¹¶ç”¨ Smooth-SwiGLU æ”¹è¿›ä¹‹ã€‚
- E=ã€ŠMassive activations in large language modelsã€‹ - 2024.02 - https://arxiv.org/pdf/2402.17762
  - æå°‘æ•°è¶…å¤§ outlier æ¿€æ´»å€¼æ™®éå­˜åœ¨äºå„ LLMï¼ˆä¹ƒè‡³å¤§å‡º 10 ä¸‡å€ï¼‰ï¼Œæ–‡ä¸­å«è¿™ outliers ä¸º massive activationsï¼ˆä¸”è§äº paper æ ‡é¢˜ï¼‰ã€‚
    - æ­¤æ–‡å¹¶ä¸æ˜¯è®² FP8 è®­ç»ƒæ‰å¦‚æ­¤ã€‚è€Œæ˜¯å„ç§ç²¾åº¦çš„éƒ½æœ‰å¯èƒ½
  - æŸäº›ç»´ã€æŸäº› token æ‰å®¹æ˜“å‘ç”Ÿ
    - ä¸æ˜¯æ‰€æœ‰ channel éƒ½ massiveï¼šoutliers æ€»æ˜¯å‡ºç°åœ¨æŸäº› channel ç»´åº¦ï¼ˆä¸”å‡ºç°å‡ ç‡å¾ˆå°ï¼‰ã€‚
    - ä¸æ˜¯æ‰€æœ‰ token éƒ½ massiveï¼šåœ¨ä¸€äº›ç‰¹æ®Š token ä¸Šï¼ˆèµ·å§‹ <BOS>ã€å¥å· â€œ.â€ã€æ¢è¡Œç¬¦ \nã€åˆ†éš”ç¬¦ç­‰ï¼‰æ‰å¦‚æ­¤ã€‚
    - <img width="1162" height="658" alt="image" src="https://github.com/user-attachments/assets/293d84b3-e8b3-471c-a250-1d7633336fb2" />
  - ä»–ä»¬èµ·çš„ä½œç”¨æ˜¯ biasesï¼Œè‹¥å»æ‰ä¹‹ä¼šæ€§èƒ½ä¸‹é™ï¼ˆMassive activations act as fixed but important biases in LLMsï¼‰ã€‚attn ä¸­ç›¸å½“äºéšå¼ bias

> Although significant progress has been made in inference quantization (è§ä¸‹é¢å¼•æ–‡F,G), there are relatively few studies demonstrating successful application of low-precision techniques in large-scale language model pre-training (è§ä¸‹é¢å¼•æ–‡ D).
>
> ã€æ¨ç†é‡åŒ–æœ‰è¿›å±•ï¼Œä½†æ˜¯å¤§è§„æ¨¡ä½ç²¾åº¦è®­ç»ƒçš„æˆåŠŸè¿˜æœªè§ã€‘
- æ¨ç†æ—¶é‡åŒ–ï¼š
  - Fï¼šã€ŠGptq: Accurate post-training quantization for generative pre-trained transformersã€‹- 2022.10 - https://arxiv.org/pdf/2210.17323
  - Gï¼šã€ŠSmoothquant: Accurate and efficient post-training quantization for large language modelsã€‹ - 2022.11 - https://arxiv.org/pdf/2211.10438
- Dï¼šã€ŠScaling FP8 training to trillion-token llmsã€‹- 2024.09 - ï¼ˆæ³¨æ„ä¸Šé¢ä¹Ÿå‡ºç°äº†ï¼‰ https://arxiv.org/pdf/2409.12517
  - ç”¨ 2T token è®­äº†ä¸ª 7B modelï¼Œå‘ç° fp8 çš„è®­ç»ƒä¸ç¨³æ¥è‡ª SwiGLU å¯¼è‡´çš„å¼‚å¸¸å€¼æ”¾å¤§ï¼Œå¹¶ç”¨ Smooth-SwiGLU æ”¹è¿›ä¹‹ã€‚

> To address this challenge and effectively extend the dynamic range of the FP8 format, ã€äºæ˜¯æ¨å‡º deepseek-v3 çš„è§£æ³•ã€‘
>
> ï¼ˆ1ï¼‰ã€we introduce a fine-grained quantization strategy: tile-wise grouping with 1 Ã— ğ‘ğ‘ elements or block-wise grouping with ğ‘ğ‘ Ã— ğ‘ğ‘ elements. The associated dequantization overhead is largely mitigated under our increased-precision accumulation process, a critical aspect for achieving accurate FP8 General Matrix Multiplication (GEMM).
>
> ï¼ˆ2ï¼‰ã€Moreover, to further reduce memory and communication overhead in MoE training, we cache and dispatch activations in FP8, while storing low-precision optimizer states in BF16.
>
> ï¼ˆ3ï¼‰ã€We validate the proposed FP8 mixed precision framework on two model scales similar to DeepSeek-V2-Lite and DeepSeekV2, training for approximately 1 trillion tokens (see more details in Appendix B.1). Notably, compared with the BF16 baseline, the relative loss error of our FP8-training model remains consistently below 0.25%, a level well within the acceptable range of training randomness.

ä»ä»¥ä¸Šçš„å¯ç¤ºæ˜¯ï¼Œç”¨ fp8 æœ‰å¥½å¤„ï¼Œä½†æ˜¯æ€»æ˜¯æœ‰å¼‚å¸¸å¤§å€¼ï¼ˆoutliersï¼‰é—®é¢˜ã€‚deepseek-v3 è¯•å›¾è§£å†³è¿™ç‚¹ã€‚

### deepseek-v3 çš„è§£æ³•

#### ï¼ˆ1ï¼‰æ•´ä½“åšæ³•

<img width="1658" height="668" alt="image" src="https://github.com/user-attachments/assets/054cf088-55d2-4bab-9e0f-a4a17eb2e660" />

å¦‚ä¸Šå›¾åªè€ƒè™‘ **Linear å±‚**ï¼ˆå…¨è¿æ¥å±‚ï¼‰ã€‚ä»¤ $y=XW$, $Loss = L(y, ..)$, å…¶ä¸­ X = g(..) æ˜¯ y çš„è¾“å…¥ï¼ŒW æ˜¯æƒé‡ã€‚

æ³¨æ„å¯¹ Loss æ±‚æ¢¯åº¦æ—¶ï¼Œä¸ä½†è¦å¯¹ W æ±‚ï¼Œè¿˜è¦å¯¹ X=g(..) å†…çš„å‚æ•°æ±‚ã€‚è€Œä¸ºæ±‚åè€…ï¼Œå¿…é¡»å…ˆæ±‚ $\partial L / \partial X$ã€‚äºæ˜¯ä¸€æ¬¡ backward æ—¢è¦å¯¹ W ä¹Ÿè¦å¯¹ X æ±‚æ¢¯åº¦ã€‚

ä¸‹é¢çœ‹ $XW$ åœ¨è®­ç»ƒæ—¶æ¶‰åŠçš„ä¸‰ç±» GEMMï¼ˆçŸ©é˜µä¹˜åŠ è¿ç®—ï¼‰ï¼š

(1) **Fprop (Forward propagation)**

æ¶‰åŠè®¡ç®—æ˜¯ $XW$, æŠŠè¾“å…¥æ¿€æ´» $X$ ä¸æƒé‡ $W$ ç›¸ä¹˜ã€‚

å¯¹åº”ä¸Šå›¾ä¸­ï¼Œè¿™ä¸ª GEMM åœ¨ FP8 è®¡ç®—ï¼Œä½†ç»“æœä¼šåœ¨ FP32 ä¸­ç´¯åŠ  (å›¾ä¸­ Î£)ï¼Œæœ€åå­˜æˆ BF16/FP32ã€‚

(2) **Wgrad (Weight gradient)**

åå‘ä¼ æ’­é‡Œè®¡ç®—æƒé‡çš„æ¢¯åº¦, æ¶‰åŠè®¡ç®—æ˜¯

$$
\begin{cases}
\nabla y &= \frac{\partial \mathcal{L}}{\partial y} \\
\nabla W &= X^{\top} \nabla y
\end{cases}
$$

å³ç”¨è¾“å…¥ $X$ å’ŒæŸå¤±å¯¹è¾“å‡ºçš„æ¢¯åº¦ $\nabla y$ åå‘è®¡ç®—æƒé‡æ›´æ–°æ–¹å‘ã€‚

å¯¹åº”ä¸Šå›¾ä¸­ï¼Œè¿™ä¸ª GEMM åœ¨FP8 æ‰§è¡Œï¼Œç„¶åè¿›å…¥ FP32 ç´¯åŠ (Î£), å¹¶æœ€ç»ˆç”¨äºæ›´æ–° FP32 ä¸»å‚æ•°ã€‚

(3) **Dgrad (Data gradient / Activation gradient)**

è¿™ä¸€æ­¥è¿›è¡Œåå‘ä¼ æ’­é‡Œè®¡ç®—è¾“å…¥çš„æ¢¯åº¦, å³

$$
\nabla X = \nabla y W^{\top}
$$

æŠŠæŸå¤±æ¢¯åº¦ $\nabla y$ åå‘ä¼ æ’­åˆ°è¾“å…¥ï¼Œä¾›å‰ä¸€å±‚ä½¿ç”¨ã€‚

å¯¹åº”ä¸Šå›¾ä¸­, è¿™ä¸ª GEMM åŒæ ·åœ¨ FP8 æ‰§è¡Œï¼Œå¹¶åœ¨ FP32 ç´¯åŠ (å›¾ä¸­ Î£), å¹¶ cast ç»´ FP16 ä¾›ä¸Šä¸€å±‚æ±‚æ¢¯åº¦ç”¨ã€‚

æ¢ä¸ªå½¢å¼çœ‹ï¼š

<img width="852" height="650" alt="image" src="https://github.com/user-attachments/assets/fc7df4bd-e13f-4c2f-ac29-916942bc24f8" />
