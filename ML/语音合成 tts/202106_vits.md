# vits 《Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech》 https://arxiv.org/pdf/2106.06103

下面是整体流程：

![image](https://github.com/user-attachments/assets/33c57b25-6448-4f9e-b9b6-a714f950eb51)

下面是一些细节说明：

![image](https://github.com/user-attachments/assets/66f5cef3-2806-4c4b-a1d6-86af68b0fe0d)

### loss

![image](https://github.com/user-attachments/assets/75b31d71-f681-4773-a2d4-44fef660f9e4)

如上是和标准 VAE 的对比。重建 loss 换成了 $L_1$, 而 KL 散度则是多了条件 c = [text, Align]。loss 拆解项为：

![image](https://github.com/user-attachments/assets/4ad17118-9daa-40d5-8c5a-26af2722753f)

对应到 model 图为：

![image](https://github.com/user-attachments/assets/53cc5f9d-379b-4891-9254-b3dca5494888)

note:
- 重建用的 mel 谱，但是有些地方用的线性谱，而非非线性的 mel-谱。
- text-audio 对齐：text 切分成 tokens 长度，和生成的 audios 的某种 frames 的长度不一样，且不能线性对应，所以需要类似 CTC 之类的东西来做对齐。vits 训练时用的是"单调对齐搜索"的方式，它本身只是一个对齐算法，本身没参数，本身不需要训练。但是对齐结果可以用来当场统计出每个 text token 的时长，从而用于训练 "时长预测器"。而 inference的时候，根据时长预测结果，把 text tokens 的 hidden 表示(mu, sigma) 做 repeat 即可。
- 用到了 GAN loss。
- f_theta(z) 以及 stochastic duration predictor 都是可逆的，所以图中箭头方向在 train / infer 中是反着的。

### 实现

![image](https://github.com/user-attachments/assets/985b98db-b1b1-4e88-a055-360d15de2969)

**MSA 对齐：**

![image](https://github.com/user-attachments/assets/f975f837-3aad-41a4-862b-fe3bd3cbc218)


**随机时长预测：** 细节见 paper，不详述：

![image](https://github.com/user-attachments/assets/ded899d7-61c6-4340-af14-a6af04f9a3be)


----

### 关于文中的 flow-based model

文中两处用到。它用的是 **Affine Coupling Layer**（RealNVP《Density estimation using Real NVP》, Glow）法。具体做法是（部分内容来自 AI）：

把输入 $x$ 拆成两部分： $$x = [x_a, x_b]$$ ， 然后只对一部分做变换：

$$
\begin{cases}
y_a &= x_a \\
y_b &= x_b \cdot \exp(s(x_a)) + t(x_a)
\end{cases}
$$

其中 $s(x_a)$, $t(x_a)$ 是由神经网络输出的 scale 和 shift， $y = [y_a, y_b]$

特点是：
- 易于求逆： $$x_b = (y_b - t(x_a)) \cdot \exp(-s(x_a))$$
- Jacobian 是三角形式，行列式只需要计算 $s(x_a)$ 的和： $$\log |\det J| = \sum s(x_a)$$

鉴于只有一半维度做了变换，所以多个 layer 之间要交错变换其中的一半维度。

**训练目标：**

极大似然估计方式最大化原始数据 x 的 log 思然（而 loss = -该式)： $\log p_X(x) = \log p_Z(f(x)) + \log \left| \det \left( \frac{\partial f(x)}{\partial x} \right) \right|$，该公式是x 与 f(x) 之间的概率密度的变换公式，其形式有数学上的严格保证，其中 $p_X(x)$ 是原始数据 x 的概率密度， $p_Z(.)$ 是正态分布密度函数，f(.) 是多层的 affine coupling layer 的组合。训练结束后，期望 f(用户书序）~ N(0, I)。因为 f 可逆，所以采样一个正态分布数据 z ~ N(0, I)，则 $f^{-1}(z)$ 就是生成结果了。

为什么会最终 f(.) ~ N(0, I)? 因为 $p_Z(.)$ 是正态分布，如果换成其他，则最终 f(x) 拟合到的就是其他分布。这样通过极大似然，把 f(x) 拉向了 N(0, I) 的高密度区，从而实现了概率分布的转换映射。

note：对于 vits $f_\theta(z)$ 来说，它的网络结构与流程和标准 flow-based model 一样，但是它并不是 x -> N(0, I) 的分布转换，没通过上述极大似然方式来训，而是串进了整个流程。

**和其他 model 区别:**

扩散模型，以及 flow-matching 在训练的时候，会明确地采样一个噪声（正态 noise），预测这个噪声。而上面的 flow-based model，虽然生成的时候会采样 noise，但是训练的时候，并没 noise 参与。
