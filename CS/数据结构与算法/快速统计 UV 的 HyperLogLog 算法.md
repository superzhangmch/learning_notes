HyperLogLog 用于大量日志中获得估计的去重 UV. 它只需要很小的内存占用, 且只需要(且必须)对全体日志扫描一次, 扫完, 就可以得到 99% 以上准确率的 UV 估计.

它的基本思路是, 对 user_id 做 hash 成 binary 表示; 一旦这么多 user-id 中, 出现了某个 user-id 有更多的前导零, 概率上说明往往 UV 应该很大——从而可根据概率反推应该多少 UV 才能遇到这样的情形. 为了增加估计的准确性, 所以用了分多桶估计, 每个桶的估计值取平均作为最终估计值——所以估计得才这么准.

增加分桶只是为了更好的估计, 无关支持更大的 UV 数估计. 每个分桶的更多的前导0的支持才与更多 UV 有关. 该算法也与每个 user-id 重复多少次无关(因为本身就是一种 dict / set 那样的方式进行的).

实测, 确实是 99% 以上准确率. 

----

下面 by chatgpt

HyperLogLog（HLL）是一种**用于估计基数（distinct count，去重计数）**的概率数据结构。它可以用**极小的内存**来估计一个集合中“不同元素的数量”，误差可控，非常适合大规模数据流统计，比如 UV 统计、去重用户数、唯一 IP 数等。

# 一、要解决的问题

目标：
在数据量极大时，统计去重个数：

```
例如：10亿条日志中有多少个不同用户ID？
```

普通方法：

* 用 HashSet 存 → 内存爆炸
* 排序再去重 → 成本很高

HLL：
✅ 内存只需 KB 级
✅ 单次扫描
✅ 可合并
❌ 结果是估计值（不是精确值）

# 二、核心直觉：用“最长前导零”推测数量

关键观察：

如果把元素 hash 成随机二进制串：

```
000000101011...
```

出现“前导零很多”的概率非常低。

概率规律：

| 前导零个数 | 概率     |
| ----- | ------ |
| 1 个   | 1/2    |
| 2 个   | 1/4    |
| 3 个   | 1/8    |
| 10 个  | 1/1024 |

👉 如果你在数据里看到一个 hash 有 10 个前导零
说明你大概已经看了 ~1000 个不同元素

这就是 HLL 的数学基础。

# 三、单寄存器版本（LogLog思想）

最简单的版本：

```
max_zero = 0

for 每个元素:
    h = hash(x)
    zeros = 前导零个数(h)
    max_zero = max(max_zero, zeros)

估计基数 ≈ 2 ^ max_zero
```

问题：

* 方差太大
* 不稳定

所以要改进 → 多寄存器平均

# 四、HyperLogLog 核心结构

## 1️⃣ 使用多个桶（register）

把 hash 值分成两部分：

```
hash(x) = [ 前 p 位 | 剩余位 ]
           ↑桶编号     ↑计算前导零
```

* 前 p 位 → 选桶（bucket）
* 剩余位 → 数前导零

如果 p = 14：

```
桶数量 m = 2^14 = 16384
```

## 2️⃣ 每个桶记录最大前导零

```
register[i] = max 前导零数
```

更新规则：

```
i = 前 p 位
w = 剩余位
register[i] = max(register[i], leading_zero(w)+1)
```

## 3️⃣ 用调和平均来估计

HLL 不用普通平均，而是用：

# 👉 调和平均 + 偏置修正

公式：

$$
E = \alpha_m \cdot m^2 / \sum 2^{-register[i]}
$$

其中：

* m = 桶数量
* α_m = 修正系数
* register[i] = 第 i 个桶的值

# 五、为什么用调和平均？

因为：

```
2^{-register[i]} ≈ 1 / 估计规模
```

调和平均对极端值更稳健：

* 少数大值不会把估计拉爆
* 更抗噪声

# 六、误差范围

HLL 的误差只和桶数有关：

[
误差 ≈ 1.04 / √m
]

例子：

| m     | 误差    |
| ----- | ----- |
| 1024  | ~3%   |
| 16384 | ~0.8% |
| 65536 | ~0.4% |

# 七、空间复杂度

每个桶只需要：

```
5–6 bits
```

总空间：

```
m * 6 bits
```

例如：

```
16384 桶 → 12 KB
```

就能估计几十亿去重数。

# 八、HLL 的关键优势

## ✅ 内存极小

KB级

## ✅ 可并行可合并（重要）

两个 HLL 可以直接合并：

```
register[i] = max(A[i], B[i])
```

所以：

* MapReduce
* 分布式统计
* 实时流计算

都很好用

# 九、HLL 的改进点（HyperLogLog++）

现代系统（Redis / Google / BigQuery）用的是 HLL++：

改进包括：

* 小基数时用线性计数
* 大基数时偏置修正
* 稀疏存储
* 更好的 hash 分布处理

# 十、直观类比

可以这样理解：

你在森林里数动物数量：

* 每个桶 = 一个观察点
* 前导零 = 看到的“稀有度”
* 稀有度越高 → 动物越多
* 多个观察点平均 → 稳定估计

----

下面 by claude:

核心思想：用"最大前导零"估算基数                      

### 1. 概率直觉

假设你抛硬币，连续出现 k 个正面的概率是 1/2^k。

如果你观察到最多连续出现了 5 个正面，那大概抛了 2^5 = 32 次左右。

HyperLogLog 把这个思路用在哈希值上：
- 把每个 user_id 哈希成二进制数     
- 统计二进制中"前导零的最大个数" 
- 如果最大前导零是 k，估算大约有 2^k 个不同元素

### 2. 分桶降低方差   

单个估计值方差太大，所以用 m 个桶（本例用 16384 个）：

```
hash(user_id) = 0101 1000 1101 0010 ... 
                ^^^^
                前14位 → 决定放入哪个桶 (桶号 0~16383)
                     ^^^^^^^^^^^^^^^^^
                     剩余位 → 统计前导零个数
```

每个桶独立估算，最后取调和平均，大幅降低误差。

### 3. 内存效率惊人
```
┌─────────────┬──────────┬──────────────────────────────────┐ 
│    方法      │ 内存占用  │               说明                │
├─────────────┼──────────┼──────────────────────────────────┤
│ Set (精确)   │ ~5.5 MB  │ 存储 69,252 个 UUID               │
├─────────────┼──────────┼──────────────────────────────────┤
│ HyperLogLog │ ~12 KB   │ 只需 16,384 个寄存器，每个 6 bit    │
└─────────────┴──────────┴──────────────────────────────────┘
```
用 0.2% 的内存，达到 99.5% 的准确率！

### 4. 简化版代码逻辑
```
def add(user_id):
    h = hash(user_id)           # 64位哈希
    bucket = h >> 50            # 前14位 → 桶号
    trailing = count_leading_zeros(h & 0x3FFFFFFFFFFFF)  # 剩余50位的前导零
    registers[bucket] = max(registers[bucket], trailing + 1)

def count():
    # 调和平均 + 偏差修正
    return alpha * m^2 / sum(2^(-register[i])) # m: 分桶数; 
```

这就是为什么 Redis、BigQuery、Spark 都内置了 HyperLogLog —— 用极小内存就能统计海量 UV。

----

### 关于 Alpha 的来源

HyperLogLog 的原始估计公式是：

E_raw = m² / Σ(2^(-register[j]))

但这个估计有系统性偏差，需要乘以 alpha 修正。

理论推导

Alpha 来自对调和平均的期望值分析。当 m 个桶独立且哈希均匀分布时：

$$α_m = 1 / (m · ∫ _0 ^\infty (\log_2((2+u)/(1+u)))^m du)$$

这个积分没有封闭解，但当 m → ∞ 时收敛到：

α_∞ = 1 / (2 · ln(2)) ≈ 0.7213

实际取值

论文给出了不同 m 值的精确常数：
```
┌──────────┬────────────┬────────────────────────┐
│ m (桶数)  │ p (精度位)  │         alpha          │
├──────────┼────────────┼────────────────────────┤
│ 16       │ 4          │ 0.673                  │
├──────────┼────────────┼────────────────────────┤
│ 32       │ 5          │ 0.697                  │
├──────────┼────────────┼────────────────────────┤
│ 64       │ 6          │ 0.709                  │
├──────────┼────────────┼────────────────────────┤
│ ≥128     │ ≥7         │ 0.7213 / (1 + 1.079/m) │
└──────────┴────────────┴────────────────────────┘
```

代码中的实现：

```
if m >= 128:
    alpha = 0.7213 / (1 + 1.079 / m)  # 渐近公式
elif m == 64:
    alpha = 0.709
elif m == 32:
    alpha = 0.697
else:
    alpha = 0.673
```

直观理解

- 没有 alpha：估计值会系统性偏大
- alpha ≈ 0.72：相当于把原始估计"打个七折"修正回来

这些常数是 Flajolet 等人在 2007 年原始论文《HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm》中通过数学分析得出的。
