# QAT: 《Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference》 https://arxiv.org/pdf/1712.05877 google

现在主流的 QAT 方法，基本都源自 google 这篇 paper 的思路，包括 PyTorch 的 QAT 实现。

<img width="1032" height="530" alt="image" src="https://github.com/user-attachments/assets/bfc12902-a12d-4f0b-8c74-8d71bacfe5c6" />

**推理时数据类型：**

paper 中推理时数据类型：
- 只用整数（如上图左可见确实如此）
 - 权重 和 激活 都是 int8
 - 累加器 用 int32，以避免溢出。
 - bias 用 int32，因偏置误差累积会影响整体精度

torch QAT: 对于被量化的算子也如此。

**关键操作**

网络内插入 FakeQuantize： `A' = dequant(quant(A))`, 而在反向传播时，仿佛没有该操作（因 dequant、quant 不可导），而是直接梯度回传。

**使用时机：**

一般是先 float 预训练 → QAT fine-tune。如果一开始就 QAT 训练，则更难收敛，因为模型一开始就承受量化误差。且收敛速度慢，需要更长训练时间。

**其他**

- 为什么 paper 中 `r = S(q − Z)` 要减 Z：以为它是要量化到 uint8，如果是 int8，则不需要这不操作。
