# deepseek-DSAã€ŠDeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attentionã€‹

MLA æœ‰ä¸¤ç§ç­‰ä»·æ¨¡å¼ï¼š MHA æˆ– MQA

<img width="1790" height="904" alt="image" src="https://github.com/user-attachments/assets/0cc7725f-97f9-4367-85ac-842ab4d9193e" />

deepseek-DSA ç€çœ¼äº MQA æ¨¡å¼ï¼šåœ¨ MQA æ¨¡å¼ä¸Šçš„ç¤ºæ„å›¾å¦‚ä¸‹ï¼š

<img width="1384" height="626" alt="image" src="https://github.com/user-attachments/assets/0023b5ff-ef78-48b1-b992-01bec4c53c23" />

ç®€å•è¯´ï¼šå½“å‰ä½ç½®çš„ q å’Œå†å²æ‰€æœ‰çš„ k è®¡ç®—å‡º k çš„é‡è¦æ€§ scoreï¼Œç„¶åæ ¹æ® score å– topï¼Œè¿‡æ»¤å‡ºè¦å®é™…å‚ä¸ attn çš„ kvã€‚

## kv é‡è¦æ€§ score æ€ä¹ˆç®— ï¼ˆindexer scoreï¼‰

score è®¡ç®—å…¬å¼æ˜¯ï¼š

$$
I_{t,s} = \sum_{j=1}^{H^I} w_{t,j}^I \cdot \text{ReLU} (q_{t,j}^I \cdot k_s^I)
$$

- å…¶ä¸­æ‰€éœ€å„é¡¹ï¼š $w_{t,j}^I$, $k_s^I$ ä» hidden_states ç»çº¿æ€§å˜æ¢å¾—åˆ°, $q_{t,j}^I$ ä» MLA å‹ç¼©åçš„ q_latent ç»çº¿æ€§å˜æ¢å¾—åˆ°ã€‚q æ˜¯å¤š heads çš„ï¼Œæ‰€ä»¥ä¸Šå¼ $\sum$ å°±æ˜¯éå† q çš„å„ä¸ª headã€‚
  - $H^I$ æ˜¯ q head æ•°ã€‚
- ä¸Šå›¾ä¸­çš„ "partially apply RoPE"ï¼Œå…¶å®å°±æ˜¯å’Œ MLA ä¸€æ ·ï¼Œåˆ†æˆ rope ä¸ érope ä¸¤ä¸ªåˆ†æ”¯ã€‚
- ä¸Šé¢score ç®—çš„æ—¶å€™ï¼Œæ˜¯åœ¨ fp8 ä¸Šç®—çš„ï¼Œä¸ºæ­¤è¦åš fp8 é‡åŒ–ã€‚
  - é‡åŒ–ä¸€èˆ¬ä¼šé‡åˆ°æ•°æ®çš„ outliers é—®é¢˜ï¼Œä»ä»£ç ä¸­çœ‹åˆ°å®ƒæ˜¯é€šè¿‡å¯¹ qã€k æ–½åŠ  hadamard å˜æ¢å®Œæˆçš„ã€‚æ›´å¤šå‚è€ƒï¼šhttps://zhuanlan.zhihu.com/p/1908616046874699007 ã€ https://github.com/deepseek-ai/FlashMLA/pull/54 ï¼ˆæ‰“å¼€åæœ hadamard)
  - hadamard å˜æ¢æœ‰å…³çŸ¥è¯†ï¼Œçœ‹ä¸‹é¢ä»£ç ã€‚
- ä»å…¬å¼çœ‹ï¼Œq å’Œ å†å² {k} ä¼šéå†ç®—ç›¸å…³æ€§ï¼ŒæŒ‰è¯´ä¹Ÿä¸æ˜¯è½»é‡çº§çš„ï¼Œä½†æ˜¯å®ƒæ²¡ç®— softmaxï¼Œæ‰€ä»¥è¿˜æ˜¯å®¹æ˜“äº†å¾ˆå¤šã€‚

æˆ‘åŠ äº†æ³¨é‡Šåçš„ forked çš„ä»£ç åœ¨ï¼š https://github.com/superzhangmch/learn_DeepSeek-V3.2-Exp/blob/main/inference/model.py#L431 ï¼Œå¹¶æ‘˜å½•å¦‚ä¸‹ï¼š

```
class Indexer(torch.nn.Module):
    def __init__(self, args: ModelArgs):
        ...
        
        # è¿™æ˜¯æ˜¯ç±»ä¼¼ kv-cache ä¸€æ ·çš„ indexer-k-cacheï¼Œéœ€è¦æŠŠå†å²éƒ½å­˜ä¸‹æ¥ã€‚å­˜çš„æ˜¯ fp8 çš„ï¼Œk ä¸€ä¸ªheadï¼Œhead_dim=128, è¿™æ ·ä¸€ä¸ªtoken å  128å­—èŠ‚çš„cacheï¼›å†åŠ ä¸Š1å­—èŠ‚çš„ scaleï¼Œå…±129å­—èŠ‚ã€‚
        self.register_buffer("k_cache", torch.zeros(args.max_batch_size, args.max_seq_len, self.head_dim, dtype=torch.float8_e4m3fn), persistent=False)
        self.register_buffer("k_scale_cache", torch.zeros(args.max_batch_size, args.max_seq_len, self.head_dim // block_size, dtype=torch.float32), persistent=False) # block_size=128

    def forward(self, x: torch.Tensor, # x: hidden_state
                     qr: torch.Tensor, # qr: MLA çš„ latent_lora_q
                    start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):
        bsz, seqlen, _ = x.size()
        end_pos = start_pos + seqlen

        q = self.wq_b(qr)                                                # qr: latent_lora_q; è¿™ä¸€æ ·æŠŠå®ƒå‡ç»´åˆ° index_n_heads * index_head_dimï¼Œä¹Ÿå°±æ˜¯ Index å†…æŠŠ q è½¬æˆäº†å¤šä¸ª heads.
                                                                         # index_n_heads = 64, index_head_dim=128
        q = rearrange(q, 'b s (h d) -> b s h d', d=self.head_dim)
        q_pe, q_nope = torch.split(q, [self.rope_head_dim, self.head_dim - self.rope_head_dim], dim=-1)  # å„å  64 ç»´
        q_pe = apply_rotary_emb(q_pe, freqs_cis)
        q = torch.cat([q_pe, q_nope], dim=-1)
        
        k = self.wk(x)                                                   # k.shape = [..., index_head_dim]ï¼ŒIndex å†…æŠŠ k è½¬æˆäº†1 ä¸ª head
        k = self.k_norm(k)
        k_pe, k_nope = torch.split(k, [self.rope_head_dim, self.head_dim - self.rope_head_dim], dim=-1)
        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis).squeeze(2)
        k = torch.cat([k_pe, k_nope], dim=-1)

        def rotate_activation(x):
            # ä¸ºä»€ä¹ˆè¦æœ‰ scale=1/sqrt(hidden_size): hadamardå˜æ¢ï¼Œå¹¶ä¸æ˜¯çœŸçš„æ­£äº¤ï¼Œè€Œæ˜¯ HH' = H'H = n*I, H.shape = [n,n], æ‰€ä»¥è¦é™¤ä»¥ 1/sqrt(n)
            return hadamard_transform(x, scale=hidden_size ** -0.5)

        q = rotate_activation(q) # å†…éƒ¨å…¶å®å°±æ˜¯åšäº†ä¸€ä¸ª hadamard å˜æ¢ã€‚ hadamardå˜æ¢æ˜¯æ­£äº¤çš„ï¼Œè€Œæ­£äº¤çŸ©é˜µå…¶å®å°±æ˜¯æ—‹è½¬ï¼Œæ‰€ä»¥å« rotate_activation
        k = rotate_activation(k) 
        # ä¸ºä»€ä¹ˆä¸Šé¢ q ä¸ k è¦åš hadamard å˜æ¢ã€‚
        #   (1) é¦–å…ˆï¼Œå®ƒä¸å½±å“ç»“æœæ­£ç¡®æ€§ï¼š (qH)(kH)' = qHH'k'=qk', HH'=I å› ä¸º H æ­£äº¤ã€‚
        #   (2) æ¥ä¸‹æ¥è¦å¯¹ qã€k ä½œé‡åŒ–ï¼Œè€Œ qã€k ä¸­çš„ outliers å€¼ä¼šå¯¼è‡´é‡åŒ–çš„æ•ˆæœä¸‹é™ï¼Œäºæ˜¯å¸Œæœ›é™ä½ outliersã€‚è€Œ hadamard å˜æ¢æ­£å¥½å¯ä»¥ä½¿å¾—è¾“å…¥ç»´åº¦ä¸Šçš„ç›¸å…³æ€§è¢«æ‰“æ•£ï¼Œä½¿å¾—åˆ†é‡æ›´æ¥è¿‘ç‹¬ç«‹ã€å‡åŒ€ã€‚
        #       æ‰€ä»¥ï¼Œè¿™æ˜¯ä¸ºäº†fp8 é‡åŒ–è€Œåšçš„å‡†å¤‡ã€‚
        
        q_fp8, q_scale = act_quant(q, block_size, self.scale_fmt)      # qï¼Œk è¦ fp8 é‡åŒ–
        k_fp8, k_scale = act_quant(k, block_size, self.scale_fmt)
        self.k_cache[:bsz, start_pos:end_pos] = k_fp8
        self.k_scale_cache[:bsz, start_pos:end_pos] = k_scale
        
        weights = self.weights_proj(x) * self.n_heads ** -0.5          # weight = (xW) / sqrt(n_heads)
        weights = weights.unsqueeze(-1) * q_scale * self.softmax_scale

        # ä¸‹é¢æ˜¯åš fp8 é‡åŒ–çš„é‡è¦æ€§ score è®¡ç®—ï¼š
        #  æŒ‰ paperï¼Œè®¡ç®—å…¬å¼æ˜¯ï¼šindex_score(s,t) = \sum_h weights_h Relu(q_h k), å…¶ä¸­q_h æ˜¯æœªçŸ¥ s çš„ï¼Œk æ˜¯ä½ç½® t çš„ã€‚qæ˜¯å¤šheadsçš„ï¼Œk æ˜¯å•headçš„
        #  ä¸‹é¢çš„ fp8_index å°±æ˜¯å®ç°äº† fp8 ä¸‹çš„è¯¥æ“ä½œã€‚çœ‹ä»£ç å…¶å®ç°çš„å¤§çº¦æ˜¯ï¼š
        #       logits = accum_fp32(q_8 * k_8')
        #       logits <- ReLU(logits) * weight
        #       logits_sum = \sum_{j=1}^{N_heads} logits_{:, :, :, j}
        #       index_score = logits_sum * k_s,  k_s æ˜¯é‡åŒ–æ¢å¤ç”¨çš„
        index_score = fp8_index(q_fp8.contiguous(), 
                                weights, 
                                self.k_cache[:bsz, :end_pos].contiguous(), 
                                self.k_scale_cache[:bsz, :end_pos].contiguous())

        # ä¸‹é¢æ˜¯å– topK
        if mask is not None: index_score += mask
        topk_indices = index_score.topk(min(self.index_topk, end_pos), dim=-1)[1] # index_topk=2048
        topk_indices_ = topk_indices.clone()
        dist.broadcast(topk_indices_, src=0)
        assert torch.all(topk_indices == topk_indices_), f"{topk_indices=} {topk_indices_=}"
        return topk_indices

class MLA(nn.Module):
    def __init__(self, args: ModelArgs):
        ...
        self.indexer = Indexer(args)

        # MLA æ˜¾å­˜å ç”¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ fp32 å››å­—èŠ‚ï¼Œkv_lora_rank+qk_rope_head_dim=512+64, ä¸€ä¸ª tokençš„æ˜¾å­˜å ç”¨æ˜¯ï¼ˆ512+64ï¼‰*4ï¼›è€Œä¸€ä¸ªtokenåœ¨indexer ä¸Šåªå ç”¨ 129ä¸ªå­—èŠ‚ã€‚å¯è§æ–°å¢çš„æ˜¾å­˜å ç”¨ä¸å¤š
        #   note: attn ä¸€èˆ¬éœ€è¦é«˜ç²¾åº¦ã€‚å®é™…ä¸­ï¼Œé«˜æ•ˆæ¨ç†åˆ°åº•æ˜¯ fp16ï¼Œè¿˜æ˜¯ fp32ï¼Ÿ
        self.register_buffer("kv_cache", torch.zeros(args.max_batch_size, args.max_seq_len, self.kv_lora_rank), persistent=False)
        self.register_buffer("pe_cache", torch.zeros(args.max_batch_size, args.max_seq_len, self.qk_rope_head_dim), persistent=False)
        ...

    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):
        ...

        scores = torch.einsum("bshd,bthd->bsht", q.float(), k.float()) * self.softmax_scale # å…¨åºåˆ—çš„ QK'

        # indexerï¼š æ€ä¹ˆåœ¨ MLA ä¸­ä½¿ç”¨
        topk_indices = self.indexer(x, qr, start_pos, freqs_cis, mask)
        index_mask = torch.full((bsz, seqlen, seqlen), float("-inf"), device=x.device).scatter_(-1, topk_indices, 0) # topk_indicesä½ç½®ä¸º0å…¶ä½™æ˜¯ -inf
        index_mask += mask                                                                                           # maskï¼š0 å’Œ -inf ç»„æˆã€‚æ‰€ä»¥è¿™ä¸€å¥ç›¸å½“äºæ˜¯ä¸¤ä¸ª mask æ±‚äº¤é›†
        scores += index_mask.unsqueeze(2)                                                                            # exp(-inf) = 0, æ‰€ä»¥æŠŠ mask åŠ è¿› score

        scores = scores.softmax(dim=-1, dtype=torch.float32)                                                         # æŒ‰è¯´è¿™é‡Œéœ€è¦ä½œç¨€ç–è®¡ç®—ï¼Œè¿™é‡Œé€šè¿‡æ–½åŠ  maskçš„æ–¹å¼ï¼Œåªæ˜¯æ¨¡æ‹Ÿäº†ç¨€ç–è®¡ç®—ã€‚çœŸæ­£çº¿ä¸Šå¹¶ä¸èƒ½ç”¨è¿™å¥ï¼Œå®ƒä¸æé€Ÿ
        # ä¸Šé¢ scores å·²ç»æ˜¯ç»è¿‡ indexer è¿‡æ»¤åçš„åªé’ˆå¯¹ 2048 ä¸ª tokençš„ï¼ˆå…¶ä½™çš„æ˜¯0ï¼‰

        ... go on ..
        return ..

```

è¶…å‚æ•°é…ç½®ï¼š

```
{
    ...
    # MLA çš„é…ç½®
    "n_heads": 128,

    "q_lora_rank": 1536,       # q è¢«å‹æˆ 1536 ç»´ 
    "kv_lora_rank": 512,       # k è¢«å‹æˆ 512 ç»´

    "qk_nope_head_dim": 128,   # érope åˆ†æ”¯ head dim
    "qk_rope_head_dim": 64,    # rope åˆ†æ”¯ head dim

    "v_head_dim": 128,

    # indexer çš„é…ç½®
    "index_n_heads": 64,
    "index_head_dim": 128,     # indexer æ€» head_dim=128ã€‚å…¶ä¸­ä¼šåˆ† ropeã€ érope ä¸¤éƒ¨åˆ†ï¼Œçœ‹ä»£ç æ˜¯å„å 64ç»´
    "index_topk": 2048         # åºåˆ—å†é•¿ï¼Œä¹Ÿåªä¸€æ¬¡ä¼˜é€‰å‡º 2048 ä¸ª token ä½œattn
}
```

- indexer ç›¸æ¯” MLA attnï¼š headæ•°å‡åŠï¼ˆ128=>64), head_dim å˜ä¸‰åˆ†ä¹‹äºŒ(128+64 => 128).
- cache æ˜¾å­˜å ç”¨ï¼šindexer ä¹Ÿéœ€è¦æœ‰ç±»ä¼¼ kv-cache ä¸€æ ·çš„ä¸œè¥¿ï¼Œå®ƒæ˜¯ k-cacheã€‚
  - indexer å¯¹å•ä¸ª token ç”¨ 129å­—èŠ‚ï¼ˆåŒ…æ‹¬ 128å­—èŠ‚çš„ fp8 æ ¼å¼çš„ kï¼Œä»¥åŠ1å­—èŠ‚çš„ fp8é‡åŒ– scale å› å­ï¼‰
  - è€Œ MLA çš„ kv-cache ä¸€ä¸ªtoken ç”¨ ï¼ˆ512+64ï¼‰* sizeof(dtype); dtype=fp32ï¼ˆattn éœ€è¦é«˜ç²¾åº¦ï¼‰ï¼Œåˆ™ æ€»å…± (512+64)*4=2304
  - æ–°å¢æ˜¾å­˜å ç”¨ 129/2304 = 5.6%ï¼Œç¡®å®æ–°å¢ä¸å¤š
- è®¡ç®—é‡ï¼šæ¨ç†æˆæœ¬é™ä½å¾ˆå¤šã€‚å…·ä½“çœ‹ä¸‹é¢

### æ€ä¹ˆè®­ç»ƒçš„

åœ¨å·²ç»ç”¨ MLA è®­ç»ƒçš„æ¨¡å‹ä¸Šï¼Œæ–°å¢ indexer ç»§ç»­è®­ç»ƒå³å¯æ–°å¢è¯¥åŠŸèƒ½ã€‚åˆ†é¢„è®­ç»ƒä¸åè®­ç»ƒã€‚

**é¢„è®­ç»ƒåˆ†ä¸¤æ­¥ï¼š**

ï¼ˆ1ï¼‰ã€å†»ç»“å…¶ä»–å‚æ•°ï¼Œåªè®­ç»ƒ indexerï¼šä»¤ indexer é¢„æµ‹ MLA çš„ attn softmax çš„ input QK' scoreã€‚indexer ä¸é€‰å–top2048ï¼Œè€Œæ˜¯é€‰å–æ‰€æœ‰ token

$$
Loss = \sum_t D _ {KL}( p _ {t,:} || softmax(I _ {t,:}))
$$

$p _ {t,:}$ æ˜¯ softmax(QK'), è€Œ softmax(Indexer_score)= $softmax(I _ {t,:})$ åº”è¯¥é€¼è¿‘å®ƒã€‚å­¦ä¹ ç‡ 1e-3ï¼Œè®­ç»ƒ 1000 æ­¥ï¼Œæ¯æ­¥ 16 ä¸ª 128K token åºåˆ—ï¼Œæ€»è®¡ 21 äº¿ tokenã€‚

ï¼ˆ2ï¼‰ã€åœ¨å…¨é‡å‚æ•°ä¸Šè®­ï¼Œä¸” indexer é€‰å– topK=2048ã€‚loss å’Œä¸Šä¸€æ­¥ä¸€æ ·ï¼Œåªæ˜¯é™åˆ¶åœ¨ topK scored tokens ä¸Šã€‚

**åè®­ç»ƒï¼š**

æµç¨‹å’Œ DeepSeek-V3.1-Terminus ä¿æŒä¸€è‡´ï¼Œä»¥ä¾¿ä¸¥æ ¼å¯¹æ¯” DSA ç¨€ç–æ³¨æ„åŠ›çš„å½±å“ã€‚

- Specialist Distillationï¼ˆä¸“å®¶è’¸é¦ï¼‰ï¼Œä¸ºä¸åŒä»»åŠ¡å•ç‹¬è®­ç»ƒä¸“å®¶æ¨¡å‹ï¼ˆæ•°å­¦ã€ç«èµ›ç¼–ç¨‹ã€é€»è¾‘æ¨ç†ã€æ™ºèƒ½ä»£ç ã€æ™ºèƒ½æœç´¢ï¼‰ã€‚
- Mixed RL Trainingï¼šä½¿ç”¨ GRPO ç®—æ³•ã€‚

æœ€ç»ˆæ•ˆæœçœ‹èµ·æ¥æ²¡é€€åŒ–ã€‚

### ç”¨äº train/prefill å’Œç”¨äº decoding

**prefill:** é‡ç‚¹åœ¨äºè¦ batch è€Œä¸æ˜¯ä¸€ä¸ªä¸€ä¸ª
- çŸ­åºåˆ—ï¼šèµ° MHA æ¨¡å¼çš„ DSAã€‚åœ¨æ ‡å‡† MHA ä¸Šï¼Œå¹²é¢„ attention mask çš„æ–¹å¼ï¼ˆåº”è¯¥å°±æ˜¯ä¸Šé¢æ‰€å¼•è¿°çš„ä»£ç çš„æ–¹å¼ï¼› training æ—¶ï¼Œåº”è¯¥ä¹Ÿæ˜¯è¿™æ ·ï¼‰ã€‚
  > for short-sequence prefilling, we specially implement a masked MHA mode to simulate DSA, which can achieve higher efficiency under short-context conditions.
- é•¿åºåˆ—ï¼š
  - Indexer çš„ score è®¡ç®—ï¼Œæ˜¯å¯ä»¥åƒ attn ä¸­ QK' è¿™æ ·ä¸€æ¬¡æ€§ batch ç®—å‡ºæ¥çš„ï¼Œæ‰€ä»¥å¯ä»¥å¹¶è¡Œ
  - ä¸‹ä¸€æ­¥æ˜¯æ¯ä¸ª token ä½ç½®åˆ†åˆ«é€‰ topK=top_2048, æŒ‰ ai è§£é‡Šï¼Œè¿™ä¸€æ­¥ç¡¬ä»¶ä¸Šä¹Ÿæ˜¯å¯ä»¥å¹¶è¡Œç®—çš„ã€‚
    - aiï¼šç°ä»£ GPU/TPU å®ç°ï¼ˆå¦‚ PyTorch çš„ torch.topk æˆ– CUDA å†…æ ¸ï¼‰éƒ½æ˜¯æŠŠè¾“å…¥çŸ©é˜µçš„æ¯ä¸€è¡Œåˆ†é…åˆ°ä¸€ä¸ª warp/blockï¼›æ¯è¡Œç‹¬ç«‹åœ°æ‰§è¡Œ top-k æ’åºï¼›
  - æ¯ä¸ª token ä½ç½®çš„ top2048 çš„ä½ç½®ä¸åŒä¸”ä¸è¿ç»­ï¼Œå› æ­¤éœ€è¦æŠŠæ¯ä¸ªä½ç½®çš„2048 ä¸ª top ä½ç½®çš„ kv æ”¶é›†èµ·æ¥ã€‚æŒ‰ aiï¼Œå¯ä»¥é«˜æ•ˆ Gatherï¼ŒæŠŠç¨€ç–å˜â€œç¨ å¯†å°å—â€
    - aiï¼šä¸æ˜¯â€œä¸€ä¸ª token ä¸€ä¸ª tokenâ€ åœ°å¾ªç¯ gatherï¼Œè€Œæ˜¯å¯ä»¥é«˜åº¦å¹¶è¡Œçš„ã€‚æ”¶é›†å®Œï¼Œå¾—åˆ°äº† $K_{sel}ã€V_{sel} \in \mathbb{R}^{batch \times seq \times 2048 \times dim}$
  - å‰é¢çš„å¯ä»¥å¹¶è¡Œå¹¶å¾—åˆ°è¿ç»­çš„ $K_{sel}, V_{sel}$ï¼Œåˆ™ $\text{softmax}(Q \cdot K' _ {sel}) \cdot V _ {sel}$ æŒ‰ä¸€èˆ¬ attn é‚£æ · batch è®¡ç®—å°±æ˜¯äº†ã€‚

training æ—¶ï¼Œå’Œ prefill ä¸€æ ·ã€‚

**decoding:**
- MQA æ¨¡å¼é€ token ç”Ÿæˆã€‚

---

## è®¡ç®—æˆæœ¬

é•¿åºåˆ—æ—¶ï¼Œæ¨ç†æˆæœ¬ä¸‹é™å¾ˆå¤šï¼Œæ— è®ºæ˜¯ prefill è¿˜æ˜¯å• step inferenceã€‚
> DSA reduces the core attention complexity of the main model from O(ğ¿Â²) to O(ğ¿ğ‘˜), where ğ‘˜(â‰ªğ¿) is the number of selected tokens. Although the lightning indexer still has a complexity of O(ğ¿Â²), it requires much less computation compared with MLA in DeepSeek-V3.1-Terminus

<img width="1294" height="604" alt="image" src="https://github.com/user-attachments/assets/97a782af-19d6-430f-8004-6913475bdb9c" />

ï¼ˆä»å›¾çœ‹ï¼ŒçŸ­åºåˆ—æ¨ç†æˆæœ¬ä¼šç•¥ä¸Šå‡ï¼‰

### è®¡ç®—é‡åˆ†æï¼ˆFLOPSï¼Œai è¾…åŠ©è®¡ç®—ï¼‰

**ï¼ˆ1ï¼‰FFNï¼ˆMOE) å±‚**

- Dense-FFN æ¯å±‚ä¸ºï¼ˆå‰ 3 å±‚ï¼‰ï¼šä¸‰æ¬¡çº¿æ€§ 3 Ã— dim Ã— ffn_hid = 3 Ã— 7168 Ã— 18432 = 396361728 
- MOE æ¯å±‚ä¸ºï¼ˆå 58 å±‚ï¼‰ï¼š
  - æ¯ä¸ªä¸“å®¶è®¡ç®—é‡ï¼š 3 Ã— dim Ã— moe_hid = 3 Ã— 7168 Ã— 2048 = 44040192 ã€silu æ¿€æ´»çš„ FFN: $\text{MLP}(x) = W_2 \big( \text{SiLU}(W_1 x) \odot (W_3 x) \big)$ ã€‘
  - æ¯ token æ¿€æ´» 8 ä¸ªä¸“å®¶ï¼š 8 Ã— 44040192 = 352321536
  - ä¸€ä¸ªå…±äº«ä¸“å®¶ï¼šæ‰¿ä¸Šä¸º 44040192
  - è·¯ç”±æ‰“åˆ†ï¼šdim Ã— n_experts = 7168 Ã— 256 = 1835008
  - åˆè®¡ï¼š352321536 + 44040192 + 1835008 = 398196736
- 61å±‚æ€»è®¡ç®—é‡ï¼š58 * 398196736 + 3 * 396361728 = 24284495872

**ï¼ˆ2ï¼‰LM Headï¼ˆæœ€å logitsï¼‰**
- dim Ã— vocab = 7168 Ã— 129280 = 926679040

**ï¼ˆ3ï¼‰æ³¨æ„åŠ›**

**ä¸éš n å¢é•¿çš„éƒ¨åˆ†ï¼š**

**MLAï¼ˆMQA æ¨¡å¼ï¼‰**
- Qï¼š
  - Wq_a : dim Ã— q_rank = 7168 Ã— 1536 = 11010048    ã€å¾—åˆ° q çš„å‹ç¼© latent è¡¨ç¤ºã€‘
  - Wq_b : q_rank â†’ (heads Ã— (rope_head_dim + non_rope_head_dim)) = 1536 Ã— (128 Ã— (64+128)) = 37748736 ã€q è§£å‹è¿˜åŸã€‘
  - q_nope Ã— W(128 â†’ 512): 128å¤´ Ã— 128 Ã— 512 = 8388608  ã€è·å¾— MQA çš„ Q := q_no_rope Ã— k_up_projã€‘
- Kï¼š
  - Wkv_a : dim â†’ (k_rank + k_rope) = (512 + 64) = 576 : 7168 Ã— 576 = 4128768  ã€å¾—åˆ° kv çš„å‹ç¼© latent è¡¨ç¤ºï¼Œ ä½œä¸ºä¸‹ä¸€ä¸ªtoken è®¡ç®— MLA çš„ MQA çš„ Kçš„ä¸€éƒ¨åˆ†ï¼ˆæœ¬è½®ä¼šæŠŠå®ƒæ”¾å…¥ kv-cacheï¼‰ã€‘
- Vï¼š
  - value å›æŠ• (512 â†’ 128): åŒä¸Š 8388608  ã€è§£å‹è¿˜åŸ vã€‘
- è¾“å‡ºæŠ•å½± Wo : (head_num Ã— head_dim) = (128 Ã— 128) = 16384 â†’ 7168 : 16384 Ã— 7168 = 117440512 ã€attn ä¹‹åçš„ output projã€‘
- ä»¥ä¸Š61å±‚æ€»å…±ï¼š61*(11010048+37748736+8388608+4128768+8388608+117440512)=11413422080

**indexer ä¸­çš„å®šå€¼å¼€é”€:**
- Wq_b : q_rank â†’ (index_head_num Ã— index_head_dim): 1536 â†’ (64 Ã— 128) = 8192 : 1536 Ã— 8192 = 12582912 ã€å¾—åˆ°å‚ä¸ index score è®¡ç®—çš„å¤šheadçš„ qã€‘
- Wk: hidden_dim â†’ index_head_dim: 7168 â†’ 128 : 7168 Ã— 128 = 917504 ã€å¾—åˆ°å‚ä¸ index score è®¡ç®—çš„å•head çš„ kã€‘
- weights_proj : hidden_dim â†’ index_head_numï¼š7168 â†’ 64 : 7168 Ã— 64 = 458752  ã€å¾—åˆ°å‚ä¸ index score è®¡ç®—çš„é€head weightã€‘
- ä»¥ä¸Š61å±‚æ€»å…±ï¼š61*(12582912+917504+458752) = 851509248

**éš n çº¿æ€§å¢é•¿çš„éƒ¨åˆ†(è§£ç è¦ä¸å†å² n ä¸ªä½ç½®åšç‚¹ç§¯)ï¼š**

**MLA(MQA æ¨¡å¼ï¼‰**

å¦‚æœæ˜¯å…¨åºåˆ—ç®—ï¼š

- QK'
  - no-ropeéƒ¨åˆ†: q_nope Ã— KV-cache: 128å¤´ Ã— 512 Ã— n = 65536n
  - ropeéƒ¨åˆ†ï¼šq_pe Ã— PE-cache: 128å¤´ Ã— 64 Ã— n = 8192n
- softmax(.)V: softmaxæƒé‡ Ã— V-cache: 128å¤´ Ã— 512 Ã— n = 65536n
- ä»¥ä¸Š61å±‚æ€»å…±ï¼š61*(65536+8192+65536)*n=8495104n
  
å¦‚æœæ˜¯ topK=top-2048 æŒ‰ç¨€ç–æ³¨æ„åŠ›ç®—ï¼šå³ n=2048

- QK'
  - no-ropeéƒ¨åˆ†: q_nope Ã— KV-cache: 128å¤´ Ã— 512 Ã— 2048 = 134217728
  - ropeéƒ¨åˆ†ï¼šq_pe Ã— PE-cache: 128å¤´ Ã— 64 Ã— 2048 = 16777216
- softmax(.)V: softmaxæƒé‡ Ã— V-cache: 128å¤´ Ã— 512 Ã— 2048 = 134217728
- ä»¥ä¸Š61å±‚æ€»å…±: 61 * (65536+8192+65536) * 2048=17397972992

**indexer**

- QK': q Ã— k_cache: 64å¤´ Ã— 128 Ã— n = 8192n
- ä»¥ä¸Š61å±‚æ€»å…±: 61 * 8192 * n=499712n

é‚£ä¹ˆï¼š

ï¼ˆ1ï¼‰ã€å¦‚æœä¸å¼•å…¥ indexerï¼Œè®¡ç®—é‡æ˜¯ï¼š
- ffnï¼š24284495872
- lm_headï¼š926679040
- mla-staticï¼š11413422080
- mla-dynamicï¼š 8495104n
- æ€»å…±ï¼š36624596992+8495104n

ï¼ˆ2ï¼‰ã€å¦‚æœå¼•å…¥ indexerï¼Œä½œç¨€ç–æ³¨æ„åŠ›è®¡ç®—é‡æ˜¯ï¼š
- ffnï¼š24284495872
- lm_headï¼š926679040
- mla-staticï¼š11413422080
- mla-top2048ï¼š 17397972992
- indexer_static: 851509248
- indexer_dynamic: 499712n
- æ€»å…±ï¼š54874079232 + 499712n

æ¨æ–­ n ä½ç½®çš„è®¡ç®—é‡ï¼šå¼•å…¥indexerå vs åŸå§‹ï¼Œæ¯”ç‡æ˜¯ï¼š (54874079232 + 499712 * n) / (36624596992+8495104 * n). æ³¨æ„å½“ $n -> \infty$, æé™æ˜¯ 1/17ã€‚åŠ é€Ÿæ¯”ç¡®å®å¾ˆå¯è§‚

| n | è®¡ç®—é‡æ¯”ç‡ |
|------------|----------------------|
| 2000       | 1.0421               |
| 2200       | 1.0119               |
| 2300       | 0.9975               |
| 2500       | 0.9699               |
| 3000       | 0.9076               |
| 4000       | 0.8055               |
| 8000       | 0.5629               |
| 16000      | 0.3644               |
| 32000      | 0.2297               |
| 64000      | 0.1497               |
| 128000     | 0.1057               |
| 256000     | 0.0827               |
| 512000     | 0.0708               |
| $\infty$  | 1/17=0.0588 |


