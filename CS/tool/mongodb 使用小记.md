# mongodb 使用小记

本次有七八个表好几亿的数据需要存储且提供查询，如果存存mysql数据库，存进去后的响应速度怎样先不说，首先导进去就会累死人，只怕五天五夜夜晚成不了。因此就没有考虑mysql。

久闻mongodb大名，决定试试它。因为靠感觉，以及它的自我介绍，性能优异。

<br>

只是，现实总是与理想差太远。现在想到哪，写到哪，把遇到的问题，以及解决方式记录于下面。以备下次查看。

<br>

首先，单纯靠多线程插入，来加快插入速度没太大用。网上看说mongodb加的锁的粒度比较大，是database粒度，那就是说同一个db内部的多个表（或者mongodb的术语是collection）是没法同时做插入操作的。反正我就每个表一个db了。而且，在十几核的机器上，只好只是三四个插入线程。

看mongodb说全索引支持。我以为它会自动索引所有字段，同时还保持高效率。后来才发现我错了。根本不是那回事。好不容易把所有数据插入后，检索巨慢无比。看来当然没有自动索引。'

于是看怎样加索引，并掉用相应的命令。但是，这个加索引好慢啊，等了半天也没用反应。于是只好全删除数据，采用先加索引再插入数据的策略。这一次又发现，尼玛，插入数据有几分之一后，变得巨慢无比了，cpu io wait time还百分之十几二十的样子。

没办法，只好重来。我估计是索引太大导致的，于是对一个表分成了十几份。但是，最后还是遇到那样的问题。后来从网上看到说确实带索引插入容易导致那样的问题，似乎是内存大小受限的缘故。看了下内存，确实所剩不多了。但是已经60G内存吃掉了，真没有更多了。

最后灵机一动，想到可以试试先不带索引的分表插入，最后建索引。这次终于是还差不多。因为不带索引插入，所以速度还可以，也没用出现io wait太大问题。最后建索引的时候，因为每个分表都很小，只有几百万数据，因此还是比较快的。总共算下来，有四五个小时把几亿条数据导入了。

<br>

总结起来看就是，使用mongodb的时候，第一要注意该加索引的字段一定要加，否则检索慢死；第二，即使想好了哪些字段应该加索引，如果数据规模非常大，什么时候执行加索引，以及怎样的分表策略都是需要注意的（如果不考虑插入时间，当然何必分表呢）。

2014-12-02
