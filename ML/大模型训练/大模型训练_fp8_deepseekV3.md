
### ËÉåÊôØ

ÂÖ≥‰∫é deepseek-v3 ÁöÑ fp8 ËÆ≠ÁªÉÁöÑËÉåÊôØ‰ªãÁªçÔºåÂéüÊñáÊëòÂΩïÂ¶Ç‰∏ãÔºà3.3ËäÇ „ÄäFP8 Training„ÄãÔºâÔºö

> Inspired by recent advances in low-precision trainingÔºàËßÅ‰∏ãÈù¢ÂºïÊñáA,B,CÔºâ, we propose a fine-grained mixed precision framework utilizing the FP8
data format for training DeepSeek-V3.
>
> „ÄêÂèóÂà∞Âà´ÁöÑ fp8 ÊñπÊ°àÂêØÂèë„Äë
- A=„ÄäGpt3. int8 (): 8-bit matrix multiplication for transformers at scale„Äã- 2022.08 - https://arxiv.org/pdf/2208.07339
- B=„Ää8-bit numerical formats for deep neural networks„Äã- 2022.06 - https://arxiv.org/pdf/2206.02915
- C=„ÄäFP8-LM: Training FP8 large language models„Äã - 2023.10 - https://arxiv.org/pdf/2310.18313

> While low-precision training holds great promise, it is often limited by the presence of outliers in activations, weights, and gradientsÔºàËßÅ‰∏ãÈù¢ÂºïÊñáD,EÔºâ.
>
> „Äê‰ΩÜÊòØÂΩìÂâçÁöÑ fp8 ÊÄªÊòØÂèóÂõ∞‰∫é outlier ÈóÆÈ¢ò„Äë
- D=„ÄäScaling FP8 training to trillion-token llms„Äã- 2024.09 - https://arxiv.org/pdf/2409.12517
- E=„ÄäMassive activations in large language models„Äã - 2024.02 - https://arxiv.org/pdf/2402.17762

> Although significant progress has been made in inference quantization (ËßÅ‰∏ãÈù¢ÂºïÊñáF,G), there are relatively few studies demonstrating successful application of low-precision techniques in large-scale language model pre-training (ËßÅ‰∏ãÈù¢ÂºïÊñá D).
>
> „ÄêÊé®ÁêÜÈáèÂåñÊúâËøõÂ±ïÔºå‰ΩÜÊòØÂ§ßËßÑÊ®°‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉÁöÑÊàêÂäüËøòÊú™ËßÅ„Äë
- FÔºö„ÄäGptq: Accurate post-training quantization for generative pre-trained transformers„Äã- 2022.10 - https://arxiv.org/pdf/2210.17323
- GÔºö„ÄäSmoothquant: Accurate and efficient post-training quantization for large language models„Äã - 2022.11 - https://arxiv.org/pdf/2211.10438
- DÔºö„ÄäScaling FP8 training to trillion-token llms„Äã- 2024.09 - ÔºàÊ≥®ÊÑè‰∏äÈù¢‰πüÂá∫Áé∞‰∫ÜÔºâ https://arxiv.org/pdf/2409.12517

> To address this challenge and effectively extend the dynamic range of the FP8 format, „Äê‰∫éÊòØÊé®Âá∫ deepseek-v3 ÁöÑËß£Ê≥ï„Äë
>
> Ôºà1Ôºâ„ÄÅwe introduce a fine-grained quantization strategy: tile-wise grouping with 1 √ó ùëÅùëê elements or block-wise grouping with ùëÅùëê √ó ùëÅùëê elements. The associated dequantization overhead is largely mitigated under our increased-precision accumulation process, a critical aspect for achieving accurate FP8 General Matrix Multiplication (GEMM).
>
> Ôºà2Ôºâ„ÄÅMoreover, to further reduce memory and communication overhead in MoE training, we cache and dispatch activations in FP8, while storing low-precision optimizer states in BF16.
>
> Ôºà3Ôºâ„ÄÅWe validate the proposed FP8 mixed precision framework on two model scales similar to DeepSeek-V2-Lite and DeepSeekV2, training for approximately 1 trillion tokens (see more details in Appendix B.1). Notably, compared with the BF16 baseline, the relative loss error of our FP8-training model remains consistently below 0.25%, a level well within the acceptable range of training randomness.
