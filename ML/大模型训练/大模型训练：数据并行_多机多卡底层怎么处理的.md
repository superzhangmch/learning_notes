# 数据并行：多机多卡底层怎么处理的 

### 数据并行（DP=data parallel）

只做数据并行，则多机多卡与单机多卡无本质区别，都是在本卡上作forward再backward拿到本卡分配的数据的梯度后，在多卡上对梯度求和，再用此梯度“和”来更新参数。也就是DP 下，前向后向是并行的，但梯度求求和与参数更新却没法并行。

梯度求求和与参数更新不能并行，为此有两种方法：
- 参数 server 方法：求和与更新集中在某一server 上，但显然扩展性差。
  - 有一个（或几个）专门的服务器节点负责收集所有工作卡的梯度，求和更新，再把新参数广播回去。
  - 优点：实现简单，逻辑清晰。
  - 缺点：通信瓶颈严重 → 所有梯度数据都要集中到 server，server 带宽/计算容易成为瓶颈，扩展到很多卡时性能急剧下降。
  - <img width="1488" height="444" alt="image" src="https://github.com/user-attachments/assets/035f0a0e-00cc-4c72-a3e4-a5025304d8f6" />
- all-reduce：即每个卡都做求和更新，这样通信开销大。
  - 没有集中式服务器，而是所有卡通过分布式通信协议（如 NCCL、Ring All-Reduce）在彼此之间传输数据，最终得到相同的梯度和（reduce的方式求和，而非拿到各分片独立求和），然后各自覆盖更新自己的参数副本。
  - 优点：去中心化，通信负载分摊，扩展性好（尤其是 GPU-NVLink 或高速网络下）。
  - 缺点：通信量依然大（传输的是整个梯度张量），而且需要优化拓扑和调度来减小延迟。
  - <img width="532" height="854" alt="image" src="https://github.com/user-attachments/assets/a2a73964-0c79-4b97-acfd-32b634ca287e" />

### 大模型分布式训练，网络带宽要求很高。

现在是大模型时代，1B 参数的都不算大，但 1B 参数的model，单卡虽加载容纳没问题，按参数server模式传送自己梯度出去需 4GB 带宽(单参数用 float32)，接受最终结果也得 4GB，在一个 iteration 内就需这么大的网络流量。何况同时有好多个GPU 都在这样做。训练集群内网络流量真是海量！

所以大模型分布式训练，首先对网络通信要求就好高（注意在传统分布式系统中（非分布式训练），网络流量要小得多）

上面说用参数server 才做到了 1B model 一次迭代，单卡只需 4+4 = 8GB 带宽，但是参数server本身单点，有扩展问题。

若不用他，则按朴素想法一对一互传，网络流量就更高了：假如有 N 卡并行，则本卡需将 4G 大小的梯度都传给 N 卡的，总通信是N(N-1)/2。

猜测或许有这样一种广播方式：每个人发出自己1份，然后收听接受别人的广播（N-1），也就是全员同时广播自己数据，大家并行接收并本地累加的“全广播 + 本地累加”。这样总链路里只有 N 份数据。就像每个人都拿对讲机，广播自己，同时接受所有人，而空中只有 n 个电波。但是：在无线电（对讲机）中，“空中”是共享介质，每个人的信号可以物理上被所有人同时接收（多播/广播天然成立）。GPU 之间是通过点对点链路连接的，你发的数据只会到你的直连链路对端，除非交换机/路由器帮你复制。数据没法并存于数据链路。每个接收端要同时接收 N-1 条数据流，这会占满它所有的入链路带宽，并且在大型集群里很快成为瓶颈。

要让所有 GPU 都收到你的数据，必须网络设备为你复制 N-1 份数据（多播），或者软件逐个发 → 成本和延迟上去。

### ring-all-reduce 降分布式网络通信量

为降网络通讯量，人们想出了 Ring-allreduce 方法：即对其 N 卡把参数切分为 N 份，每卡只负责一份；这份参数以环状流经各卡并逐次累加，传到负责它的卡时，该卡拿到了最终梯度和，于是该卡可以更新参数，并把更新后参数再沿环传一圈。于是，每卡的进出数据量都等于约各总梯度数据，实现了最小化。

原理看下图：参 https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/

先是数据起流，沿途累加（Reduce-Scatter阶段）：

<img width="2416" height="954" alt="image" src="https://github.com/user-attachments/assets/83292a7e-a3c9-42f3-8831-20cbb841ec60" />

累加满后，继续沿环传播（All-Gather阶段）：

<img width="1210" height="886" alt="image" src="https://github.com/user-attachments/assets/0a8814f4-9f33-4d13-bb1a-44dd27620b11" />

单个 GPU_i 的通信量：
- 对于自己负责起始触发的那份数据(data_i)：发（起始发送）+ 收（收到完整梯度）+ 发（转发出去）
- 对于到自己正好累加够的那份数据(data_{i-N})：收（收到完整梯度）+ 发（加满后发出去）
- 自己是最后一个拿到最终结果的：收（收到累加中的数据）+ 发（累加自己后的数据）+ 收（收到最终结果）
- 对于其他数据：收（收到累加中的数据）+ 发（累加自己后的数据）+ 收（收到最终结果）+ 发（把最终结果传下去）

以上共 2(N-1) 个收，2(N-1) 个发，每个收发的数据是 1/N。所以整个过程的单卡in、out 流量都是 2(N-1)/N ≈ 2倍的参数量。

这个方法问题是：数据要沿环转一圈，卡数多时，latency太大。

于是人们想出了其它方法来改进，主要思路是分层做 ring-allreduce，比如单机多卡间通讯快，那就先单机内reduce出梯度，然后再在机器间做 ring-allreduce。简单注意：GPU间无论单机内还是机器间，往往有特殊高速连接设备（比如 NV-Link、NV-Switch），及各种复杂的拓扑结构，而非通过 TCP-IP 这样的互联，否则也实现不了 n多卡时分布式训练的。

英伟达的NCCL即实现多机多卡间通讯的一个库，它实现了一个叫 double-二叉树 的算法，比 ring-allreduce 还高效；在保持网络流量持平基础上，延时由线性降到了(log(N卡))，因其用了二叉树的逐层往上reduce的方式，故时延是log的。而用两棵树是为了最小化网络流量：令每卡要么在这树中做leaf，要么另一树中做leaf，从而两树中每个卡的父卡与子卡个数都正好是2，这样每树负责一半的数据即可。

这样，在特殊网络硬件与好的算法支持下，做一个几百卡的分布式训练就不是梦了。
