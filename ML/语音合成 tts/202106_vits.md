# vits 《Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech》 https://arxiv.org/pdf/2106.06103

下面是整体流程：

![image](https://github.com/user-attachments/assets/33c57b25-6448-4f9e-b9b6-a714f950eb51)

----

下面是一些细节说明：

![image](https://github.com/user-attachments/assets/66f5cef3-2806-4c4b-a1d6-86af68b0fe0d)

----

### loss

![image](https://github.com/user-attachments/assets/4ad17118-9daa-40d5-8c5a-26af2722753f)

![image](https://github.com/user-attachments/assets/53cc5f9d-379b-4891-9254-b3dca5494888)

note:
- 重建用的 mel 谱，但是有些地方用的线性谱，而非非线性的 mel-谱。
- text-audio 对齐：text 切分成 tokens 长度，和生成的 audios 的某种 frames 的长度不一样，且不能线性对应，所以需要类似 CTC 之类的东西来做对齐。vits 训练时用的是"单调对齐搜索"的方式，它本身只是一个对齐算法，本身没参数，本身不需要训练。但是对齐结果可以用来当场统计出每个 text token 的时长，从而用于训练 "时长预测器"。而 inference的时候，根据时长预测结果，把 text tokens 的 hidden 表示(mu, sigma) 做repeat 即可。
- 用到了 GAN loss。
- f_theta(z) 以及 stochastic duration predictor 都是可逆的，所以图中箭头方向在 train / infer 中是反着的。
  
