# 关于用带权重的样本来训练决策树

一般机器学习算法面对带权重的训练样本时，处理方式往往是权重乘该样本损失形式：
 $final\\_loss = \sum w_i  L(x_i, y_i)$ ，
其中{ $(x_i, y_i)$ } 是样本集， $w_i$ 是 ( $x_i, y_i$ ) 的weight。也就是说，转化成了对 Loss 的加权。

对于决策树，微软的lightGbm的实现也是体现了上述方式。lightGBM 支持训练样本带权重，通过分析其源代码，发现是把 weight 乘上了 loss 函数的梯度及 hessian（二阶导数）当做新的梯度与hessian。这时，pseudo-reponse就是改写后的了。

似乎还有另外一张实现 sample weight 的方式（见 [Ranking Relevance in Yahoo Search](http://www.kdd.org/kdd2016/papers/files/adf0361-yinA.pdf) 。其中介绍 LogisticRank 算法时，特意提到了这种不同： 
> This method（指LogisticRank中对GBDT的pseudo-reponse加权的方式） is actually equivalent to adding different weights on loss for different labels, but it is different from adding sample weights, which actually affect the tree growth of each stage m but do not change pseudo-response.

显然，这篇文章中的方法正是 lightGBM 的做法）。这时，似乎体现在树的构建中，通过影响 split 函数（比如信息增益或dini）来影响节点的分裂。

【参】
- https://stackoverflow.com/questions/36700819/weighing-samples-in-a-decision-tree
- https://stackoverflow.com/questions/1132805/weighted-decision-trees-using-entropy
- https://stackoverflow.com/questions/34389624/what-does-sample-weight-do-to-the-way-a-decisiontreeclassifier-works-in-skle
- http://scikit-learn.org/stable/modules/tree.html#classification
