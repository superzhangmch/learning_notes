### 思路总结：

对于被推荐的 item（video item），token 化表示它。从而对于用户的历史消费 items，就可以变成 token list。对它们作 encode 的结果给到一个 T5 一样的 encoder-decoder 结构的 model，由该 model 作自回归生成新的 token list（每个token代表一个 item），完成推荐。
- 所用的大模型，不是预先训好的（通用）模型，而是从零开始训的——毕竟它不使用 text tokens，只使用 item tokens。
- 用户对单个 item 的反馈（时长、点踩）在特征输入层面没有反馈，这个是通过 RLHF-DPO 时的 reward model 使用到的。
- 所用 model 是 1B 参数的。

model inference 流程：

<img width="1688" height="664" alt="image" src="https://github.com/user-attachments/assets/9266f9b2-3cf5-479b-a927-5274c802f797" />

### 推荐 item 怎么 token 化

用的 RQ-VAE 算法离散化成的 token。

推荐 item 的 token 化过程：先提取 item embedding，然后依次通过多个 codebook，逐次对前几级处理的残差作最近邻匹配得到 code，并把各级code拼起来得到最终 token。

<img width="702" height="566" alt="image" src="https://github.com/user-attachments/assets/ea56028b-63d3-48a1-aadf-fd88d2e42b99" />

而那些残差 codebooks 怎么来：从 item 库，逐级残差方式统计出来的（用 Balanced K-means Clustering 方法， paper 中 算法一）。

### 反馈数据怎么用： RLHF-DPO

