# QAT: 《Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference》 https://arxiv.org/pdf/1712.05877 google

现在主流的 QAT 方法，基本都源自 google 这篇 paper 的思路，包括 PyTorch 的 QAT 实现。

<img width="1032" height="530" alt="image" src="https://github.com/user-attachments/assets/bfc12902-a12d-4f0b-8c74-8d71bacfe5c6" />

**推理时数据类型：**

paper 中推理时数据类型：
- 只用整数运算：没有 float 参与，整个运算链路都是整数（如上图左可见确实如此）
 - 权重 和 激活 都是 8-bit 整数（uint8 / int8）。
 - 累加器 使用 32-bit 整数（int32），这样可以避免溢出。
 - bias（偏置） 也存成 32-bit 整数，因为偏置误差累积会影响整体精度

torch QAT: 对于量化算子部分如此。

**使用时机：**

一般是先 float 预训练 → QAT fine-tune。如果一开始就 QAT 训练，则更难收敛，因为模型一开始就承受量化误差。且收敛速度慢，需要更长训练时间。

