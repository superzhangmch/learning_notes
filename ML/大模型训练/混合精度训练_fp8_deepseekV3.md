
## èƒŒæ™¯

å…³äº deepseek-v3 ï¼ˆ https://arxiv.org/pdf/2412.19437v1 ï¼‰ çš„ fp8 è®­ç»ƒçš„èƒŒæ™¯ä»‹ç»ï¼ŒåŸæ–‡æ‘˜å½•å¦‚ä¸‹ï¼ˆ3.3èŠ‚ ã€ŠFP8 Trainingã€‹ï¼‰ï¼š

> Inspired by recent advances in low-precision trainingï¼ˆè§ä¸‹é¢å¼•æ–‡A,B,Cï¼‰, we propose a fine-grained mixed precision framework utilizing the FP8
data format for training DeepSeek-V3.
>
> ã€å—åˆ°åˆ«çš„ fp8 æ–¹æ¡ˆå¯å‘ã€‘
- A=ã€ŠGpt3. int8 (): 8-bit matrix multiplication for transformers at scaleã€‹- 2022.08 - https://arxiv.org/pdf/2208.07339
  - int8 é‡åŒ–æ¥æé€Ÿæ¨ç†ï¼šInt8 çŸ©é˜µä¹˜æ³•æ–¹æ¡ˆï¼Œç”¨åœ¨ Transformer FFN å’Œ Attn projã€‚
  - å‘ç°ç‰¹å¾ç»´åº¦ä¸­æœ‰ä¸€äº› outliers å½±å“é‡åŒ–æ•ˆæœï¼šäºæ˜¯åˆ†è€Œæ²»ä¹‹ã€‚
  - <img width="1000" alt="image" src="https://github.com/user-attachments/assets/752d7062-b186-474e-b1b9-5962ecde0542" />
- B=ã€Š8-bit numerical formats for deep neural networksã€‹- 2022.06 - https://arxiv.org/pdf/2206.02915
  - å…³æ³¨è®­ç»ƒã€‚
  - ç”¨æµ®ç‚¹ fp8 æ¯” fixed-pointï¼ˆint8ï¼‰å¥½ã€‚
    - å®šç‚¹ int8 å¯è¡¨ç¤ºçš„æ•°åˆ—ï¼Œç›¸é‚»é—´éš”å›ºå®šã€‚è€Œ fp8ï¼Œåˆ™æ˜¯é—´éš”ä¸ä¸€ï¼ˆ0 é™„è¿‘ç²¾ç»†ï¼Œè€Œç»å¯¹å€¼è¶Šå¤§ï¼Œçº¦ç²—ï¼‰ã€‚è€Œç¥ç»ç½‘ç»œçš„å‚æ•°æ¿€æ´»æ¢¯åº¦ç­‰éƒ½æ˜¯é›¶å‡å€¼çš„ã€‚æ‰€ä»¥ç”¨ fp8 æ›´å¥½ã€‚
    - <img width="1096" height="846" alt="image" src="https://github.com/user-attachments/assets/afd8684e-62aa-423f-87fd-1d14469d71b7" />
    - fp8 è¡¨ç¤ºçš„éçº¿æ€§æ€§å¦‚å›¾ï¼Œå¯å‚ https://asawicki.info/articles/fp8_tables.phpã€‚æ³¨æ„ E4M3 è¿˜æœ‰ä¸€ç§ç®—æ³•èŒƒå›´æ˜¯ -448~448.
  - æ¨èï¼šæ¿€æ´»/æƒé‡ç”¨ fp8=E4M3ï¼Œæ¢¯åº¦ç”¨ fp8=E5M2
  - å®ƒç”¨äº†å…¨å±€ loss scale è€Œéç»†ç²’åº¦é€å±‚æˆ–é€å¼ é‡ scale
- C=ã€ŠFP8-LM: Training FP8 large language modelsã€‹ - 2023.10 - https://arxiv.org/pdf/2310.18313
  - Nvidia Transformer Engineåªå¯¹çŸ©é˜µä¹˜æ³•ç”¨ fp8ï¼Œæœ¬æ–‡æŠŠ FP8 åº”ç”¨åˆ°è®¡ç®—ã€å­˜å‚¨å’Œé€šä¿¡å…¨è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œåˆ†å¸ƒå¼è®­ç»ƒã€‚
  - per-tensor scaling
  - ç²¾åº¦åˆ†é…
    - ä¸»æƒé‡ fp16, ä¼˜åŒ–å™¨adamçŠ¶æ€ï¼šfp8ï¼ˆä¸€é˜¶ï¼‰+fp16ï¼ˆäºŒé˜¶ï¼‰ï¼›æ¢¯åº¦ fp8ã€‚è¿™äº›æœ¬æ¥ä¸€ä¸ªå‚æ•°éœ€è¦16å­—èŠ‚ï¼Œå˜æˆäº† 6å­—èŠ‚
    - forwardã€backward æ—¶ï¼Œå…³é”®åœ°æ–¹å¤–ï¼ˆGELUã€Softmaxã€LayerNorm, dropoutç­‰ï¼‰ï¼Œéƒ½æ˜¯ fp8

> While low-precision training holds great promise, it is often limited by the presence of outliers in activations, weights, and gradientsï¼ˆè§ä¸‹é¢å¼•æ–‡D,Eï¼‰.
>
> ã€ä½†æ˜¯å½“å‰çš„ fp8 æ€»æ˜¯å—å›°äº outlier é—®é¢˜ã€‘
- D=ã€ŠScaling FP8 training to trillion-token llmsã€‹- 2024.09 - https://arxiv.org/pdf/2409.12517
  - ç”¨ 2T token è®­äº†ä¸ª 7B modelï¼Œå‘ç° fp8 çš„è®­ç»ƒä¸ç¨³æ¥è‡ª SwiGLU å¯¼è‡´çš„å¼‚å¸¸å€¼æ”¾å¤§ï¼Œå¹¶ç”¨ Smooth-SwiGLU æ”¹è¿›ä¹‹ã€‚
- E=ã€ŠMassive activations in large language modelsã€‹ - 2024.02 - https://arxiv.org/pdf/2402.17762
  - æå°‘æ•°è¶…å¤§ outlier æ¿€æ´»å€¼æ™®éå­˜åœ¨äºå„ LLMï¼ˆä¹ƒè‡³å¤§å‡º 10 ä¸‡å€ï¼‰ï¼Œæ–‡ä¸­å«è¿™ outliers ä¸º massive activationsï¼ˆä¸”è§äº paper æ ‡é¢˜ï¼‰ã€‚
    - æ­¤æ–‡å¹¶ä¸æ˜¯è®² FP8 è®­ç»ƒæ‰å¦‚æ­¤ã€‚è€Œæ˜¯å„ç§ç²¾åº¦çš„éƒ½æœ‰å¯èƒ½
  - æŸäº›ç»´ã€æŸäº› token æ‰å®¹æ˜“å‘ç”Ÿ
    - ä¸æ˜¯æ‰€æœ‰ channel éƒ½ massiveï¼šoutliers æ€»æ˜¯å‡ºç°åœ¨æŸäº› channel ç»´åº¦ï¼ˆä¸”å‡ºç°å‡ ç‡å¾ˆå°ï¼‰ã€‚
    - ä¸æ˜¯æ‰€æœ‰ token éƒ½ massiveï¼šåœ¨ä¸€äº›ç‰¹æ®Š token ä¸Šï¼ˆèµ·å§‹ <BOS>ã€å¥å· â€œ.â€ã€æ¢è¡Œç¬¦ \nã€åˆ†éš”ç¬¦ç­‰ï¼‰æ‰å¦‚æ­¤ã€‚
    - <img width="1162" height="658" alt="image" src="https://github.com/user-attachments/assets/293d84b3-e8b3-471c-a250-1d7633336fb2" />
  - ä»–ä»¬èµ·çš„ä½œç”¨æ˜¯ biasesï¼Œè‹¥å»æ‰ä¹‹ä¼šæ€§èƒ½ä¸‹é™ï¼ˆMassive activations act as fixed but important biases in LLMsï¼‰ã€‚attn ä¸­ç›¸å½“äºéšå¼ bias

> Although significant progress has been made in inference quantization (è§ä¸‹é¢å¼•æ–‡F,G), there are relatively few studies demonstrating successful application of low-precision techniques in large-scale language model pre-training (è§ä¸‹é¢å¼•æ–‡ D).
>
> ã€æ¨ç†é‡åŒ–æœ‰è¿›å±•ï¼Œä½†æ˜¯å¤§è§„æ¨¡ä½ç²¾åº¦è®­ç»ƒçš„æˆåŠŸè¿˜æœªè§ã€‘
- æ¨ç†æ—¶é‡åŒ–ï¼š
  - Fï¼šã€ŠGptq: Accurate post-training quantization for generative pre-trained transformersã€‹- 2022.10 - https://arxiv.org/pdf/2210.17323
  - Gï¼šã€ŠSmoothquant: Accurate and efficient post-training quantization for large language modelsã€‹ - 2022.11 - https://arxiv.org/pdf/2211.10438
- Dï¼šã€ŠScaling FP8 training to trillion-token llmsã€‹- 2024.09 - ï¼ˆæ³¨æ„ä¸Šé¢ä¹Ÿå‡ºç°äº†ï¼‰ https://arxiv.org/pdf/2409.12517
  - ç”¨ 2T token è®­äº†ä¸ª 7B modelï¼Œå‘ç° fp8 çš„è®­ç»ƒä¸ç¨³æ¥è‡ª SwiGLU å¯¼è‡´çš„å¼‚å¸¸å€¼æ”¾å¤§ï¼Œå¹¶ç”¨ Smooth-SwiGLU æ”¹è¿›ä¹‹ã€‚

> To address this challenge and effectively extend the dynamic range of the FP8 format, ã€äºæ˜¯æ¨å‡º deepseek-v3 çš„è§£æ³•ã€‘
>
> ï¼ˆ1ï¼‰ã€we introduce a fine-grained quantization strategy: tile-wise grouping with 1 Ã— ğ‘ğ‘ elements or block-wise grouping with ğ‘ğ‘ Ã— ğ‘ğ‘ elements. The associated dequantization overhead is largely mitigated under our increased-precision accumulation process, a critical aspect for achieving accurate FP8 General Matrix Multiplication (GEMM).
>
> ï¼ˆ2ï¼‰ã€Moreover, to further reduce memory and communication overhead in MoE training, we cache and dispatch activations in FP8, while storing low-precision optimizer states in BF16.
>
> ï¼ˆ3ï¼‰ã€We validate the proposed FP8 mixed precision framework on two model scales similar to DeepSeek-V2-Lite and DeepSeekV2, training for approximately 1 trillion tokens (see more details in Appendix B.1). Notably, compared with the BF16 baseline, the relative loss error of our FP8-training model remains consistently below 0.25%, a level well within the acceptable range of training randomness.

ä»ä»¥ä¸Šçš„å¯ç¤ºæ˜¯ï¼Œç”¨ fp8 æœ‰å¥½å¤„ï¼Œä½†æ˜¯æ€»æ˜¯æœ‰å¼‚å¸¸å¤§å€¼ï¼ˆoutliersï¼‰é—®é¢˜ã€‚deepseek-v3 è¯•å›¾è§£å†³è¿™ç‚¹ã€‚


## deepseek-v3 çš„è§£æ³•

### ä¸€ã€æ•´ä½“åšæ³•

å¤§ä½“ä¸Šå’Œ fp16ã€fp32 æ··åˆç²¾åº¦è®­ç»ƒçš„æ€è·¯æ˜¯ä¸€æ ·çš„ï¼šforwardã€backward ç”¨ä½ç²¾åº¦ã€‚è€Œä¼˜åŒ–å™¨å†…éƒ¨ç”¨é«˜ç²¾åº¦ï¼š

<img width="852" height="650" alt="image" src="https://github.com/user-attachments/assets/fc7df4bd-e13f-4c2f-ac29-916942bc24f8" />

ä¸»è¦æ˜¯å¯¹äºçŸ©é˜µä¹˜æ³•åšäº† FP8ï¼Œè€Œå¯¹ä¸‹é¢éƒ¨åˆ†ä¿æŒé«˜çš„ç²¾åº¦ï¼š
- the embedding module
- the output head
- MoE gating modules,
- normalization operators
attention operators

å…¶ç»†èŠ‚å¦‚ä¸‹å›¾ï¼š

<img width="1658" height="668" alt="image" src="https://github.com/user-attachments/assets/054cf088-55d2-4bab-9e0f-a4a17eb2e660" />

æ‹†è§£å¼€çœ‹ï¼š

<img width="678" height="616" alt="image" src="https://github.com/user-attachments/assets/11824955-e551-49ce-b022-03e403755594" />

å¦‚ä¸Šå›¾æ˜¯åªè€ƒè™‘ Linear å…¨è¿æ¥å±‚çš„æƒ…å½¢ã€‚æ³¨æ„çœ‹ FP8 GEMM è®¡ç®—è¿‡ç¨‹ï¼šä¼šåœ¨ FP32 ä¸­ç´¯åŠ  (å›¾ä¸­ Î£)ï¼Œè¿™æ˜¯ä¸ºäº†é˜²æ­¢ä½ç²¾åº¦æ—¶çš„ç´¯åŠ æ—¶ç²¾åº¦ä¸å¤Ÿï¼Œä¹ƒ deepseek-v3 çš„åˆ›æ–°ä¹‹ä¸€ã€‚å¦ä¸€åˆ›æ–°åˆ™æ˜¯ä¸ºè§£å†³ outlier é—®é¢˜ï¼Œå¯¹çŸ©é˜µtensor åˆ†å°å—ä½œ scaleã€‚

ä»¤ $y=XW$, $Loss = L(y, ..)$, å…¶ä¸­ X = g(..) æ˜¯ y çš„è¾“å…¥ï¼ŒW æ˜¯æƒé‡ã€‚å¯¹ Loss æ±‚æ¢¯åº¦æ—¶ï¼Œä¸ä½†è¦å¯¹ W æ±‚ï¼Œè¿˜è¦å¯¹ X=g(..) å†…çš„å‚æ•°æ±‚ã€‚è€Œä¸ºæ±‚åè€…ï¼Œå¿…é¡»å…ˆæ±‚ $\partial L / \partial X$ã€‚äºæ˜¯ä¸€æ¬¡ backward æ—¢è¦å¯¹ W ä¹Ÿè¦å¯¹ X æ±‚æ¢¯åº¦ã€‚åˆ™ $XW$ åœ¨è®­ç»ƒæ—¶æ¶‰åŠçš„ä¸‰ç±» GEMMï¼ˆçŸ©é˜µä¹˜åŠ è¿ç®—ï¼‰ï¼š

(a) **Fprop (Forward propagation)**

è®¡ç®—æ˜¯ $XW$, æŠŠè¾“å…¥æ¿€æ´» $X$ ä¸æƒé‡ $W$ ç›¸ä¹˜ã€‚

(b) **Wgrad (Weight gradient)**

åå‘ä¼ æ’­è®¡ç®—æƒé‡çš„æ¢¯åº¦, è®¡ç®—æ˜¯ $\nabla W = X^{\top} \nabla y$, ä¹ƒå¤–ç§¯ï¼Œå…¶ä¸­ $\nabla y = \frac{\partial \mathcal{L}}{\partial y}$ã€‚

(c) **Dgrad (Data gradient / Activation gradient)**

è¿™ä¸€æ­¥è¿›è¡Œåå‘ä¼ æ’­é‡Œè®¡ç®—è¾“å…¥çš„æ¢¯åº¦, å³ $\nabla X = \nabla y W^{\top}$ï¼ŒæŠŠæŸå¤±æ¢¯åº¦ $\nabla y$ åå‘ä¼ æ’­åˆ°è¾“å…¥ï¼Œä¾›å‰ä¸€å±‚ä½¿ç”¨ã€‚

**å…³äºæ¿€æ´»æ˜¾å­˜çš„å¤„ç†**

æ¿€æ´»å¦‚æœåªç”¨ä½œæ¨è¿› forwardï¼Œå¯ä»¥éšç”¨éšä¸¢ã€‚ä½†æ˜¯å¾€å¾€åå‘ä¼ æ’­æ—¶ä¹Ÿä¼šç”¨åˆ°å®ƒã€‚backward ç”¨åˆ°å®ƒçš„æ—¶å€™ï¼Œé€‚ç”¨äº Wgrad = $X^{\top} \nabla y$ è®¡ç®—çš„æ—¶å€™ï¼Œè€Œæ­¤æ—¶åªéœ€è¦å®ƒä»¥ FP8 çš„å½¢æ€å‡ºç°ã€‚æ‰€ä»¥æ¿€æ´»æŒ‰è¯´éƒ½å¯ä»¥å­˜ä¸º FP8 æ ¼å¼ã€‚ä½†æœ‰ä¸ªä¾‹å¤–ï¼šattn ä¹‹åçš„ proj çš„ input æ˜¯ï¼šattnRes=softmax(QK')V'ï¼Œå³å®ƒå‚ä¸ attn_layer_final_out = $attnRes \cdot W_o$ è®¡ç®—ã€‚ä½†æ˜¯backward æ—¶å®ƒä¼šä¼ ç»™ attnï¼Œè€Œè¿™é‡Œéœ€è¦é«˜ç²¾åº¦ã€‚æ‰€ä»¥å¯¹æ­¤æ¿€æ´» attnResï¼Œéœ€è¦ä¿ç•™è¾ƒé«˜ç²¾åº¦ 

å¦å¤–å¯¹äº $SwiGLU(X)=(X W_1â€‹)âŠ™Ïƒ(X W_2â€‹)$ï¼Œä¼šä¸å­˜ $X W_1$ ä¸ $X W_2$ï¼Œè€Œæ˜¯åœ¨backward æ—¶ï¼Œé‡è®¡ç®—ã€‚

MOE å±‚ï¼šå¯¹å•ä¸ªä¸“å®¶ FFNï¼Œä¸€å…¥å£å°±æ˜¯linearï¼Œå®ƒçš„ input è¦æ±‚æ˜¯ fp8ï¼Œæ‰€ä»¥ dispatch åˆ° MOE input å¯ä»¥æ˜¯ fp8 çš„ã€‚è€Œ backward æ—¶ï¼Œä¼ å…¥ MOE çš„æ¿€æ´»æ¢¯åº¦ï¼Œä¹Ÿæ˜¯ä¸€æ · FP8 å³å¯ã€‚ä½†å¯¹äº MOE çš„è¾“å‡ºï¼Œè¦ä¿æŒ fp16.
> For both the forward and backward combine components, we retain them in BF16 to preserve training precision in critical parts of the training pipeline.

### äºŒã€å®é™…æ“ä½œä¸­çš„ç»†èŠ‚

#### **(1) outlier æ€ä¹ˆè§£å†³**

<img width="712" height="420" alt="image" src="https://github.com/user-attachments/assets/1bdfbe04-0125-4ec1-9c15-998e7d1cf756" />

å¦‚å›¾, å‡è®¾æ˜¯è¦ç®— $XW = \[ x_1, x_2, .., x_n\] \cdot \[W_1, W_2, .., W_n\]^T$ï¼ˆä¸”æœ¬æ¥ä¹Ÿä¸éœ€è¦åˆ†å—ç®—ï¼‰ã€‚

å¦‚æœ X æˆ–è€… W æœ‰ä¸€ä¸¤ä¸ª outlierï¼Œåš tensor-wise scale ä¼šå¯¼è‡´ç»å¤§éƒ¨åˆ†å€¼å½’é›¶ã€‚äºæ˜¯é‡‡ç”¨çš„æ–¹æ³•æ˜¯ï¼ŒçŸ©é˜µåˆ†å—ï¼šå¯¹äº Xï¼ŒæŒ‰ 1 x 128 åˆ†ï¼Œå¯¹äºæƒé‡ Wï¼ŒæŒ‰ 128x128 åˆ†ï¼Œæ¯ä¸€å—åˆ†åˆ« scaleï¼š

<img width="860" height="710" alt="image" src="https://github.com/user-attachments/assets/e7247d0d-4c00-4534-b5e8-07f353058060" />

å‡è®¾æ¯ä¸ªåˆ†å—çš„ scaling æ˜¯ $x_i = \lambda_i a_i$, $W_i = \gamma_i B_i$ï¼Œåˆ™æœ¬æ¥ $XW = \sum_i x_i W_i$, ç°åœ¨å°±å˜æˆäº† $XW = \sum_i \[(\lambda_i \gamma_i) (a_i\cdot B_i)\]$ã€‚

**å…³äº block size çš„é€‰å–ï¼š**

è§ä¸Šæ–‡å¯è§ï¼Œä¸€å…±ç‰µæ‰¯åˆ°ä¸‰ä¸ª GEMMï¼ˆXWï¼Œ $\nabla y W^T$, $X^T\nabla y$)ï¼Œå‚ä¸çš„çŸ©é˜µæœ‰
- X=ä¸Šä¸€å±‚çš„activationï¼ŒåŒæ—¶æ˜¯æœ¬å±‚çš„input
  - block å¤§å°æ˜¯ $1 \times N_c = 1 \times 128$ï¼Œæ„æˆäº† tile-wise
- $\nabla y$=ä¸‹ä¸€å±‚ä¼ æ¥çš„æ¿€æ´»æ¢¯åº¦(å³ Dgrad)ã€‚
  - block å¤§å°æ˜¯ $1 \times N_c = 1 \times 128$
- W=weight
  - block å¤§å°æ˜¯ $N_c \times N_c = 128 \times 128$

éƒ½æ˜¯çŸ©é˜µï¼Œä¸ºä»€ä¹ˆä¸ç»Ÿä¸€æŒ‰ 128x128å‘¢ï¼Ÿpaperä¸­å®éªŒç»“æœæ˜¯å°±åº”è¯¥ä¸åŒå¤„ç†ã€‚paper æ¨æµ‹ï¼Œå¯¹ Dgrad æ¥è¯´ï¼šä¸åŒ token çš„ Dgrad å·®å¼‚è¾ƒå¤§ï¼Œæ‰€ä»¥ outlier ä¸tokenç›¸å…³å§ï¼Œå› æ­¤è¦ä¸åŒ token ä¸èƒ½åœ¨åŒä¸€ä¸ª block å†…ã€‚å¦‚ä¸Šå¯¼è‡´ $X^T\nabla y$ æ˜¯ [128x1] åˆ†å—å’Œ [1x128] åˆ†å—çš„ä¸¤ä¸ªçŸ©é˜µç›¸ä¹˜ã€‚

#### **(2) çŸ©é˜µä¹˜ç´¯åŠ ç²¾åº¦**

**é—®é¢˜äº§ç”Ÿçš„åŸå› ï¼š**

å¯¹ FP8 çŸ©é˜µ A ä¸ Bï¼ŒçŸ©é˜µä¹˜ç§¯ AB çš„å•ä¸ªå…ƒç´  $AB_{ij} = \sum_{k=1}^{K} a_{ik} b_{kj}$ï¼Œç´¯åŠ é¡¹ä¸­çš„å•é¡¹ $a_{ik} \cdot b_{kj}$ éœ€è¦æ¯” FP8 æ›´é«˜çš„ç²¾åº¦ï¼›é‰´äºæ˜¯å¤šé¡¹æ±‚å’Œï¼Œé‚£ä¹ˆå…¶å®éœ€è¦æ¯”å•é¡¹æ›´é«˜çš„ç²¾åº¦ã€‚åœ¨ GPU ç¡¬ä»¶å†…éƒ¨ï¼Œæµ®ç‚¹æ•°å…¶å®ä¸ç”¨ FP16ï¼ŒFP32 è¿™æ ·çš„æ ¼å¼ï¼Œè€Œæ˜¯å†…éƒ¨ç§æœ‰æ ¼å¼ã€‚å°±è‹±ä¼Ÿè¾¾ H800 GPU è€Œè¨€ï¼Œä½œ FP8 çŸ©é˜µä¹˜æ³•çš„æ—¶å€™ï¼Œ $\sum_{k=1}^{K} a_{ik} b_{kj}$ ç´¯ç§¯æ‰€ç”¨çš„ç´¯åŠ å™¨çš„ç²¾åº¦åªæœ‰ 14 bitsï¼Œå¹¶æ²¡è¾¾åˆ° FP32 çš„ç²¾åº¦ã€‚è¿™æ ·ç´¯åŠ è¿‡ç¨‹ä¸­å°±æœ‰èˆå…¥è¯¯å·®ã€‚æŒ‰å…¶æ–‡ï¼ŒK=4096 åˆ™å¯èƒ½æœ‰ 2% çš„è¯¯å·®ã€‚äºæ˜¯ deepseek-v3 çš„è§£æ³•æ˜¯ï¼Œéœ€è¦æ›´é«˜çš„ç´¯åŠ ç²¾åº¦ã€‚

- FP32 ä½œä¸ºç´¯åŠ å™¨ï¼šFP32 æœ‰æ•ˆæ•°å­—ï¼ˆmantissaï¼‰ = 23 + 1(hidden) = 24 bits â†’ åè¿›åˆ¶å¤§çº¦èƒ½ä¿è¯ 7â€“8 ä½æœ‰æ•ˆæ•°å­—
- H800 FP8 GEMM ç´¯åŠ å™¨ï¼š14 bits â†’ åè¿›åˆ¶å¤§çº¦åªèƒ½ä¿è¯ 4â€“5 ä½æœ‰æ•ˆæ•°å­—

å¦‚ä¸Šï¼Œè‹¥ç”¨ 14bitsï¼Œè¶…å‡º14 bit éƒ¨åˆ†ä¼šæœ‰èˆå…¥è¯¯å·®ï¼Œå¦‚æœç´¯åŠ é¡¹è¾ƒå°ï¼Œåˆ™ç›´æ¥è¢«ä¸¢å¼ƒã€‚ä»è€Œæœ‰ç»“æœç²¾åº¦é—®é¢˜ã€‚

**ç»“åˆç¡¬ä»¶å†é˜è¿°ä¸‹**

ç”¨è‹±ä¼Ÿè¾¾ GPU è®¡ç®—å¤§çŸ©é˜µä¹˜æ³•çš„æ—¶å€™ï¼Œä½ æ˜¯é€šè¿‡ CUDA è¿™æ ·çš„ä¸œè¥¿è¿›è¡Œçš„ã€‚ä½ æŠŠä¸¤ä¸ªå¤§çŸ©é˜µä¼ ç»™äº†å®ƒï¼Œä½†æ˜¯ GPU æ ¸å¿ƒå…¶å®å¹¶ä¸æ”¯æŒé‚£ä¹ˆå¤§çš„çŸ©é˜µç›¸ä¹˜ã€‚æ‰€ä»¥æ˜¯ CUDA å†…éƒ¨åšäº†åˆ†å—çŸ©é˜µä¹˜æ³•ï¼Œé€šè¿‡ä»£ç å±‚å¾ªç¯æ–¹å¼ã€‚

ä¹Ÿå°±æ˜¯è¯´å¯¹äº A*Bï¼Œå®ƒå¤§çº¦æ˜¯è¿™æ ·åšçš„ï¼šå‡è®¾çŸ©é˜µåˆ†å—ç»´ A=[A1,A2, ..An], B=[B1,B2,.., Bn]áµ€, $A \cdot B=\sum_i A_i \cdot B_i$ï¼Œæ‰§è¡Œæ¯ä¸ª $A_i \cdot B_i$ æ˜¯ç›´æ¥è°ƒç”¨çš„ GPUï¼Œè€Œ $\sum_i$ å¾ªç¯æ˜¯ä»£ç å±‚åšçš„ã€‚ä»£ç å±‚ä¼šæ˜¯å¾ªç¯æ‰§è¡Œ $C_{i+1} = A_i \cdot B_i + C_i$ "ä¹˜&åŠ " çš„å½¢å¼ã€‚è¿™ä¸ª "ä¹˜&åŠ " å¯ç”± gpu ä¸€æ¬¡å®Œæˆã€‚

NVIDIA GPU ç¡¬ä»¶å±‚é¢æä¾›çš„çŸ©é˜µä¹˜æ³•æŒ‡ä»¤ï¼ˆåªè®°ä¸‹åŠ©ç†è§£ï¼‰ï¼š

- FMA (Fused Multiply-Add)ï¼šæ ‡é‡çº§å•ç‚¹è®¡ç®—ï¼Œå¦‚æœä½œçŸ©é˜µä¹˜æ³•ï¼Œéœ€è¦è‡ªå·±å†™æ‰€æœ‰å¾ªç¯ã€‚
- MMA (Matrix Multiply-Accumulate)
  - V100å¼•å…¥ï¼ŒæŒ‡ä»¤æ—: mma.sync.* ï¼Œwarp çº§åˆ«çš„çŸ©é˜µä¹˜åŠ æŒ‡ä»¤ã€‚
  - å¯è®¡ç®— shape=[16, 16] x shape=[16, 16] å¤§å°çš„çŸ©é˜µä¹˜
- WGMMA (Warp Group MMA)
  - H100/H800ï¼ŒæŒ‡ä»¤æ—ï¼šwgmma.mma_async.* ï¼Œ**å¼‚æ­¥**ï¼ˆä»æŒ‡ä»¤åå¯çœ‹å‡ºï¼‰æ‰§è¡Œï¼Œå…è®¸æµæ°´çº¿åŒ–ã€‚
  - å¯è®¡ç®— shape=[m=64, k=16] x [k=16, n=128] æˆ– [64, 32] x [32, 128] ç­‰å¤§å°çš„çŸ©é˜µä¹˜
  ```
  function GEMM(A[M,K], B[K,N]) -> C[M,N]:
    for bm in 0..M step 64:         # æŒ‰ 64 è¡Œåˆ†å—
        for bn in 0..N step 128:    # æŒ‰ 128 åˆ—åˆ†å—
            acc = zeros(64,128)     # ç´¯åŠ å™¨å¯„å­˜å™¨

            for k0 in 0..K step 16: # æ²¿ K ç»´åˆ†å—
                A_tile = A[bm:bm+64, k0:k0+16]
                B_tile = B[k0:k0+16, bn:bn+128]

                acc = WGMMA(acc, A_tile, B_tile) # ä¹ƒä¸€æ¡ GPU instructionï¼Œä¸€æ¬¡ 64Ã—128Ã—16 ä¹˜åŠ ã€‚å¯¹äº MMAï¼Œä¼ªä»£ç å’Œè¿™ä¸ªä¸€æ ·ï¼Œåªéœ€ WGMMA=> MMA, å¹¶ç¼©å° tile å¤§å°ã€‚
                # ä¸Šé¢ WGMMA(acc, A_tile, B_tile) = A_tile*B_tile + acc, å¯¹ FP8 gemm A_tile*B_tile æ˜¯ 14bitç²¾åº¦ï¼Œè€Œ æ›´é«˜ç²¾åº¦çš„ acc ä¼šè½¬æˆ 14bit ç²¾åº¦ï¼Œç„¶åäºŒè€…æ±‚å’Œï¼Œ
                #     14bit ç²¾åº¦çš„ç»“æœï¼Œä¼šé‡æ–° copy åˆ° accã€‚è¿™æ ·å¯¼è‡´çš„ fp8 GEMM ç²¾åº¦é—®é¢˜
            C[bm:bm+64, bn:bn+128] = acc
  ```

çœ‹ä¸Šé¢ä¼ªä»£ç å¯çŸ¥ deepseek-v3 æ‰€è¯´çš„ fp8 gemm å†…éƒ¨çš„ WGMMA 14bit ç´¯åŠ ç²¾åº¦æ€ä¹ˆäº§ç”Ÿçš„ã€‚deepseek çš„ç ´è§£æ³•æ˜¯ï¼ŒWGMMA å¯¹ acc çš„ç´¯åŠ æ¯å‘ç”Ÿè‹¥å¹²æ¬¡ï¼Œå°±æŠŠè¯¥ç»“æœå¦å¤–é«˜ç²¾åº¦ç´¯åŠ ï¼Œè€Œ WGMMA çš„ acc å‚æ•°æ¸…é›¶ã€‚ä¹Ÿå°±æ˜¯è¿™æ ·ï¼š

```
  function GEMM_new(A[M,K], B[K,N]) -> C[M,N]:
    for bm in 0..M step 64:         # æŒ‰ 64 è¡Œåˆ†å—
        for bn in 0..N step 128:    # æŒ‰ 128 åˆ—åˆ†å—
            acc = zeros(..)     # ç´¯åŠ å™¨å¯„å­˜å™¨
            acc_hi = 0
            pending = 0
            for k0 in 0..K step 16: # æ²¿ K ç»´åˆ†å—
                A_tile = A[bm:bm+64, k0:k0+16]
                B_tile = B[k0:k0+16, bn:bn+128]

                acc = WGMMA(acc, A_tile, B_tile) # WGMMA å†…éƒ¨åªæ˜¯å±€éƒ¨ç´¯åŠ ã€‚
                pending += 1
                if pending == 4: # wgmma ç´¯ç§¯4æ¬¡ï¼Œåˆ™åšä¸€æ¬¡ acc_hi ç´¯åŠ ã€‚4æ¬¡æ­£å¥½å¯¹åº”çŸ©é˜µçš„ 128 ä¸ª a_i*b_i çš„ç´¯åŠ ï¼Œå’Œå®ƒçš„ block-wise scaling ç²’åº¦èƒ½å¯¹é½
                    acc_hi += acc # ä¸åœ¨ WGMMA å†…éƒ¨ä½œå…¨å±€ç´¯åŠ 
                    acc = 0
                    pending = 0 
            C[bm:bm+64, bn:bn+128] = acc_hi
  ```

paper ä¸­ç¤ºæ„å›¾å¦‚ä¸‹ã€å®ƒ 4 ä¸ª wgmma å¯¹åº” 128ä¸ª ç´¯åŠ ï¼Œé‚£ä¹ˆæ¯ä¸ª wgmma ç›¸ä¹˜å°çŸ©é˜µçš„ K=32, çœ‹ https://zhuanlan.zhihu.com/p/32383172703 ï¼Œå…¶ä»£ç ä¸­åº”è¯¥æ˜¯ç”¨äº† wgmma.mma_async.sync.aligned.m64n128k32ï¼Œ ä¹Ÿå°±æ˜¯ [m=64, k=32] x [k=32, n=128] å¤§å°çš„çŸ©é˜µä¹˜æ³•ã€‚ä½†æ˜¯æœ€æ–°çš„ deepGEEM ä»£ç ä¸­æœä¸åˆ° m64n128k32 äº†ã€‘ï¼š

<img width="1006" height="884" alt="image" src="https://github.com/user-attachments/assets/38d846b6-4aac-435d-92aa-35f979be8db7" />

å¦å¤–paper è¯´ï¼ŒH800 ä¸Šä¸€æ¬¡å¯ä»¥å¹¶è¡Œä¸¤ä¸ª WGMMAï¼Œäºæ˜¯å¯ä»¥ä¸€ä¸ªåšç²¾åº¦æå‡ç´¯åŠ ï¼ŒåŒæ—¶å¦ä¸€ä¸ªåš MMAï¼Œä»è€Œå¾ˆé«˜æ•ˆã€‚
> However, on the H800 architecture, it is typical for two WGMMA to persist concurrently: while one warpgroup
performs the promotion operation, the other is able to execute the MMA operation. This design
enables overlapping of the two operations, maintaining high utilization of Tensor Cores.

#### ï¼ˆ3ï¼‰ã€å…¶ä»–

**åªç”¨ä¸€ç§ fp8æ ¼å¼**

ä¸€èˆ¬å»ºè®® fp8 E4M3 ç”¨äº Fprop ï¼Œfp8 E5M2 ç”¨äº Dgrad and Wgradã€‚æœ‰äº†è¿™æ ·çš„ç»†ç²’åº¦çš„ scale æ§åˆ¶åï¼Œdeepseek åªç”¨äº†å°±åªç”¨ E4M3 è¿™ä¸€ç§ fp8 æ ¼å¼äº†ã€‚

**åœ¨çº¿ fp8 é‡åŒ–**

ä¸€èˆ¬ç”¨ fp8 æ—¶ï¼Œscale å› å­æ˜¯ä»å†å²ç»Ÿè®¡å‡ºçš„ï¼ˆæ»‘åŠ¨å¹³å‡ï¼Œæˆ–è€…å–æœ€è¿‘ n ä¸ªçš„ max ä¹‹ç±»ï¼‰ã€‚è€Œ deepseek çš„ fp8 ä»å½“å‰çš„ 1x128 æˆ–è€… 128x128 block ä¸­å½“åœºç®—å‡º scale å› å­ã€‚
