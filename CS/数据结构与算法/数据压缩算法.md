数据压缩算法, 无怪乎去重复. 一种是 Huffman 方式, 对常见符号用更短的bit 来表示. 还有一种方式是找重复,对于第二、第三次重复用更短的标记表示 (LZ77). 或许还有其他的方法. 这里只是学习下, 就只关注下 LZ77.

LZ77 + Huffman 联合, 就是所谓的 DEFLATE 算法. 

参考本文同目录下的 compress.py ( https://github.com/superzhangmch/learning_notes/blob/main/CS/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/compressor.py )

就 LZ77 而言, 它的方法是: 从左往右扫描原始 content, 在当前扫描位置, 在前面的一个 32k 大小的窗口内, 寻找一个最长的子串(限定不能超 258 字节), 使得当前位置开始的字符串和它一样. 如果找到, 就可以用两个数字来表示这段重复了: [回退多少字节d, 持续多长L] 的字符串和当前位置的L长字符串相等, 从而达到了压缩效果. (代码: `def lz77_compress(...):` )

<img width="592" height="195" alt="image" src="https://github.com/user-attachments/assets/ebc834c1-b171-4fa9-b910-7695b7737d26" />

注意: 
- 理论需要暴力搜索找最长重复: 这可以用 hash 法来加速(直接定位到可能的匹配, 跳过无效的循环检查)
- 检测到重复后, 需要能用很短字节来指出重复位置: 这需要对重复位置与长度这两个数字作压缩表示. 大体思路是用huffman 的方式, 更短更近更有可能重复, 对这些数字应该用更短编码. (代码: LENGTH_CODES DISTANCE_CODES)

一个需要注意的点: 这个压缩算法是流式的, 扫描到当前位置看窗口内的内容时仍然看的原始 input, 而非 output 的前序window. 这样就不用管匹配到的重复子串本身又匹配到什么了.

经此之后, 可以再加一个独立 huffman. 实测起作用的大头在于 LZ77.

