
### „Äädeepseek-v3„ÄãpaperÔºö3.3ËäÇ FP8 Training

> Inspired by recent advances in low-precision trainingÔºàËßÅ‰∏ãÈù¢‰∏â‰∏™ÂºïÊñáÔºâ, we propose a fine-grained mixed precision framework utilizing the FP8
data format for training DeepSeek-V3.
>
> „ÄêÂèóÂà∞Âà´ÁöÑ fp8 ÊñπÊ°àÂêØÂèë„Äë
- „ÄäGpt3. int8 (): 8-bit matrix multiplication for transformers at scale„Äã- 2022.08 - https://arxiv.org/abs/2208.07339
- „Ää8-bit numerical formats for deep neural networks„Äã- 2022.06 - https://arxiv.org/abs/2206.02915
- „ÄäFP8-LM: Training FP8 large language models„Äã - 2023.10 - https://arxiv.org/abs/2310.18313

> While low-precision training holds great promise, it is often limited by the presence of outliers in activations, weights, and gradientsÔºàËßÅ‰∏ãÈù¢‰∏§‰∏™ÂºïÊñáÔºâ.
>
> „Äê‰ΩÜÊòØÂΩìÂâçÁöÑ fp8 ÊÄªÊòØÂèóÂõ∞‰∫é outlier ÈóÆÈ¢ò„Äë
- „ÄäScaling FP8 training to trillion-token llms„Äã- 2024.09 - https://arxiv.org/abs/2409.12517
- „ÄäMassive activations in large language models„Äã - 2024.02 - https://arxiv.org/abs/2402.17762

> Although significant progress has been made in inference quantization (ËßÅ‰∏ãÈù¢‰∏§‰∏™ÂºïÊñá A ‰∏é B), there are relatively few studies demonstrating successful application of low-precision techniques in large-scale language model pre-training (ËßÅ‰∏ãÈù¢ÂºïÊñá C).
>
> „ÄêÊé®ÁêÜÈáèÂåñÊúâËøõÂ±ïÔºå‰ΩÜÊòØÂ§ßËßÑÊ®°‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉÁöÑÊàêÂäüËøòÊú™ËßÅ„Äë
- AÔºö„ÄäGptq: Accurate post-training quantization for generative pre-trained transformers„Äã- 2022.10 - https://arxiv.org/abs/2210.17323
- BÔºö„ÄäSmoothquant: Accurate and efficient post-training quantization for large language models„Äã - 2022.11 - https://arxiv.org/abs/2211.10438
- CÔºö„ÄäScaling FP8 training to trillion-token llms„Äã- 2024.09 - ÔºàÊ≥®ÊÑè‰∏äÈù¢‰πüÂá∫Áé∞‰∫ÜÔºâ https://arxiv.org/abs/2409.12517

> To address this challenge and effectively extend the dynamic range of the FP8 format,
>
> Ôºà1Ôºâ„ÄÅwe introduce a fine-grained quantization strategy: tile-wise grouping with 1 √ó ùëÅùëê elements or block-wise grouping with ùëÅùëê √ó ùëÅùëê elements. The associated dequantization overhead is largely mitigated under our increased-precision accumulation process, a critical aspect for achieving accurate FP8 General Matrix Multiplication (GEMM).
>
> Ôºà2Ôºâ„ÄÅMoreover, to further reduce memory and communication overhead in MoE training, we cache and dispatch activations in FP8, while storing low-precision optimizer states in BF16.
>
> Ôºà3Ôºâ„ÄÅWe validate the proposed FP8 mixed precision framework on two model scales similar to DeepSeek-V2-Lite and DeepSeekV2, training for approximately 1 trillion tokens (see more details in Appendix B.1). Notably, compared with the BF16 baseline, the relative loss error of our FP8-training model remains consistently below 0.25%, a level well within the acceptable range of training randomness.
