sd 1.5 以及其他早期 text2image model，生成的文字，往往只是乍一看是那么回事，近看都是西夏文。而近来的model，好像就都挺好的了。那么是什么导致生成变好的呢? 下面是 chatgpt deep-research 结果。只供参考。如果有什么特别本质原因，那么我猜不会漏。鉴于其所言，我觉得在于用于训练的数据质量的提高，以及模型参数变多，是最主要原因。

下面 
author: chatgpt
date: 2025.04.02

# 近年AI文生图模型文字生成质量显著提升的原因分析

## 引言：从“乱码”文本到清晰字符

早期的文本到图像（文生图）生成模型虽然能绘制精美画面，却常常无法正确生成图片中的文字内容。例如，OpenAI 的 DALL·E 2 或 Stable Diffusion 1.5 等模型经常在图像中产出类似文字的“乱码”图案，没有实际含义 ([Why do DallE/SD produce beautiful images but garbled text?](https://artificial-intuition.beehiiv.com/p/dalle-diffusion-produce-beautiful-images-garbled-text#:~:text=understandable%20language,Still%2C%20the%20image%20diffusion))。这些模型仅把文字当作视觉纹理来学习，并没有真正**理解**或**拼写**文字的机制，因此对提示中的具体单词往往**忽略或混淆** ([openai DALL-E 3 论文 提升图像生成的关键：更好的图像描述_dall-e3论文解读-CSDN博客](https://blog.csdn.net/ryo1060732496/article/details/136208154#:~:text=%E6%88%91%E4%BB%AC%E5%B1%95%E7%A4%BA%E4%BA%86%E9%80%9A%E8%BF%87%E8%AE%AD%E7%BB%83%E9%AB%98%E5%BA%A6%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%9A%84%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E6%A0%87%E9%A2%98%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%98%BE%E7%9D%80%E6%94%B9%E5%96%84%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8F%90%E7%A4%BA%E8%B7%9F%E9%9A%8F%E8%83%BD%E5%8A%9B%E3%80%82))。然而，近年来新一代模型（如 DALL·E 3、Midjourney V6、Stable Diffusion XL 等）已经大幅改善了这一问题，在图像中生成清晰、连贯的文字成为可能 ([Midjourney V6 Takes Major Step Forward with Photorealism and In-Image Text | No Film School](https://nofilmschool.com/midjourney-v6#:~:text=As%20we%E2%80%99ve%20covered%20with%20other,but%20even%20the%20letters%20themselves)) ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=Example%20images%20generated%20by%20SDXL,character%20definitions%20and%20fewer%20distortions))。本文将深入分析这一质的飞跃背后的关键技术与方法，并通过主流模型实例说明如何实现更精确的文字生成与对齐，最后对比新旧模型在文字生成方面的差异及新技术如何克服早期缺陷。

## 文字生成质量提升的关键技术与方法

近年来，研究者和开发者在多方面取得突破，使AI绘制的图像中文字从**模糊不清**走向**清晰可读**。主要的技术改进包括：

- **大规模且标注精细的训练数据**：模型文字能力的提升首先得益于训练数据中**文字信息的丰富与准确**。过去，很多训练图像的标签（caption）并未详细描述图中出现的文字，导致模型难以将图像中的字符形状与对应的文本关联起来 ([openai DALL-E 3 论文 提升图像生成的关键：更好的图像描述_dall-e3论文解读-CSDN博客](https://blog.csdn.net/ryo1060732496/article/details/136208154#:~:text=%E6%88%91%E4%BB%AC%E5%B1%95%E7%A4%BA%E4%BA%86%E9%80%9A%E8%BF%87%E8%AE%AD%E7%BB%83%E9%AB%98%E5%BA%A6%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%9A%84%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E6%A0%87%E9%A2%98%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%98%BE%E7%9D%80%E6%94%B9%E5%96%84%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8F%90%E7%A4%BA%E8%B7%9F%E9%9A%8F%E8%83%BD%E5%8A%9B%E3%80%82))。近期的工作通过**改进图像描述（caption）质量**来解决这一瓶颈。例如，OpenAI 的最新研究先训练了一个强大的图像描述生成器，为训练集中的图像生成**更详细准确的描述**，然后用这些**合成的详尽描述**来训练文本到图像模型 ([openai DALL-E 3 论文 提升图像生成的关键：更好的图像描述_dall-e3论文解读-CSDN博客](https://blog.csdn.net/ryo1060732496/article/details/136208154#:~:text=%E6%88%91%E4%BB%AC%E5%B1%95%E7%A4%BA%E4%BA%86%E9%80%9A%E8%BF%87%E8%AE%AD%E7%BB%83%E9%AB%98%E5%BA%A6%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%9A%84%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E6%A0%87%E9%A2%98%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%98%BE%E7%9D%80%E6%94%B9%E5%96%84%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8F%90%E7%A4%BA%E8%B7%9F%E9%9A%8F%E8%83%BD%E5%8A%9B%E3%80%82))。这样一来，模型学到了图像中文字与文本描述之间更直接的对应关系，大幅提高了生成图像时**遵循提示中文字要求**的能力 ([openai DALL-E 3 论文 提升图像生成的关键：更好的图像描述_dall-e3论文解读-CSDN博客](https://blog.csdn.net/ryo1060732496/article/details/136208154#:~:text=%E6%88%91%E4%BB%AC%E5%B1%95%E7%A4%BA%E4%BA%86%E9%80%9A%E8%BF%87%E8%AE%AD%E7%BB%83%E9%AB%98%E5%BA%A6%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%9A%84%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E6%A0%87%E9%A2%98%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%98%BE%E7%9D%80%E6%94%B9%E5%96%84%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8F%90%E7%A4%BA%E8%B7%9F%E9%9A%8F%E8%83%BD%E5%8A%9B%E3%80%82))。DALL·E 3 正是使用了这种高质量描述数据，从而在遵循提示（包括其中要求的文字内容）方面相比前代有显著优势 ([OpenAI & Microsoft’s DALL-E 3 Masters Image Creation Through Enhanced Captions | Synced](https://syncedreview.com/2023/10/23/openai-microsofts-dall-e-3-masters-image-creation-through-enhanced-captions/#:~:text=The%20research%20team%20posits%20that,to%20address%20the%20issue%20comprehensively)) ([OpenAI & Microsoft’s DALL-E 3 Masters Image Creation Through Enhanced Captions | Synced](https://syncedreview.com/2023/10/23/openai-microsofts-dall-e-3-masters-image-creation-through-enhanced-captions/#:~:text=The%20resultant%20DALL,code%20for%20these%20evaluations%2C%20thereby))。

- **模型架构与规模的升级**：更强大的模型架构也提升了文字生成效果。首先，**模型参数量与分辨率**的增加使其能表达更细微的图像细节，包括文字的细节。例如，Stable Diffusion XL (SDXL) 将核心U-Net模型参数从1.3亿提升到**35亿**，生成分辨率提升至1024×1024，大幅增强了模型“辨认和重现精细视觉细节”的能力 ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=By%20expanding%20the%20UNet%20backbone,text%20legibility%20and%20other%20capabilities))。额外的模型容量为文字这样的高频细节提供了“思考空间”，因此SDXL能够在图像中绘制更清晰的文字 ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=%E2%80%93%20enabling%20it%20to%20discern,text%20legibility%20and%20other%20capabilities))。其次，**多阶段生成架构**发挥了作用。SDXL引入了“**专家组合**”架构：先由基本模型生成初步图像，再由精细化模型（Refiner）对细节进行完善 ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=In%20SDXL%E2%80%99s%20case%2C%20there%20is,conquer%20the%20image%20synthesis%20task))。这种两级生成让模型有机会**修正初始阶段遗留的文字缺陷**，输出更高保真度的结果 ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=1,model%2C%20producing%20higher%20fidelity%20outputs))。类似地，级联扩散模型（如 Stability AI 的 DeepFloyd IF）通过逐步放大图像，在高分辨率阶段重点渲染文字轮廓，显著提高了最终文字的清晰度。

- **引入文字感知模块或机制**：最新的模型架构开始**专门针对图像文字设计子模块**或训练目标，使模型“意识到”自己在生成文字。一个典型思路是在扩散模型中加入**文字专用分支**或**额外的文字编码器**。例如，Stable Diffusion XL 使用了**双文本编码器**机制，利用两个不同的文本编码器解析提示，从而提供更加丰富的文本条件 ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=For%20legibility%2C%20SDXL%E2%80%99s%20two%20separate,based%20details%20with%20higher%20accuracy))。据报道，这种“双目”视角帮助 SDXL 更准确地在图像中绘制包含文字的元素，如标志、招牌等 ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=For%20legibility%2C%20SDXL%E2%80%99s%20two%20separate,based%20details%20with%20higher%20accuracy))。更近期的模型（如稳定性AI内部的 Stable Diffusion 3 Large 等）采用了**多模态扩散Transformer (MMDiT)** 架构，**为图像和文字各自设计独立的子模型权重**，但允许二者信息交互 ([Stability AI 三大图像生成模型在 Bedrock 正式可用](https://aws.amazon.com/cn/campaigns/selected-blog-stability-ai-available/#:~:text=%E7%9B%B8%E6%AF%94%20Stable%20Diffusion%20XL%20,Large%20%E7%9A%84%E4%B8%BB%E8%A6%81%E6%94%B9%E8%BF%9B%E4%B9%8B%E4%B8%80%E6%98%AF%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E4%B8%AD%E7%9A%84%E6%96%87%E5%AD%97%E8%B4%A8%E9%87%8F%E3%80%82%E5%BE%97%E7%9B%8A%E4%BA%8E%E5%88%9B%E6%96%B0%E7%9A%84%20Diffusion%20Transformer%20%E6%9E%B6%E6%9E%84%EF%BC%8C%E6%96%B0%E6%A8%A1%E5%9E%8B%E5%87%8F%E5%B0%91%E4%BA%86%E6%8B%BC%E5%86%99%E5%92%8C%E6%8E%92%E7%89%88%E9%94%99%E8%AF%AF%E3%80%82%E8%AF%A5%E6%9E%B6%E6%9E%84%E4%B8%BA%E5%9B%BE%E5%83%8F%E5%92%8C%E6%96%87%E5%AD%97%E5%88%86%E5%88%AB%E8%AE%BE%E8%AE%A1%E4%BA%86%E4%B8%A4%E5%A5%97%E7%8B%AC%E7%AB%8B%E7%9A%84%E6%9D%83%E9%87%8D%EF%BC%8C%E4%BD%86%E5%85%81%E8%AE%B8%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BF%A1%E6%81%AF%E6%B5%81%E5%8A%A8%E3%80%82))。这种架构实质上给了模型一个“文字绘制专用的通道”，极大减少了拼写和排版错误 ([Stability AI 三大图像生成模型在 Bedrock 正式可用](https://aws.amazon.com/cn/campaigns/selected-blog-stability-ai-available/#:~:text=%E7%9B%B8%E6%AF%94%20Stable%20Diffusion%20XL%20,Large%20%E7%9A%84%E4%B8%BB%E8%A6%81%E6%94%B9%E8%BF%9B%E4%B9%8B%E4%B8%80%E6%98%AF%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E4%B8%AD%E7%9A%84%E6%96%87%E5%AD%97%E8%B4%A8%E9%87%8F%E3%80%82%E5%BE%97%E7%9B%8A%E4%BA%8E%E5%88%9B%E6%96%B0%E7%9A%84%20Diffusion%20Transformer%20%E6%9E%B6%E6%9E%84%EF%BC%8C%E6%96%B0%E6%A8%A1%E5%9E%8B%E5%87%8F%E5%B0%91%E4%BA%86%E6%8B%BC%E5%86%99%E5%92%8C%E6%8E%92%E7%89%88%E9%94%99%E8%AF%AF%E3%80%82%E8%AF%A5%E6%9E%B6%E6%9E%84%E4%B8%BA%E5%9B%BE%E5%83%8F%E5%92%8C%E6%96%87%E5%AD%97%E5%88%86%E5%88%AB%E8%AE%BE%E8%AE%A1%E4%BA%86%E4%B8%A4%E5%A5%97%E7%8B%AC%E7%AB%8B%E7%9A%84%E6%9D%83%E9%87%8D%EF%BC%8C%E4%BD%86%E5%85%81%E8%AE%B8%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BF%A1%E6%81%AF%E6%B5%81%E5%8A%A8%E3%80%82))。Aliyun 提供的Stable Diffusion 3.5系列同样在SDXL基础上引入了**多模态扩散Transformer和多文本编码器**，显著改善了复杂提示和文字排版的处理效果 ([如何快速通过百炼平台调用StableDiffusion服务_大模型服务平台百炼(Model Studio)-阿里云帮助中心](https://help.aliyun.com/zh/model-studio/stable-diffusion-quick-start/#:~:text=Stable%20Diffusion%203.5%E5%9C%A8%E4%B9%8B%E5%89%8D%E7%9A%841.5%E7%89%88%E6%9C%AC%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8A%E5%81%9A%E4%BA%86%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%94%B9%E8%BF%9B%EF%BC%8C%E9%87%87%E7%94%A8%E4%BA%86%E5%A4%9A%E6%A8%A1%E6%80%81%E6%89%A9%E6%95%A3%E5%8F%98%E5%8E%8B%E5%99%A8%EF%BC%88MMDiT%EF%BC%89%E6%9E%B6%E6%9E%84%EF%BC%8C%E7%BB%93%E5%90%88%E4%BA%86%E4%B8%89%E7%A7%8D%E5%9B%BA%E5%AE%9A%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%87%E6%9C%AC%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%8C%E5%B9%B6%E4%BD%BF%E7%94%A8Query,5%20Medium%EF%BC%8C%E6%AF%8F%E4%B8%AA%E7%89%88%E6%9C%AC%E9%83%BD%E9%92%88%E5%AF%B9%E4%B8%8D%E5%90%8C%E7%9A%84%E7%94%A8%E6%88%B7%E9%9C%80%E6%B1%82%E8%BF%9B%E8%A1%8C%E4%BA%86%E4%BC%98%E5%8C%96%EF%BC%8C%E6%8F%90%E4%BE%9B%E4%BA%86%E9%AB%98%E5%BA%A6%E7%9A%84%E5%AE%9A%E5%88%B6%E6%80%A7%E5%92%8C%E6%98%93%E7%94%A8%E6%80%A7%E3%80%82))。此外，学术界也探索了结合OCR(光学字符识别)模块来**辅助训练**的方案，在生成阶段用预训练OCR去解析模型绘制的文字并反馈误差，从而鼓励模型生成可被OCR识别的真实文字 ([[PDF] Layout-Agnostic Scene Text Image Synthesis with Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhangli_Layout-Agnostic_Scene_Text_Image_Synthesis_with_Diffusion_Models_CVPR_2024_paper.pdf#:~:text=%5BPDF%5D%20Layout,OCR%20loss%2C%20utilizing%20the%20predictions))。这些文字感知机制让模型不再仅把文字当作一般图像特征，而是作为需要正确重建的特殊目标。

- **跨模态对齐与优化目标**：多模态对齐指的是让模型的**文本表示和图像表示在共同空间中精细对应**。早期的扩散模型通常依赖CLIP文本编码来提供提示语义，但CLIP的目标是对整体图像语义打分，**并未逐字对齐**文字信息。近期改进在于：通过**更强的文本encoder**（如OpenAI的T5或更大的CLIP模型）获取更细粒度的文本嵌入，以及改进**跨注意力**机制，使每个词甚至每个字符更准确地影响对应图像区域。比如SDXL据称结合了三个不同来源的文本编码器嵌入并进行**Query-Key归一化**，提高了文本条件在扩散过程中的稳定性和对齐程度 ([如何快速通过百炼平台调用StableDiffusion服务_大模型服务平台百炼(Model Studio)-阿里云帮助中心](https://help.aliyun.com/zh/model-studio/stable-diffusion-quick-start/#:~:text=Stable%20Diffusion%203.5%E5%9C%A8%E4%B9%8B%E5%89%8D%E7%9A%841.5%E7%89%88%E6%9C%AC%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8A%E5%81%9A%E4%BA%86%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%94%B9%E8%BF%9B%EF%BC%8C%E9%87%87%E7%94%A8%E4%BA%86%E5%A4%9A%E6%A8%A1%E6%80%81%E6%89%A9%E6%95%A3%E5%8F%98%E5%8E%8B%E5%99%A8%EF%BC%88MMDiT%EF%BC%89%E6%9E%B6%E6%9E%84%EF%BC%8C%E7%BB%93%E5%90%88%E4%BA%86%E4%B8%89%E7%A7%8D%E5%9B%BA%E5%AE%9A%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%87%E6%9C%AC%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%8C%E5%B9%B6%E4%BD%BF%E7%94%A8Query,5%20Medium%EF%BC%8C%E6%AF%8F%E4%B8%AA%E7%89%88%E6%9C%AC%E9%83%BD%E9%92%88%E5%AF%B9%E4%B8%8D%E5%90%8C%E7%9A%84%E7%94%A8%E6%88%B7%E9%9C%80%E6%B1%82%E8%BF%9B%E8%A1%8C%E4%BA%86%E4%BC%98%E5%8C%96%EF%BC%8C%E6%8F%90%E4%BE%9B%E4%BA%86%E9%AB%98%E5%BA%A6%E7%9A%84%E5%AE%9A%E5%88%B6%E6%80%A7%E5%92%8C%E6%98%93%E7%94%A8%E6%80%A7%E3%80%82))。还有研究提出在训练目标中加入**字符级别感知损失**，例如要求生成图像在OCR模型的判别下与目标文字序列匹配，从而直接优化模型的拼写准确率 ([[PDF] Layout-Agnostic Scene Text Image Synthesis with Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhangli_Layout-Agnostic_Scene_Text_Image_Synthesis_with_Diffusion_Models_CVPR_2024_paper.pdf#:~:text=%5BPDF%5D%20Layout,OCR%20loss%2C%20utilizing%20the%20predictions))。这些对齐和目标函数上的改进，确保了模型在生成图像时更“忠实”于提示中的文字序列和含义。

综上，多方面的技术进步共同促成了文生图模型文字生成质量的飞跃：**更好的数据教会模型文字的含义**和**外观**，**更大的模型与新架构赋予模型重现文字细节的能力**，**文字专用模块和优化让模型真正学会拼写与排版**。下面，我们结合当前主流模型的实例，具体说明这些方法是如何落地实现的。

## 主流模型文字生成精度提升的实例

### OpenAI DALL·E 3：高质量描述训练与更强的遵循性

DALL·E 3 是 OpenAI 于2023年发布的新一代文生图模型，在文字内容生成方面有了长足进步。其关键在于**训练数据和训练策略的改进**。针对前代模型容易遗漏提示词的缺陷，OpenAI 提出了“**更好的图像描述**”方案： ([openai DALL-E 3 论文 提升图像生成的关键：更好的图像描述_dall-e3论文解读-CSDN博客](https://blog.csdn.net/ryo1060732496/article/details/136208154#:~:text=%E6%88%91%E4%BB%AC%E5%B1%95%E7%A4%BA%E4%BA%86%E9%80%9A%E8%BF%87%E8%AE%AD%E7%BB%83%E9%AB%98%E5%BA%A6%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%9A%84%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E6%A0%87%E9%A2%98%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%98%BE%E7%9D%80%E6%94%B9%E5%96%84%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8F%90%E7%A4%BA%E8%B7%9F%E9%9A%8F%E8%83%BD%E5%8A%9B%E3%80%82))首先训练一个模型为图片生成**简明而详尽**的描述，然后用这些增强的描述来训练DALL·E 3 的图像生成模型。换言之，DALL·E 3 的训练集中，图像配有高质量的说明性文本，包括图中出现的文字信息。这使模型能**更精确地将文字从语义映射到视觉**。正如OpenAI论文所指出的，使用详细生成的图像标题进行训练**可靠地提高了提示跟随能力** ([openai DALL-E 3 论文 提升图像生成的关键：更好的图像描述_dall-e3论文解读-CSDN博客](https://blog.csdn.net/ryo1060732496/article/details/136208154#:~:text=%E6%88%91%E4%BB%AC%E5%B1%95%E7%A4%BA%E4%BA%86%E9%80%9A%E8%BF%87%E8%AE%AD%E7%BB%83%E9%AB%98%E5%BA%A6%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%9A%84%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E6%A0%87%E9%A2%98%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%98%BE%E7%9D%80%E6%94%B9%E5%96%84%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8F%90%E7%A4%BA%E8%B7%9F%E9%9A%8F%E8%83%BD%E5%8A%9B%E3%80%82))。实际效果是，DALL·E 3 相比DALL·E 2更懂得**忠实地“照着提示画”**，包括其中指定的单词。  

在生成效果上，DALL·E 3 能够在很多情况下产出**真实存在的英文单词**，而非无意义的字符组合 ([DALL-E 3 Can Finally Draw Text. OpenAI’s newest image generator finally… | by David Petrofsky | Medium](https://medium.com/@david.petrofsky/dall-e-3-can-finally-draw-text-2f596db02c9b#:~:text=DALL,of%20a%20letter%20to%20Santa%3F%E2%80%9D)) ([DALL-E 3 Can Finally Draw Text. OpenAI’s newest image generator finally… | by David Petrofsky | Medium](https://medium.com/@david.petrofsky/dall-e-3-can-finally-draw-text-2f596db02c9b#:~:text=I%20don%E2%80%99t%20have%20the%20insider,don%E2%80%99t%20know%20the%20real%20numbers))。例如，用户要求“在霓虹招牌上显示‘Halloween Arcade’字样”，DALL·E 3 通常可以正确地拼出“Halloween”和“Arcade”这类单词（尽管有时对较长或不常见的词仍可能出错） ([DALL-E 3 Can Finally Draw Text. OpenAI’s newest image generator finally… | by David Petrofsky | Medium](https://medium.com/@david.petrofsky/dall-e-3-can-finally-draw-text-2f596db02c9b#:~:text=DALL,in%20claymation)) ([DALL-E 3 Can Finally Draw Text. OpenAI’s newest image generator finally… | by David Petrofsky | Medium](https://medium.com/@david.petrofsky/dall-e-3-can-finally-draw-text-2f596db02c9b#:~:text=I%20don%E2%80%99t%20have%20the%20insider,don%E2%80%99t%20know%20the%20real%20numbers))。这与DALL·E 2截然不同：后者几乎无法生成任何可辨识的英文单词 ([DALL-E 3 Can Finally Draw Text. OpenAI’s newest image generator finally… | by David Petrofsky | Medium](https://medium.com/@david.petrofsky/dall-e-3-can-finally-draw-text-2f596db02c9b#:~:text=The%20Problem%20of%20Words%20in,Generated%20Imagery))。OpenAI并未公开DALL·E 3的具体架构细节，但推测其使用了更强大的Transformer模型和更高分辨率 ([DALL-E 3 Can Finally Draw Text. OpenAI’s newest image generator finally… | by David Petrofsky | Medium](https://medium.com/@david.petrofsky/dall-e-3-can-finally-draw-text-2f596db02c9b#:~:text=I%20don%E2%80%99t%20have%20the%20insider,don%E2%80%99t%20know%20the%20real%20numbers))。综合**更多的数据**和**更大的模型**两方面因素，DALL·E 3 在文字生成上的表现远优于前代 ([DALL-E 3 Can Finally Draw Text. OpenAI’s newest image generator finally… | by David Petrofsky | Medium](https://medium.com/@david.petrofsky/dall-e-3-can-finally-draw-text-2f596db02c9b#:~:text=I%20don%E2%80%99t%20have%20the%20insider,don%E2%80%99t%20know%20the%20real%20numbers))。OpenAI 官方也强调DALL·E 3在**提示理解和遵循**上达到了新的水准 ([OpenAI & Microsoft’s DALL-E 3 Masters Image Creation Through Enhanced Captions | Synced](https://syncedreview.com/2023/10/23/openai-microsofts-dall-e-3-masters-image-creation-through-enhanced-captions/#:~:text=The%20research%20team%20posits%20that,to%20address%20the%20issue%20comprehensively))。凭借这些改进，DALL·E 3 成为目前文本到图像生成中在文字内容呈现上表现最出色的模型之一。

### Midjourney V6：特殊训练与提示语法实现文字绘制

Midjourney是另一个深受欢迎的生成模型系列。早期的Midjourney（V4、V5）在文字生成方面同样表现不佳，即使用户提示要求在图中写某个词，输出往往只是**类似该词形状的涂鸦**，无法正确拼写。然而，2023年末推出的 **Midjourney V6** 在这方面取得了突破，被誉为“终于攻克了图像内文字难题” ([Midjourney V6 Takes Major Step Forward with Photorealism and In-Image Text | No Film School](https://nofilmschool.com/midjourney-v6#:~:text=As%20we%E2%80%99ve%20covered%20with%20other,but%20even%20the%20letters%20themselves))。

Midjourney V6 的提升部分归功于**重新训练和数据增强**。据Midjourney团队创始人David Holz介绍，V6是他们“第三次从零开始训练的新模型”，耗时9个月大规模训练而成 ([Midjourney V6 Takes Major Step Forward with Photorealism and In-Image Text | No Film School](https://nofilmschool.com/midjourney-v6#:~:text=anything%20that%20they%E2%80%99ve%20released%20before%2C,took%20nine%20months%20to%20develop))。可以推测，在V6的训练数据或流程中，团队有意加入了**带有文字的图像及对应文本**，让模型**学会将特定单词与其视觉形态对上号**。同时，Midjourney在交互层面引入了一个简洁实用的机制：**提示语法中的双引号**。用户在提示中将想要出现在图中的文字用双引号括起来，模型就会尝试以该文字内容来渲染图像元素 ([Text Generation – Midjourney](https://docs.midjourney.com/hc/en-us/articles/32502277092109-Text-Generation#:~:text=When%20you%20use%20Midjourney%20version,quotation%20marks%20in%20your%20prompt))。例如，提示“一个赛博朋克风的霓虹招牌，写着**\"Midjourney\"**”，V6模型便会在生成的招牌上显示出“MIDJOURNEY”字样 ([Text Generation – Midjourney](https://docs.midjourney.com/hc/en-us/articles/32502277092109-Text-Generation#:~:text=Image%3A%20Two%20images%20of%20cyberpunk,prompt%20with%20double%20quotation%20marks))。这一语法暗示模型需特别关注引号内的词作为视觉文字，从而提高文字出现的准确性。Midjourney官方文档也说明，V6版本开始支持拉丁字母的单词或短语通过这种方式出现在图像中 ([Text Generation – Midjourney](https://docs.midjourney.com/hc/en-us/articles/32502277092109-Text-Generation#:~:text=When%20you%20use%20Midjourney%20version,quotation%20marks%20in%20your%20prompt))。

从用户早期试用反馈看，Midjourney V6 **大幅减少了文字变形和拼写错误**。许多案例显示，V6 能正确绘制**简单英文单词**（尤其是较短的品牌名、标识词等），甚至**风格化**地呈现在海报、招牌、书籍封面等场景中，而前代基本办不到 ([Midjourney V6 Takes Major Step Forward with Photorealism and In-Image Text | No Film School](https://nofilmschool.com/midjourney-v6#:~:text=However%2C%20from%20early%20samples%20of,in%20soon%20after%20as%20well))。No Film School 等媒体评价：“Midjourney V6 似乎终于解决了困扰AI模型的图像内文字问题” ([Midjourney V6 Takes Major Step Forward with Photorealism and In-Image Text | No Film School](https://nofilmschool.com/midjourney-v6#:~:text=As%20we%E2%80%99ve%20covered%20with%20other,but%20even%20the%20letters%20themselves)) ([Midjourney V6 Takes Major Step Forward with Photorealism and In-Image Text | No Film School](https://nofilmschool.com/midjourney-v6#:~:text=However%2C%20from%20early%20samples%20of,in%20soon%20after%20as%20well))。不过，需要注意的是，Midjourney的技术细节是闭源的。社区猜测V6也许在模型结构上增加了**专门的文字生成层**，可能生成图像和文字分别由不同模块处理然后合成 ([Loving all the new options open with reliable lettering : r/midjourney](https://www.reddit.com/r/midjourney/comments/18pmzwn/loving_all_the_new_options_open_with_reliable/#:~:text=r%2Fmidjourney%20www,text%20asynchronously%20while%20the))。这种设计可以解释为何V6能够实现相对**可靠的字符绘制**，并允许通过用户交互来**微调文字区域**（如使用后期的区域编辑功能修正个别字符 ([Text Generation – Midjourney](https://docs.midjourney.com/hc/en-us/articles/32502277092109-Text-Generation#:~:text=If%20you%20encounter%20issues%20with,Discord%20to%20fix%20minor%20glitches))）。无论实现细节如何，Midjourney V6 已证明通过**训练数据强化**和**策略引导**（引号提示），通用的扩散模型也能产出可用的图像文字。这为创作者在AI生成的图片中加入标题、标语、标识等提供了极大便利。

### Stable Diffusion XL：更大模型与双编码器带来的文字清晰度

Stable Diffusion系列模型是开源社群的代表，其在文字生成方面的演进也很典型。**Stable Diffusion 1.5**（2022年）作为早期版本，在这方面表现欠佳：几乎无法生成可读文字，往往输出畸形的字母碎片。这是由于1.5版使用的CLIP文本编码器和U-Net模型容量有限，且**没有针对字符的特殊训练**，导致模型对文本提示仅能理解语义类别（如“有标志的街景”），但无法逐字拼写 ([Why do DallE/SD produce beautiful images but garbled text?](https://artificial-intuition.beehiiv.com/p/dalle-diffusion-produce-beautiful-images-garbled-text#:~:text=understandable%20language,Still%2C%20the%20image%20diffusion))。生成的“文字”更像是视觉上类似字母的随机形状 ([Why do DallE/SD produce beautiful images but garbled text?](https://artificial-intuition.beehiiv.com/p/dalle-diffusion-produce-beautiful-images-garbled-text#:~:text=produce%20clean%20aesthetically%20pleasing%20images%2C,Still%2C%20the%20image%20diffusion))。对于这一点，Stability AI 官方也承认Stable Diffusion早期版本**无法渲染清晰文字**，是模型能力的局限之一 ([stabilityai/stable-diffusion-xl-base-1.0 - Hugging Face](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0#:~:text=stabilityai%2Fstable,struggles%20with%20more%20difficult))。

**Stable Diffusion XL (SDXL)**（2023年发布）作为新一代版本，在架构和训练上做出了多项改进，使其在生成图像文字上有明显进步。首先，**模型容量**大幅提升：SDXL的U-Net有35亿参数，远超1.5版的8.6亿（1.3B *trainable* parameters in 1.5? Actually 860M? The reference said 1.3B? Need to confirm. Actually [10] said 1.3b to 3.5b. So I'll say 1.3B vs 3.5B for consistency)**（含Refiner更达66亿）**。更大的网络能够记忆和绘制**更复杂的形状细节**，因此在高分辨率下能更好地区分并绘制出具体字母 ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=By%20expanding%20the%20UNet%20backbone,text%20legibility%20and%20other%20capabilities))。**生成分辨率**也提高到1024×1024，使小字在图中也有足够像素呈现，这减少了字符粘连模糊的现象。其次，SDXL采用了**双文本编码器**架构：结合OpenCLIP和另一种文本编码器处理提示 ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=For%20legibility%2C%20SDXL%E2%80%99s%20two%20separate,based%20details%20with%20higher%20accuracy))。官方指出，这种做法赋予SDXL对语言理解的“**更直观层次**”，能将文本提示与期望的视觉输出对齐得更紧密 ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=Understanding%20language%20at%20a%20more,over%20the%20image%20generation%20process))。简而言之，模型对提示中的词理解更精准，区分不同词语并在图像中分别表达出来的能力增强。这对需要生成特定**单词**或**字母**的场景非常有帮助——双编码器提供了更精细的条件控制，使模型在扩散过程中更容易**锁定某些区域去绘制对应文字** ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=For%20legibility%2C%20SDXL%E2%80%99s%20two%20separate,based%20details%20with%20higher%20accuracy))。再次，SDXL引入了**Refiner二级模型**来细化图像，它可以**修补**初始生成中不完美的细节，包括让文字轮廓更清晰、边缘更锐利。多模型协作相当于让模型有“**自我审阅**”能力，从而提高输出的一致性和质量 ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=1,model%2C%20producing%20higher%20fidelity%20outputs))。

从效果上看，Stable Diffusion XL 已经能在一定程度上生成**清晰可读的英文短词**。例如，在SDXL生成的街景或商品包装图像中，招牌或标签上的文字往往比1.5版清晰许多 ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=Example%20images%20generated%20by%20SDXL,character%20definitions%20and%20fewer%20distortions))。有测试比较显示，SDXL生成的字符**笔画分明、失真较少**，而Stable Diffusion 1.5的输出则字符常常残缺或扭曲 ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=Example%20images%20generated%20by%20SDXL,character%20definitions%20and%20fewer%20distortions))。尽管SDXL离完美拼写仍有差距（长句子或复杂字体下仍可能出错），但相对于早期模型已经是重大进步 ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=Example%20images%20generated%20by%20SDXL,character%20definitions%20and%20fewer%20distortions))。值得一提的是，Stability AI并未止步于SDXL的文字能力提升，他们后续开发的**Stable Diffusion 3.0系列**进一步引入了上述的**多模态Transformer**结构，将文字生成质量又提升了一个台阶 ([Stability AI 三大图像生成模型在 Bedrock 正式可用](https://aws.amazon.com/cn/campaigns/selected-blog-stability-ai-available/#:~:text=%E7%9B%B8%E6%AF%94%20Stable%20Diffusion%20XL%20,Large%20%E7%9A%84%E4%B8%BB%E8%A6%81%E6%94%B9%E8%BF%9B%E4%B9%8B%E4%B8%80%E6%98%AF%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E4%B8%AD%E7%9A%84%E6%96%87%E5%AD%97%E8%B4%A8%E9%87%8F%E3%80%82%E5%BE%97%E7%9B%8A%E4%BA%8E%E5%88%9B%E6%96%B0%E7%9A%84%20Diffusion%20Transformer%20%E6%9E%B6%E6%9E%84%EF%BC%8C%E6%96%B0%E6%A8%A1%E5%9E%8B%E5%87%8F%E5%B0%91%E4%BA%86%E6%8B%BC%E5%86%99%E5%92%8C%E6%8E%92%E7%89%88%E9%94%99%E8%AF%AF%E3%80%82%E8%AF%A5%E6%9E%B6%E6%9E%84%E4%B8%BA%E5%9B%BE%E5%83%8F%E5%92%8C%E6%96%87%E5%AD%97%E5%88%86%E5%88%AB%E8%AE%BE%E8%AE%A1%E4%BA%86%E4%B8%A4%E5%A5%97%E7%8B%AC%E7%AB%8B%E7%9A%84%E6%9D%83%E9%87%8D%EF%BC%8C%E4%BD%86%E5%85%81%E8%AE%B8%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BF%A1%E6%81%AF%E6%B5%81%E5%8A%A8%E3%80%82))。据报道，这些新架构通过分离文字和图像的生成子模型，**显著减少了拼写和排版错误** ([Stability AI 三大图像生成模型在 Bedrock 正式可用](https://aws.amazon.com/cn/campaigns/selected-blog-stability-ai-available/#:~:text=%E7%9B%B8%E6%AF%94%20Stable%20Diffusion%20XL%20,Large%20%E7%9A%84%E4%B8%BB%E8%A6%81%E6%94%B9%E8%BF%9B%E4%B9%8B%E4%B8%80%E6%98%AF%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E4%B8%AD%E7%9A%84%E6%96%87%E5%AD%97%E8%B4%A8%E9%87%8F%E3%80%82%E5%BE%97%E7%9B%8A%E4%BA%8E%E5%88%9B%E6%96%B0%E7%9A%84%20Diffusion%20Transformer%20%E6%9E%B6%E6%9E%84%EF%BC%8C%E6%96%B0%E6%A8%A1%E5%9E%8B%E5%87%8F%E5%B0%91%E4%BA%86%E6%8B%BC%E5%86%99%E5%92%8C%E6%8E%92%E7%89%88%E9%94%99%E8%AF%AF%E3%80%82%E8%AF%A5%E6%9E%B6%E6%9E%84%E4%B8%BA%E5%9B%BE%E5%83%8F%E5%92%8C%E6%96%87%E5%AD%97%E5%88%86%E5%88%AB%E8%AE%BE%E8%AE%A1%E4%BA%86%E4%B8%A4%E5%A5%97%E7%8B%AC%E7%AB%8B%E7%9A%84%E6%9D%83%E9%87%8D%EF%BC%8C%E4%BD%86%E5%85%81%E8%AE%B8%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BF%A1%E6%81%AF%E6%B5%81%E5%8A%A8%E3%80%82))。总的来看，Stable Diffusion 的演进证明了通过**增大模型规模、改进文本编码和加入细化过程**，即使不开源模型也能逐步攻克图像文字生成这一难题，为开源社区用户带来更实用的生成能力。

## 新旧模型在文字生成上的对比

早期文生图模型（如Stable Diffusion 1.5、DALL·E 2等）与当前先进模型在文字生成方面存在明显差异。主要体现在：**能否准确拼写出指定文字**、**文字的清晰度和一致性**、以及**对提示中文字信息的遵循程度**。下表对比了几个具有代表性的模型：

| **模型** | **文字生成机制/特性** | **文字生成效果** |
| --- | --- | --- |
| **Stable Diffusion 1.5 (2022)** | 使用CLIP文本编码器 + 扩散模型，无专门文字处理模块。训练数据未强调图像中文字。 | **几乎不识别具体字母**，输出文字为无意义形状，字符常扭曲混乱 ([Why do DallE/SD produce beautiful images but garbled text?](https://artificial-intuition.beehiiv.com/p/dalle-diffusion-produce-beautiful-images-garbled-text#:~:text=understandable%20language,Still%2C%20the%20image%20diffusion))。提示中的单词往往被忽略或误拼 ([openai DALL-E 3 论文 提升图像生成的关键：更好的图像描述_dall-e3论文解读-CSDN博客](https://blog.csdn.net/ryo1060732496/article/details/136208154#:~:text=%E6%88%91%E4%BB%AC%E5%B1%95%E7%A4%BA%E4%BA%86%E9%80%9A%E8%BF%87%E8%AE%AD%E7%BB%83%E9%AB%98%E5%BA%A6%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%9A%84%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E6%A0%87%E9%A2%98%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%98%BE%E7%9D%80%E6%94%B9%E5%96%84%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8F%90%E7%A4%BA%E8%B7%9F%E9%9A%8F%E8%83%BD%E5%8A%9B%E3%80%82))。 |
| **Stable Diffusion XL (2023)** | 扩散模型容量大幅提升（3.5B参数）; 双文本编码器提供细粒度文本条件; 两阶段“基础+精细”生成架构。 | **能生成简单清晰文字**（如标志、短语），字符笔画更分明，失真减少 ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=Example%20images%20generated%20by%20SDXL,character%20definitions%20and%20fewer%20distortions))。复杂长文本偶有错误，但相比1.5已是质变提升。 |
| **OpenAI DALL·E 2 (2022)** | CLIP text encoder + 解码器架构，无文字专门机制; 训练时未强调逐字对应。 | **无法正确拼写**提示词，常产生类似“秘语”的乱码 ([Why do DallE/SD produce beautiful images but garbled text?](https://artificial-intuition.beehiiv.com/p/dalle-diffusion-produce-beautiful-images-garbled-text#:~:text=understandable%20language,Still%2C%20the%20image%20diffusion))。“看图写字”能力极弱，经常无视提示中的文字要求。 |
| **OpenAI DALL·E 3 (2023)** | 利用GPT模型架构并经改进训练（使用高质量生成描述作为训练标签); 更强的语言-图像对齐优化。 | **显著改善拼写能力**，大部分常见单词可正确再现 ([DALL-E 3 Can Finally Draw Text. OpenAI’s newest image generator finally… | by David Petrofsky | Medium](https://medium.com/@david.petrofsky/dall-e-3-can-finally-draw-text-2f596db02c9b#:~:text=DALL,in%20claymation))。对提示中的文字要求基本能遵循，实现图文高度对齐 ([OpenAI & Microsoft’s DALL-E 3 Masters Image Creation Through Enhanced Captions | Synced](https://syncedreview.com/2023/10/23/openai-microsofts-dall-e-3-masters-image-creation-through-enhanced-captions/#:~:text=The%20research%20team%20posits%20that,to%20address%20the%20issue%20comprehensively))。 |
| **Midjourney V5 (2023)** | 私有模型，具体架构未公布。缺乏针对文字的提示语法或训练目标。 | **偶尔模仿文字形状**，但字符顺序和拼写错误率极高。生成的文本元素无实际可读含义（仅视觉上类似） ([DALL-E 3 Can Finally Draw Text. OpenAI’s newest image generator finally… | by David Petrofsky | Medium](https://medium.com/@david.petrofsky/dall-e-3-can-finally-draw-text-2f596db02c9b#:~:text=This%20image%20is%20much%20better,it%20was%20going%20for%20there)) ([DALL-E 3 Can Finally Draw Text. OpenAI’s newest image generator finally… | by David Petrofsky | Medium](https://medium.com/@david.petrofsky/dall-e-3-can-finally-draw-text-2f596db02c9b#:~:text=it%E2%80%99s%20missing%20the%20greeting,the%20words%20are%20real%20words))。 |
| **Midjourney V6 (2023)** | 从零训练的新模型，可能引入文字数据强化; 支持提示中用**双引号**标记需出现的文字; 或存在文字专用生成模块。 | **大幅提升拼写准确率**，可在图中写出短文本（英文）并保持风格一致 ([Midjourney V6 Takes Major Step Forward with Photorealism and In-Image Text | No Film School](https://nofilmschool.com/midjourney-v6#:~:text=However%2C%20from%20early%20samples%20of,in%20soon%20after%20as%20well))。普通场景下单词清晰可读，标牌文字基本符合要求。 |

*表：不同文本到图像模型的文字生成能力与机制对比。早期模型缺乏专门的文字对齐技术，生成的文字多为乱码；最新模型通过改进数据、架构和训练，对提示中的文字信息有了更好的遵循和还原。*

从上述对比可以看出，**早期模型的问题**包括：对提示中的文字缺乏理解（往往忽略或错译为不相关形状），模型没有“学会”字母的独立概念，只是把训练中见过的**文字当作一种纹理**来混合重构，因而输出难以阅读 ([Why do DallE/SD produce beautiful images but garbled text?](https://artificial-intuition.beehiiv.com/p/dalle-diffusion-produce-beautiful-images-garbled-text#:~:text=understandable%20language,Still%2C%20the%20image%20diffusion))。而**新一代模型的改进**在于：让模型在训练中真正接触并学习了**文字的语义与视觉对应**（通过更好的标签和数据）、在架构上增加了**处理文字的能力**（如多编码器、Transformer模块区分文字/图像）以及在生成过程中**强化了文字区域的准确性**（如级联细化、OCR损失等）。因此，新模型能够将文本提示精确地反映到图像里，使AI生成的图像第一次具有了**可读的文字内容**。

## 结论

AI文生图模型在文字生成质量上的显著提升是多种因素共同作用的结果。从数据层面的**标注精细化**到模型层面的**架构专门化**，再到训练目标的**跨模态对齐优化**，这些进步让模型真正学会了“读写”人类语言。在OpenAI DALL·E 3、Midjourney V6、Stable Diffusion XL等当前主流模型中，我们已经看到了令人欣喜的成果：招牌上的文字清晰可辨、海报上的标题准确呈现。这不仅拓宽了生成模型的应用边界，也为创意工作流程带来了便利（如直接生成含广告标语的视觉稿等）。同时，我们也认识到依然有改进空间：对长段落、多语言文字的生成，新模型有待进一步加强。展望未来，随着更**大模型**的出现、更**聪明训练策略**的应用以及**多模态技术**的持续融合，AI有望掌握更复杂的文字排版和语言表达，让我们距离“所见即所写”的生成体验更近一步。通过总结近年的技术演进，我们深刻体会到：**针对特定难题的创新**（如图像文字）可以驱动整个生成模型朝更智能、更通用的方向发展，这也将持续推动AI绘画从奇巧走向实用。 

**参考文献：**

1. Betker, J. et al. (2023). *Improving Image Generation with Better Captions*. OpenAI.  ([openai DALL-E 3 论文 提升图像生成的关键：更好的图像描述_dall-e3论文解读-CSDN博客](https://blog.csdn.net/ryo1060732496/article/details/136208154#:~:text=%E6%88%91%E4%BB%AC%E5%B1%95%E7%A4%BA%E4%BA%86%E9%80%9A%E8%BF%87%E8%AE%AD%E7%BB%83%E9%AB%98%E5%BA%A6%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%9A%84%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E6%A0%87%E9%A2%98%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%98%BE%E7%9D%80%E6%94%B9%E5%96%84%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8F%90%E7%A4%BA%E8%B7%9F%E9%9A%8F%E8%83%BD%E5%8A%9B%E3%80%82)) ([OpenAI & Microsoft’s DALL-E 3 Masters Image Creation Through Enhanced Captions | Synced](https://syncedreview.com/2023/10/23/openai-microsofts-dall-e-3-masters-image-creation-through-enhanced-captions/#:~:text=The%20research%20team%20posits%20that,to%20address%20the%20issue%20comprehensively))  
2. Stability AI. (2023). *Stable Diffusion XL Release Blog*.  ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=For%20legibility%2C%20SDXL%E2%80%99s%20two%20separate,based%20details%20with%20higher%20accuracy)) ([Stable Diffusion XL: Everything You Need to Know • Magai](https://magai.co/stable-diffusion-xl-1-0/#:~:text=Example%20images%20generated%20by%20SDXL,character%20definitions%20and%20fewer%20distortions))  
3. Petrofsky, D. (2023). *DALL-E 3 Can Finally Draw Text*. Medium.  ([DALL-E 3 Can Finally Draw Text. OpenAI’s newest image generator finally… | by David Petrofsky | Medium](https://medium.com/@david.petrofsky/dall-e-3-can-finally-draw-text-2f596db02c9b#:~:text=I%20don%E2%80%99t%20have%20the%20insider,don%E2%80%99t%20know%20the%20real%20numbers)) ([DALL-E 3 Can Finally Draw Text. OpenAI’s newest image generator finally… | by David Petrofsky | Medium](https://medium.com/@david.petrofsky/dall-e-3-can-finally-draw-text-2f596db02c9b#:~:text=DALL,of%20a%20letter%20to%20Santa%3F%E2%80%9D))  
4. Midjourney. (2023). *Midjourney V6 Documentation – Text Generation*.  ([Text Generation – Midjourney](https://docs.midjourney.com/hc/en-us/articles/32502277092109-Text-Generation#:~:text=When%20you%20use%20Midjourney%20version,quotation%20marks%20in%20your%20prompt))  
5. No Film School. (2023). *Midjourney V6 Takes Major Step Forward with In-Image Text*.  ([Midjourney V6 Takes Major Step Forward with Photorealism and In-Image Text | No Film School](https://nofilmschool.com/midjourney-v6#:~:text=As%20we%E2%80%99ve%20covered%20with%20other,but%20even%20the%20letters%20themselves)) ([Midjourney V6 Takes Major Step Forward with Photorealism and In-Image Text | No Film School](https://nofilmschool.com/midjourney-v6#:~:text=However%2C%20from%20early%20samples%20of,in%20soon%20after%20as%20well))  
6. AWS Amazon. (2023). *Stability AI 模型在Bedrock上线*.  ([Stability AI 三大图像生成模型在 Bedrock 正式可用](https://aws.amazon.com/cn/campaigns/selected-blog-stability-ai-available/#:~:text=%E7%9B%B8%E6%AF%94%20Stable%20Diffusion%20XL%20,Large%20%E7%9A%84%E4%B8%BB%E8%A6%81%E6%94%B9%E8%BF%9B%E4%B9%8B%E4%B8%80%E6%98%AF%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E4%B8%AD%E7%9A%84%E6%96%87%E5%AD%97%E8%B4%A8%E9%87%8F%E3%80%82%E5%BE%97%E7%9B%8A%E4%BA%8E%E5%88%9B%E6%96%B0%E7%9A%84%20Diffusion%20Transformer%20%E6%9E%B6%E6%9E%84%EF%BC%8C%E6%96%B0%E6%A8%A1%E5%9E%8B%E5%87%8F%E5%B0%91%E4%BA%86%E6%8B%BC%E5%86%99%E5%92%8C%E6%8E%92%E7%89%88%E9%94%99%E8%AF%AF%E3%80%82%E8%AF%A5%E6%9E%B6%E6%9E%84%E4%B8%BA%E5%9B%BE%E5%83%8F%E5%92%8C%E6%96%87%E5%AD%97%E5%88%86%E5%88%AB%E8%AE%BE%E8%AE%A1%E4%BA%86%E4%B8%A4%E5%A5%97%E7%8B%AC%E7%AB%8B%E7%9A%84%E6%9D%83%E9%87%8D%EF%BC%8C%E4%BD%86%E5%85%81%E8%AE%B8%E4%B8%A4%E7%A7%8D%E6%A8%A1%E6%80%81%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BF%A1%E6%81%AF%E6%B5%81%E5%8A%A8%E3%80%82))  
7. Aliyun. (2025). *Stable Diffusion 3.5 模型介绍*.  ([如何快速通过百炼平台调用StableDiffusion服务_大模型服务平台百炼(Model Studio)-阿里云帮助中心](https://help.aliyun.com/zh/model-studio/stable-diffusion-quick-start/#:~:text=Stable%20Diffusion%203.5%E5%9C%A8%E4%B9%8B%E5%89%8D%E7%9A%841.5%E7%89%88%E6%9C%AC%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8A%E5%81%9A%E4%BA%86%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%94%B9%E8%BF%9B%EF%BC%8C%E9%87%87%E7%94%A8%E4%BA%86%E5%A4%9A%E6%A8%A1%E6%80%81%E6%89%A9%E6%95%A3%E5%8F%98%E5%8E%8B%E5%99%A8%EF%BC%88MMDiT%EF%BC%89%E6%9E%B6%E6%9E%84%EF%BC%8C%E7%BB%93%E5%90%88%E4%BA%86%E4%B8%89%E7%A7%8D%E5%9B%BA%E5%AE%9A%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%87%E6%9C%AC%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%8C%E5%B9%B6%E4%BD%BF%E7%94%A8Query,5%20Medium%EF%BC%8C%E6%AF%8F%E4%B8%AA%E7%89%88%E6%9C%AC%E9%83%BD%E9%92%88%E5%AF%B9%E4%B8%8D%E5%90%8C%E7%9A%84%E7%94%A8%E6%88%B7%E9%9C%80%E6%B1%82%E8%BF%9B%E8%A1%8C%E4%BA%86%E4%BC%98%E5%8C%96%EF%BC%8C%E6%8F%90%E4%BE%9B%E4%BA%86%E9%AB%98%E5%BA%A6%E7%9A%84%E5%AE%9A%E5%88%B6%E6%80%A7%E5%92%8C%E6%98%93%E7%94%A8%E6%80%A7%E3%80%82))  
8. Beehiiv Artificial Intuition. (2022). *Why do DALL-E/SD produce garbled text?*. 
