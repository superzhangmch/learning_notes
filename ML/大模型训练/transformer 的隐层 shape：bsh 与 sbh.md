# transformer çš„éšå±‚ shape ç”¨ [B,S,H] è¿˜æ˜¯ [S,B,H] layout, å•¥åŒºåˆ«

megatron ç¬¬äºŒç¯‡ [ã€ŠEfficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LMã€‹](https://arxiv.org/pdf/2104.04473) 4.2 èŠ‚ï¼ˆè®¡ç®—ä¼˜åŒ–ä¸€èŠ‚ï¼‰è¯´ï¼š
> we changed the data layout in the transformer layer to avoid memory-intensive transpose operations, and to enable the use of strided batched GEMM kernels. Specifically, we changed the data layout from [ğ‘, ğ‘ , ğ‘, â„] to [ğ‘ , ğ‘, ğ‘, â„], where ğ‘, ğ‘ , ğ‘, and â„ are batch, sequence, attention-head, and hidden-size dimensions, respectively

ä¹Ÿå°±æ˜¯è¯´ä¸ºäº†ä½¿ç”¨ strided batched GEMM ä¹˜æ³•ï¼ˆå®ƒè¦æ±‚ [ğ‘ , ğ‘, ğ‘, â„] çš„è¾“å…¥ï¼‰ï¼Œç‰¹æ„ä» [B, S, H] è½¬åˆ°äº† [S, B, H]ã€‚è‹¥ä¸è½¬ï¼Œåˆ™ä½¿ç”¨ strided batched GEMM ä¼šæœ‰é«˜æ˜‚çš„ transpose æˆæœ¬ï¼ˆæ³¨æ„ï¼Œéä¸å¾—å·²ï¼Œåº•å±‚çš„å®ç°ä¸ç”¨åšç‰©ç†ä¸Šçš„ transposeï¼Œé‚£ä¹ˆè¿™é‡ŒæŒ‡çš„æ˜¯ç‰©ç†transposeï¼‰ã€‚ä»æ­¤ megatron å˜æˆäº† sequence first çš„ã€‚

è¿™ä¸€æ®µæ·±è¡¨å›°æƒ‘ã€‚ç‰¹ç ”ç©¶ä¸‹ã€‚

----

ä»¤ B=batch_sizeï¼Œ S=sequence_len, H=hidden_dim, transformer çš„ hidden shapeï¼Œå¯ä»¥æœ‰ä¸¤ç§é€‰æ³•ï¼š[B,S,H] (batch first) æˆ– [S,B,H] (sequence first)ã€‚

å¯¹äº transformer æ¥è¯´ï¼š
- åªæœ‰ softmax(QK')V' è¿™ä¸€æ­¥çœŸæ­£æ¶‰åŠåºåˆ—æ“ä½œï¼Œä»è€Œ [B,S,H]ã€[S,B,H] æœ‰åŒºåˆ«ã€‚
- å…¶ä»–åœ°æ–¹ï¼ŒåŒ…æ‹¬ attn çš„ projectionã€FNNã€layerNorm ç­‰ï¼Œ[B,S,H]ã€[S,B,H] æ²¡åŒºåˆ«ï¼šå®ƒä»¬éƒ½æ˜¯ token ç²’åº¦è®¡ç®—çš„ï¼Œæœ¬è´¨ä¸Š batch_size=B*Sã€‚ 

å…ˆè®°ä¸‹ç»“è®ºï¼šæ— è®ºç”¨å“ªç§ï¼Œå½“å‰ï¼ˆ2025.08.17) transformer è®¡ç®— attn æ—¶ï¼Œ**éƒ½æ˜¯ç”¨çš„ [B,S,H]**, å½¢å¼ï¼Œå¦‚æœä¸æ˜¯ï¼Œåˆ™å…ˆè½¬æˆã€‚

RNN æ—¶ä»£ hidden éƒ½æ˜¯ [S, B, H] å½¢å¼çš„ï¼ˆRNN/LSTM/GRU æ¨¡å—ï¼‰ã€‚transformer æ—¶ä»£ï¼Œä¸¤è€…éƒ½æœ‰ç”¨ï¼Œä¸»æµæ˜¯ [B,S,H]ï¼Œè€Œ megatron æ˜¯ç”¨çš„ [S,B,H] ï¼ˆä½†å†…éƒ¨ attn ä¼šè½¬æˆ [B, S, H]ï¼‰ã€‚ã€ä½†æ˜¯å®é™…è®¡ç®—ä¸­ï¼Œæ— è®º shape å•¥æ ·ï¼ŒçœŸçš„ä½œè®¡ç®—é‚£ä¸€æ­¥ï¼Œåº•å±‚ä¼šæ ¹æ®å®é™…å†…å­˜æƒ…å†µï¼Œé€‰æ‹©æœ€ä¼˜çš„æ–¹å¼ï¼Œè€Œä¸æ˜¯ä½ äº¤æ¢ç»´åº¦æˆ–è€… reshape å•¥çš„ï¼Œä¸€å®šå¯¼è‡´å†…å­˜æ“ä½œã€‘

----

ä¸‹é¢ä¸€æ¢ä¸‹å…·ä½“éƒ½æ˜¯æ€ä¹ˆå¤„ç†çš„ã€‚å°†ä¼šçœ‹åˆ°è®¡ç®— attn æ—¶ï¼Œéƒ½ä¼šå˜æˆ [batch*num_head, seq, head_dim], å³ batch firstã€æ³¨æ„å› ä¸ºæ˜¯ multi heads, æ‰€ä»¥ H ä¼šæ‹†æˆ num_head ä¸ head_dim, ä¸”æŠŠ head_dim*batch ä½œä¸ºæ–°çš„å¤§batchã€‘ï¼š

### ï¼ˆ1ï¼‰ã€torch.nn.MultiheadAttention: æœ€ç»ˆç”¨ [B, S, H]

å®ƒé»˜è®¤æŠŠ input å½“ batch_first=False ï¼ˆ[S, B, H] å¤„ç†ã€‚çœ‹ä»£ç ï¼Œå¦‚æœä¸æ˜¯ï¼Œä¹Ÿä¼šå…ˆè½¬æˆ [S,B,H]ã€‚ä½†æ˜¯åˆ°äº†çœŸæ­£åš MHA çš„æ—¶å€™ï¼Œè¿˜æ˜¯ä¼šè½¬ [B,S,H]ï¼‰:

<img width="1368" height="600" alt="image" src="https://github.com/user-attachments/assets/233f5a9a-b626-453d-8032-f3b58c4ea689" />

å®ƒæ‰€ä»¥è¿™æ ·ï¼Œæ˜¯å†å²ä¼ ç»Ÿç»§æ‰¿ã€‚transformer ä¹Ÿæ˜¯åºåˆ—æ¨¡å‹ï¼Œå®ƒæ˜¯åœ¨æ¥å£å±‚é¢ï¼Œç»§æ‰¿äº† RNN æ—¶å€™çš„æƒ¯ä¾‹è€Œå·²ã€‚

### ï¼ˆ2ï¼‰ã€flashAttentionï¼šç”¨ [B, S, H]

flashAttention æ˜¯ batch first çš„ã€‚


### ï¼ˆ3ï¼‰ã€megatronï¼š attn ç”¨ [B, S, H]

megatron åœ¨å…¶ä»–åœ°æ–¹éƒ½ç”¨çš„ [S, B, H], ä½†æ˜¯ attn æœ€æ ¸å¿ƒå¤„ï¼Œä»ç„¶ç”¨çš„ [B, S, H] å½¢å¼ã€‚è‡³äºåŸå› ï¼Œè§ä¸‹æ–‡åˆ†æã€‚

<img width="1250" height="1134" alt="image" src="https://github.com/user-attachments/assets/ea0e0cd2-cd2b-497b-871d-411b0177c175" />

æ³¨æ„ç”¨äº†ï¼š torch.bmm/torch.baddbmm

æŒ‰å¼€å¤´æ‰€è¿°ï¼Œmegatron è¯´å› ä¸º..., æ‰€ä»¥è¦ç”¨ seq firstã€‚è¿™å’Œç°åœ¨æ‰€è§çš„å®é™… megatron ä»£ç ä¸ä¸€è‡´ã€‚

é‚£ä¹ˆï¼š
- æ˜¯ megatron ç ”å‘äººå‘˜åæ¥å‘ç°è¿˜æ˜¯ batch_first æ›´å¥½ï¼Œä½†æ˜¯é™äºæƒ¯æ€§ä¸å¥½æ”¹äº†ï¼Œäºæ˜¯æ•´ä½“ç”¨ [B, S, H], è€Œåœ¨ attn æ—¶ä¸´æ—¶ seq-first ä¸€ä¸‹å—?
- è¿˜æ˜¯è¯´ä¸Šé¢å‘ˆç°çš„ batch_first çš„ torch.bmm/torch.baddbmm å†…éƒ¨ï¼Œå®ƒåˆè¦ç»™å†è½¬æˆç‰©ç†è¿ç»­çš„ seq-firstï¼ˆè€ŒåŸç”Ÿ[S, B, H] åœ¨æ­¤å¤„è½¬çš„æ—¶å€™æ— æˆæœ¬å—ï¼‰ï¼Ÿ

----

### çŸ©é˜µä¹˜æ³•çš„è¡¥å……çŸ¥è¯†

**ï¼ˆ1ï¼‰torch.bmmï¼š**

atch Matrix Multiplyï¼Œç›´æ¥åšæ‰¹é‡çŸ©é˜µä¹˜æ³•
```
A.shape = [Batch, M, K]
B.shape = [Batch, K, N]
output.shape = bmm(a,b).shape = [batch, M, N]
output = [A[i] Ã— B[i] for i in range(batch)]
```

noteï¼š

- torch.dot â†’ åªèƒ½ 1D å‘é‡ç‚¹ç§¯ã€‚ä¹Ÿå°±æ˜¯åªèƒ½ä¸¤ä¸ªå‘é‡ï¼ˆbatchsize=1ï¼‰ï¼Œ è‹¥è¦ batchsize > 1, torch ä¸­éœ€è¦ç»„åˆå¤šç®—å­æ¥å®ç°ã€‚
- torch.mm â†’ åªèƒ½ 2D çŸ©é˜µä¹˜ã€‚ä¹Ÿå°±æ˜¯ä¸¤ä¸ªçŸ©é˜µä¹˜æ³•ï¼Œæˆ–è€…è¯´ batchsize=1
- torch.bmm â†’ åªèƒ½ 3D æ‰¹é‡çŸ©é˜µä¹˜ã€‚batchsize > 1 æ—¶çš„çŸ©é˜µä¹˜æ³•

**ï¼ˆ2ï¼‰torch.baddbmmï¼š**

Batch Add + Matrix Multiplyï¼Œåœ¨çŸ©é˜µä¹˜çš„åŸºç¡€ä¸Šï¼Œè¿˜èƒ½æŠŠå·²æœ‰çš„çŸ©é˜µç»“æœåŠ è¿›å»ï¼ˆå¸¦ alpha/beta ç³»æ•°ï¼‰ã€‚åªæ˜¯ torch.bmm çš„ä¸€ä¸ªæ‹“å±•ã€‚
```
A.shape = [Batch, M, K]
B.shape = [Batch, K, N]
C.shape = [Batch, M, N] ä¸ºå·²æœ‰ç»“æœ
output = [Î²C[i] + Î±(A[i] Ã— B[i]) for i in range(batch)]
output.shape = torch.baddbmm(C, A, B).shape = [Batch, M, N]
```

**ï¼ˆ3ï¼‰strided batched GEMMï¼š**

å‰é¢ä¸¤ä¸ªéƒ½å¥½ç†è§£ã€‚strided batched GEMM æ˜¯è¿™æ ·çš„çŸ©é˜µä¹˜æ³•ï¼š
```
A.shape = [M, Batch, K] è¿ç»­å­˜å‚¨
B.shape = [N, Batch, K] è¿ç»­å­˜å‚¨
output.shape = bmm(A,B').shape = [M, batch, N]
output = [A[:,i,:] Ã— B'[:,i,:] for i in range(batch)].trans_shape_to([M, batch, N])
```

**ï¼ˆ4ï¼‰ä»€ä¹ˆæ—¶å€™ tensor çš„ reshape/transpose/permute ç­‰æ“ä½œä¼šå¯¼è‡´ç‰©ç†å†…å­˜ copy**

ä¸æ˜¯æ¯ä¸€æ¬¡è°ƒç”¨éƒ½ä¼šå¯¼è‡´ å†…å­˜ copyï¼Œè€Œæ˜¯å¿…è¦æ—¶è¿›è¡Œã€‚ä¸¾å‡ ä¸ªæ¯”è¾ƒçœŸå®çš„ MHA ä¸­çš„ä¾‹å­ï¼Œä¸‹é¢éƒ½æ˜¯ ai åˆ†æçš„ï¼š

```
# å‡è®¾è¾“å…¥
batch, seq, hid, num_heads = 2, 5, 16, 4
head_dim = hid // num_heads
q = torch.randn(batch, seq, hid)             # [batch, seq, hid], contiguous

q = q.transpose(0, 1)                        # [seq, batch, hid], zero-copy (stride æ”¹å˜)
# å…ˆå˜æˆ [seq, batch, hid]ã€‚torch.nn.MultiheadAttention å°±æ˜¯è¿™æ ·ã€‚ç„¶åä½œå¤š head åˆ‡åˆ†

q = q.reshape(seq, batch, num_heads, head_dim)  # [seq, batch, num_heads, head_dim], zero-copy
q = q.reshape(seq, batch * num_heads, head_dim) # [seq, batch*num_heads, head_dim], âš ï¸ copy
q = q.reshape(batch * num_heads, seq, head_dim) # [batch*num_heads, seq, head_dim], zero-copy
```

å¦ä¸€ä¸ªä¾‹å­ï¼š

```
batch, seq, hid, num_heads = 2, 5, 16, 4
head_dim = hid // num_heads
q = torch.randn(batch, seq, hid)             # [batch, seq, hid], contiguous

q = q.view(batch, seq, num_heads, head_dim)  # [batch, seq, num_heads, head_dim], zero-copy
q = q.permute(0, 2, 1, 3)                    # [batch, num_heads, seq, head_dim], zero-copy (stride æ”¹å˜)
q = q.reshape(batch * num_heads, seq, head_dim) # [batch*num_heads, seq, head_dim], âš ï¸ copy
```

å†ä¸€ä¸ªä¾‹å­ï¼š

```
batch, seq, hid, num_heads = 2, 5, 16, 4
head_dim = hid // num_heads
q = torch.randn(seq, batch, hid)                 # [seq, batch, hid], contiguousï¼Œ megatron æƒ…å†µ

q = q.view(seq, batch, num_heads, head_dim)      # [seq, batch, num_heads, head_dim], zero-copy (hid æ‹†æˆ num_heads*head_dim)
q = q.reshape(seq, batch * num_heads, head_dim)  # [seq, batch*num_heads, head_dim], âš ï¸ è§¦å‘ copy (batch ä¸ num_heads ä¸è¿ç»­)
q = q.permute(1, 0, 2)                           # [batch*num_heads, seq, head_dim], zero-copy (stride æ”¹å˜)
```
