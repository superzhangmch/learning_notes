核心思想：用"最大前导零"估算基数                      

### 1. 概率直觉

假设你抛硬币，连续出现 k 个正面的概率是 1/2^k。

如果你观察到最多连续出现了 5 个正面，那大概抛了 2^5 = 32 次左右。
                                                           
HyperLogLog 把这个思路用在哈希值上：
- 把每个 user_id 哈希成二进制数     
- 统计二进制中"前导零的最大个数" 
- 如果最大前导零是 k，估算大约有 2^k 个不同元素

### 2. 分桶降低方差   

单个估计值方差太大，所以用 m 个桶（本例用 16384 个）：

```
hash(user_id) = 0101 1000 1101 0010 ... 
                ^^^^
                前14位 → 决定放入哪个桶 (桶号 0~16383)
                     ^^^^^^^^^^^^^^^^^
                     剩余位 → 统计前导零个数
```

每个桶独立估算，最后取调和平均，大幅降低误差。

### 3. 内存效率惊人
```
┌─────────────┬──────────┬──────────────────────────────────┐ 
│    方法     │ 内存占用 │               说明               │
├─────────────┼──────────┼──────────────────────────────────┤
│ Set (精确)  │ ~5.5 MB  │ 存储 69,252 个 UUID              │
├─────────────┼──────────┼──────────────────────────────────┤
│ HyperLogLog │ ~12 KB   │ 只需 16,384 个寄存器，每个 6 bit │
└─────────────┴──────────┴──────────────────────────────────┘
```
用 0.2% 的内存，达到 99.5% 的准确率！

### 4. 简化版代码逻辑
```
def add(user_id):
    h = hash(user_id)           # 64位哈希
    bucket = h >> 50            # 前14位 → 桶号
    trailing = count_leading_zeros(h & 0x3FFFFFFFFFFFF)  # 剩余50位的前导零
    registers[bucket] = max(registers[bucket], trailing + 1)

def count():
    # 调和平均 + 偏差修正
    return alpha * m^2 / sum(2^(-register[i]))
```

这就是为什么 Redis、BigQuery、Spark 都内置了 HyperLogLog —— 用极小内存就能统计海量 UV。
