<<TRAINING DIFFUSION MODELS WITH REINFORCEMENT LEARNING>> （可简称DDPO）https://arxiv.org/pdf/2305.13301，https://rl-diffusion.github.io/，把txt2img的diffusion模型与强化学习结合了起来。
对一些不太期望prompt能工作的场合（比如生成画质压缩的图），如果能设计出 reward，则可以该文提出的DDPO。
文章给出了一种让让结果img与prompt更加align的reward设计：用LLava给出“生成的图”的描述text（问LLaVa，what is happening in the picture?），然后让文本相关性模型判定“llava的描述text”与原始prompt的文本相关性，并用该文本相关性作为reward。

DDPO的工作原理是：diffusion经过N步才最终生成，这每一步都可以当做一个强化学习（RL）步。一般性RL中，每一step都有reward，而RL就是要找到使得sum(per step reward)最大的策略。而DDPO中，则是只有最后一步生成的时候，才有reward，中间过程没reward。总之如同一般性的RL，最后可以用TRPO/PPO来求解。

《easyphoto》生成人体写真一文，正用到了上面的DDPO一文。它是把生成的人脸与训练集人像的相似度当做了reward。
