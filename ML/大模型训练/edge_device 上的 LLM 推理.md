# edge device 设备上作 small size LLM 推理

如果不是用 cpu 来算，那么就是在 GPU 或 NPU 这样的东西上。

现实中，往往是在 SoC 这样的芯片上做。 SoC 一般集成有 cpu+gpu+npu，还有通信模块等。但是没有内存与闪存。

### gpu vs npu

gpu 是通用的计算加速器。再复杂的操作，都可以用代码实现，并高效计算。

而 npu，是以硬件方式实现了一些特殊的算子（比如 CNN），这样对同样操作，npu 比 gpu 快。如果原生不支持某种操作，那么要用 npu 来做这种操作，必然效果低下。

npu 上一般做的是 int8 计算（当然不仅限于 int8，有些支持 int4、int16、甚至 fp16/bf16），它的算力用 TOPS（每秒多少 tera 的 int8 计算量）来衡量。 而 gpu 用 flops。

### 这类 edge device 上运行 llm，有啥限制

一般感觉限制在于算力。对于 LLM，所需要的算力可以估计为：2 * 参数量。比如 10B model，每个 token 的生成 flops 是 20G。当前芯片一般都满足这类算力。

真正的限制在于内存与内存带宽。一般内存也就 4G、8G 的样子（比如 iphone 16 是 8G 内存）。这样即使是 8bit 量化的模型，7B model，就能把 8G的内存全占满了。而显然不适合 offloading 到闪存。所以内存大小限制了能跑多大 model。

除此外是内存带宽的限制。内存带指的是计算单元cache层和 内存之间交换数据的通路带宽。在一个 token 生成过程中，需要把整个 model 参数所占据的内存（以及激活）往计算单元全移动一遍，这个通信流量是很大的。如果带宽不够，必然推理速度受限制。

### SoC 芯片和内存关系

SoC 芯片上不集成内存，而是后加的。SoC 芯片预留了加内存的接口，称为通道。每个通道都是靠一些针脚（pin）连接二者的。一般每个通道 16-pin，即一次可以传输2个字节。如果是4通道，则是一次8个字节。

不同的芯片的每个通道上可以焊接的内存块，规格都是一样的。LPDDR4X 或者 LPDDR5，所以知道用的是哪种内存，通道数等数据，就可以算出带宽，从而反推出支持多大的 LLM。
