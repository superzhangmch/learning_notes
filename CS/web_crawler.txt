当代web page比较复杂，通用抓取时不能直接wget抓取。经网上搜资料总结，现在的搜索引擎也大多（至少谷歌）是用模拟浏览器法抓取单个网页的。也就是需要模拟js执行。有些网页，不经js直接wget可以拿到东西，但是不全。

当下比较流行的js模拟抓取工具，playwright 与 selenium，前者微软出品较新。无论哪者，当用js抓取时要注意，这时候页面内容本质上是随时间动态变的，也就是难知页面何时加载完。所以需要按需在某些event触发后就当做加载好了。

抓下来后提取正文，一种直接方式是直接用moliza浏览器方出品的readability js代码（所用算法就叫readability），它本来是给浏览器作“阅读模式干净浏览”用的。

虽然基于robots.txt里的sitemap，就能拿到一站点的精华url，实际sitemap里的东西很杂。经研究谷歌，发现它非常暴力。虽然它也有优选，但对于它选定了的：甚至只有一句话的各种低信息页面，它都抓取（而且它建索引的时候，对页面内所有地方都建，比如页面脚，各种犄角旮旯，以及正文处的auther, publish time等等。当然或许它会建到不同类别index里?）
