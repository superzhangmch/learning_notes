# 梯度下降能用于求解受约束的条件极值吗

一般深度学习所用的梯度下降法，所求解的损失函数未必是凸且未必有唯一的极值，但是人们一般不太理会这点。因为最终效果往往还不错。

#### 那么梯度下降法，能用来求解受约束的条件极值问题吗？

受约束的函数求极值问题的求解，可以借助拉格朗日算子法：
比如 $f(x,y) = x + 2y$ 在约束 $x^2+x^2 = 1$ 下的极值，可以转化为 $g(x,y,u) = x + 2y + u (1-x^2-y^2)$ 的极值问题。
$g(x,y,u)$ 未必有唯一极值，未必凸。这时候可能就会觉得，用梯度下降法来求解拉格朗日算子化的条件极值g(x,y,u)问题，大概也不赖吧。

我真这样试了后发现，g(x,y,u) 确实在单调变化，但是"约束条件"却越来越不满足了。整体效果差得很远。

（有感于带核函数 SVM 的最终求解条件可以规约为条件极值。而该条件极值如果可以梯度下降处理，那岂不是带核函数 SVM可以轻松用一般的深度学习框架求解了（不带核SVM有hinge loss，确实可tensorflow等轻松求解））
