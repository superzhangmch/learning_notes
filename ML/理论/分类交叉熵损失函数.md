# 分类交叉熵损失函数

熵的计算公式为 $H = -∑(p_i \log_2(p_i))$ ， 而交叉熵损失函数的形式也正保持与此相同 $L = -∑(p1_i \log_2(p2_i))$ 。其中： $p1_i$ 指的是样本中统计出的概率值， $p2_i$ 是预测概率。注意这里{ $p1_i$ }, { $p2_i$ } 都是概率分布，所以必然满足 $∑p1_i == ∑p2_i == 1$ 。

训练的理想情况应该是两者相等，这时候就转化为真正的熵了。交叉熵损失训练的时候，在不出现过拟合情况下，是恨不得预测概率就完全等于样本经验概率——因为这时候可以说完全预测正确。

<br>

回到机器学习的分类问题。分类结果可以表示为 one-hot 形式的，也就是{ $p1_i$ } 中有且只有一个是1。显然这时{ $p1_i$ } 确实是一个概率分布。训练的目标，就是希望 $p2_i$ 只有对应的那一个尽量取概率1，其他趋于0。one-hot下只有一个为1，所以∑形式最后剩下一项不为零0 。所以我们一般的分类学习问题的交叉熵损失函数的表达式中，别看有个∑号，那只是为了 sum 不同的样本。

所以，别看交叉熵公式是 $L = -∑(p1_i\log_2(p2_i))$ ，那么复杂，一般分类问题的{ $p1_i$ } 只用到了独一的 one-hot 概率分布。

<br>

那么，交叉熵的一般形式什么时候才不会退化为 one-hot 分布这种简单的形式呢？

一种情况是：当单个样本的标注分类结果并不是确定的，而是概率形式的时候。标注都没确定label，当然不能指望能训练出确定的预测。最理想情况是预测概率分布和标注的分布一致。

另一种情况是：训练数据集中，某样本多次出现，却标注的分类不一样。如果把这多条样本合并，那么分类的标注情况就不应该是one-hot，而应该是multi hot，甚至更复杂情况。

<br>

另外注意，交叉熵损失函数其实可以由log似然函数导出来，本质就是似然函数最大化。见：http://datascience.stackexchange.com/questions/9302/the-cross-entropy-error-function-in-neural-networks ，以及http://blog.csdn.net/diligent_321/article/details/53115369

<br>

需要注意： $N = ∑k_i$, 并不是指的train dataset大小。而只是就任一条可能重复出现的样本（不考虑标注）来说的。如果要考虑整个数据集上的极大似然估计，那么每一条去重后的样本，都需要一个N以及一个 $\prod y_i^{k_i}$ 。

<br>

对于二分类，常见 $L = ∑(y1 \cdot \log_2(Y1) + (1-y1)\cdot \log_2(1-Y1))$ 形式，其实和上面形式是一致的——这里是即使相同的样本也单独算一遍的形式。

<br>

另外，只就函数 $f(p2_i)  = -∑(p1_i\cdot \log_2(p2_i))$ 来说，因为 $∑p1_i=1$ , $∑p2_i=1$, 且 $p1_i$ 是常数而 $p2_i$ 是变量，那么由拉格朗日算子法可以求得，如果要使 f 取最小值，那么必须p2_i 取值等于 $p1_i$ 才行。正好和交叉熵损失的最小化优化一致。

这也说明了，如果训练过程中交叉熵损失函数迫近了极小值，那么必然有预测概率与样本经验概率相差不大。如果令 $loss L=∑|p2_i - p1_i|$ 或者 $L=∑|p2_i - p1_i|^2$ , 那么L极小后，也必然有 $p2_i$ 取值等于 $p1_i$ 。如果损失函数一定能取到最小值，且最小值唯一且该点附近是一一的，那么用交叉熵或是绝对平方和还是绝对值和，最终效果会是一样的。只是这个条件很难满足，所以才需要本身有其特性（比如导致训练快）的交叉熵损失函数。

### 补记：
对于一个交叉熵loss的机器学习问题，理想情况当然是loss=0最好，而最坏情况是loss=1。

另外，才随机初始化模型，并开始训练时，可以用随机初始化的参数做一遍预测，则会发现每次的初始loss取值基本是一样的，一个比较大的loss值。这时候，预测值的分布，简直可以说是均匀分布——这容易让人觉得，初始loss的取值只和label的分布有关。如果这样，那么增加减少特征并不会导致初始loss的变动，实际上，增减特征往往会导致初始loss的变动。也就是说，初始loss并不是和label的分布有关。

但是label分布一定得情况下，增减特征，训练完后的最终loss是具有可比性的。
