张量并行：一般都是在单机多卡之间进行。

megatron

专家并行怎么做的

流水线并行几篇 paper

batch_size 和 lr 关系，和 model_size 关系

比较经典的论文有几篇：
	1.	《Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour》（Facebook, 2017）
	•	虽然这篇主旨是用大 batch（8k）也能训练好 ImageNet，但里面就提到，大 batch 会带来泛化性能下降的问题，需要配合 learning rate scaling + warmup 才能缓解。
	2.	《Don’t Decay the Learning Rate, Increase the Batch Size》（OpenAI, 2018）
	•	虽然提出用增大 batch 替代学习率衰减，但也承认过大 batch 会带来 sharp minima，泛化变差。
	3.	《Revisiting Small Batch Training for Deep Neural Networks》（Masters & Luschi, 2018）
	•	这篇直接实验发现，大 batch 的泛化性能明显差于小 batch，收敛速度虽快，但测试集准确率更差。
	•	结论非常明确：batch size 太大 → 泛化差 → 不是越大越好。
	4.	《An Empirical Model of Large-Batch Training》（McCandlish et al., 2018, OpenAI）
	•	提出“critical batch size”概念：超过某个 batch 大小，梯度方差减少不再带来收益，训练速度和效果都会进入“收益递减”阶段。
